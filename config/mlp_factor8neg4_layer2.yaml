experiment_type: mlp
alias: mlp_factor8neg4_layer2
num_epoch: 100
batch_size: 256  # 1024,
optimizer: adam
adam_lr: 0.001
num_users: 6040
num_items: 3706
latent_dim_mlp: 4
num_negative: 4
layers: [8, 16, 8]  # layers[0] is the concat of latent user vector & latent item vector
l2_regularization: 0.0000001  # MLP model is sensitive to hyper params
weight_init_gaussian: True
use_cuda: True
device_id: 0
pretrain: False
pretrain_mf: checkpoints/{}'.format('gmf_factor8neg4_Epoch100_HR0.6391_NDCG0.2852.model
model_dir: checkpoints/{}_Epoch{}_HR{:.4f}_NDCG{:.4f}.model