{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6641c145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "from Dataset import Dataset\n",
    "from evaluate import evaluate_model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "print(f\"Pytorch 版本为:{torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26352937",
   "metadata": {},
   "source": [
    "# GMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0839cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_factors = 8 #嵌入向量维度\n",
    "num_negatives = 4 #负实例数量\n",
    "learner = 'adam' #学习器\n",
    "learning_rate = 0.001 #学习率0.001\n",
    "epochs = 20 #100轮次\n",
    "batch_size = 256 #批次256\n",
    "verbose = 1 #每1次迭代显示一次性能，默认为 1\n",
    "\n",
    "topK = 10\n",
    "evaluation_threads = 1 \n",
    "\n",
    "# 利用源代码的Dataset.py加载数据\n",
    "t1 = time()\n",
    "dataset = Dataset('Data/ml-1m')\n",
    "train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "num_users, num_items = train.shape\n",
    "print(\"Load data done [%.1f s]. #user=%d, #item=%d, #train=%d, #test=%d\" \n",
    "      %(time()-t1, num_users, num_items, train.nnz, len(testRatings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350bea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立GMF模型\n",
    "class GMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, latent_dim):\n",
    "        super(GMF, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, latent_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, latent_dim)\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        nn.init.normal_(self.user_embedding.weight, mean=0, std=0.01)\n",
    "        nn.init.normal_(self.item_embedding.weight, mean=0, std=0.01)\n",
    "        \n",
    "        self.output = nn.Linear(latent_dim, 1)\n",
    "        nn.init.kaiming_uniform_(self.output.weight, nonlinearity='sigmoid')\n",
    "\n",
    "    def forward(self, user_input, item_input):\n",
    "        user_latent = self.user_embedding(user_input).squeeze(1)\n",
    "        item_latent = self.item_embedding(item_input).squeeze(1)\n",
    "        predict_vector = user_latent * item_latent\n",
    "        \n",
    "        # Final prediction layer\n",
    "        prediction = torch.sigmoid(self.output(predict_vector))\n",
    "        return prediction.squeeze()\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GMF(num_users, num_items, num_factors).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "if learner.lower() == \"adagrad\": \n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "elif learner.lower() == \"rmsprop\":\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "elif learner.lower() == \"adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "else:\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "print(model)\n",
    "\n",
    "# Init performance\n",
    "t1 = time()\n",
    "(hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads, device)\n",
    "hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "print('Init: HR = %.4f, NDCG = %.4f\\t [%.1f s]' % (hr, ndcg, time()-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2005c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_instances(train, num_negatives):\n",
    "    user_input, item_input, labels = [], [], []\n",
    "    num_users = train.shape[0]\n",
    "    num_items = train.shape[1]  # 假设 train 是用户-物品矩阵\n",
    "\n",
    "    for (u, i) in train.keys():\n",
    "        # 正例\n",
    "        user_input.append(u)\n",
    "        item_input.append(i)\n",
    "        labels.append(1)\n",
    "        # 负例\n",
    "        for _ in range(num_negatives):\n",
    "            j = np.random.randint(num_items)\n",
    "            while (u, j) in train:\n",
    "                j = np.random.randint(num_items)\n",
    "            user_input.append(u)\n",
    "            item_input.append(j)\n",
    "            labels.append(0)\n",
    "    return user_input, item_input, labels\n",
    "\n",
    "# Train model\n",
    "best_hr, best_ndcg, best_iter = -1, -1, -1\n",
    "for epoch in range(epochs):\n",
    "    t1 = time()\n",
    "    # Generate training instances\n",
    "    user_input, item_input, labels = get_train_instances(train, num_negatives)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(torch.LongTensor(user_input), torch.LongTensor(item_input), torch.FloatTensor(labels))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_user, batch_item, batch_label in train_loader:\n",
    "        batch_user, batch_item, batch_label = batch_user.to(device), batch_item.to(device), batch_label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_user, batch_item).squeeze() \n",
    "        loss = criterion(output, batch_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    t2 = time()\n",
    "    \n",
    "    # Evaluation\n",
    "    if epoch % verbose == 0:\n",
    "        model.eval()\n",
    "        hits, ndcgs = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads,device)\n",
    "        hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "        print(f'Iteration {epoch} [{t2-t1:.1f} s]: HR = {hr:.4f}, NDCG = {ndcg:.4f}, loss = {total_loss/len(train_loader):.4f} [{time()-t2:.1f} s]')\n",
    "        if hr > best_hr:\n",
    "            best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
    "\n",
    "print(f\"End. Best Iteration {best_iter}:  HR = {best_hr:.4f}, NDCG = {best_ndcg:.4f}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d545a58e",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429210be",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_negatives = 4 #负实例数量\n",
    "learner = 'adam' #学习器\n",
    "learning_rate = 0.001 #学习率0.001\n",
    "epochs = 50 #100轮次\n",
    "batch_size = 256 #批次256\n",
    "verbose = 1 #每1次迭代显示一次性能，默认为 1\n",
    "layers = [64,32,16,8]\n",
    "reg_layers = [0,0,0,0]\n",
    "\n",
    "topK = 10\n",
    "evaluation_threads = 1 \n",
    "\n",
    "# 利用源代码的Dataset.py加载数据\n",
    "t1 = time()\n",
    "dataset = Dataset('Data/ml-1m')\n",
    "train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "num_users, num_items = train.shape\n",
    "print(\"Load data done [%.1f s]. #user=%d, #item=%d, #train=%d, #test=%d\" \n",
    "      %(time()-t1, num_users, num_items, train.nnz, len(testRatings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e1b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_users, num_items, layers=[20, 10], reg_layers=[0, 0]):\n",
    "        super(MLP, self).__init__()\n",
    "        assert len(layers) == len(reg_layers)\n",
    "        self.num_layer = len(layers)\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.user_embedding = nn.Embedding(num_users, int(layers[0] / 2))\n",
    "        self.item_embedding = nn.Embedding(num_items, int(layers[0] / 2))\n",
    "        \n",
    "        nn.init.normal_(self.user_embedding.weight, mean=0, std=0.01)\n",
    "        nn.init.normal_(self.item_embedding.weight, mean=0, std=0.01)\n",
    "        \n",
    "        # MLP layers\n",
    "        mlp_layers = []\n",
    "        for i in range(1, self.num_layer):\n",
    "            input_size = layers[i-1]\n",
    "            output_size = layers[i]\n",
    "            mlp_layers.append(nn.Linear(input_size, output_size))\n",
    "            mlp_layers.append(nn.ReLU())\n",
    "            if reg_layers[i] > 0:\n",
    "                mlp_layers.append(nn.Dropout(reg_layers[i]))\n",
    "        self.mlp_layers = nn.Sequential(*mlp_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output = nn.Linear(layers[-1], 1)\n",
    "        nn.init.kaiming_uniform_(self.output.weight, nonlinearity='sigmoid')\n",
    "\n",
    "    def forward(self, user_input, item_input):\n",
    "        user_latent = self.user_embedding(user_input).squeeze(1)\n",
    "        item_latent = self.item_embedding(item_input).squeeze(1)\n",
    "        \n",
    "        # Concatenate user and item embeddings\n",
    "        vector = torch.cat([user_latent, item_latent], dim=-1)\n",
    "        \n",
    "        # Pass through MLP layers\n",
    "        vector = self.mlp_layers(vector)\n",
    "        \n",
    "        # Final prediction layer\n",
    "        prediction = torch.sigmoid(self.output(vector))\n",
    "        return prediction.squeeze()\n",
    "\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MLP(num_users, num_items, layers, reg_layers).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "if learner.lower() == \"adagrad\": \n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "elif learner.lower() == \"rmsprop\":\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "elif learner.lower() == \"adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "else:\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "print(model)\n",
    "\n",
    "# Init performance\n",
    "t1 = time()\n",
    "(hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads, device)\n",
    "hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "print('Init: HR = %.4f, NDCG = %.4f\\t [%.1f s]' % (hr, ndcg, time()-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352741dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "best_hr, best_ndcg, best_iter = -1, -1, -1\n",
    "for epoch in range(epochs):\n",
    "    t1 = time()\n",
    "    # Generate training instances\n",
    "    user_input, item_input, labels = get_train_instances(train, num_negatives)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(torch.LongTensor(user_input), torch.LongTensor(item_input), torch.FloatTensor(labels))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_user, batch_item, batch_label in train_loader:\n",
    "        batch_user, batch_item, batch_label = batch_user.to(device), batch_item.to(device), batch_label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_user, batch_item).squeeze() \n",
    "        loss = criterion(output, batch_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    t2 = time()\n",
    "    \n",
    "    # Evaluation\n",
    "    if epoch % verbose == 0:\n",
    "        model.eval()\n",
    "        hits, ndcgs = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads,device)\n",
    "        hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "        print(f'Iteration {epoch} [{t2-t1:.1f} s]: HR = {hr:.4f}, NDCG = {ndcg:.4f}, loss = {total_loss/len(train_loader):.4f} [{time()-t2:.1f} s]')\n",
    "        if hr > best_hr:\n",
    "            best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
    "\n",
    "print(f\"End. Best Iteration {best_iter}:  HR = {best_hr:.4f}, NDCG = {best_ndcg:.4f}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a366939f",
   "metadata": {},
   "source": [
    "# NeuMF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba5959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 256\n",
    "mf_dim = 128\n",
    "layers = [256,128,64,32]\n",
    "reg_mf = 0\n",
    "reg_layers = [0,0,0,0]\n",
    "num_negatives = 4\n",
    "learning_rate = 0.001\n",
    "learner = 'adam'\n",
    "verbose = 1\n",
    "mf_pretrain = ''\n",
    "mlp_pretrain = ''\n",
    "\n",
    "topK = 10\n",
    "evaluation_threads = 1\n",
    "# 利用源代码的Dataset.py加载数据\n",
    "t1 = time()\n",
    "dataset = Dataset('Data/ml-1m')\n",
    "train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "num_users, num_items = train.shape\n",
    "print(\"Load data done [%.1f s]. #user=%d, #item=%d, #train=%d, #test=%d\" \n",
    "      %(time()-t1, num_users, num_items, train.nnz, len(testRatings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3872ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, mf_dim=10, layers=[10], reg_layers=[0], reg_mf=0):\n",
    "        super(NeuMF, self).__init__()\n",
    "        assert len(layers) == len(reg_layers)\n",
    "        self.mf_dim = mf_dim\n",
    "        self.num_layer = len(layers)\n",
    "        \n",
    "        # MF embeddings\n",
    "        self.mf_user_embedding = nn.Embedding(num_users, mf_dim)\n",
    "        self.mf_item_embedding = nn.Embedding(num_items, mf_dim)\n",
    "        \n",
    "        # Initialize MF embeddings\n",
    "        nn.init.normal_(self.mf_user_embedding.weight, mean=0, std=0.01)\n",
    "        nn.init.normal_(self.mf_item_embedding.weight, mean=0, std=0.01)\n",
    "        \n",
    "        # MLP embeddings\n",
    "        self.mlp_user_embedding = nn.Embedding(num_users, layers[0] // 2)\n",
    "        self.mlp_item_embedding = nn.Embedding(num_items, layers[0] // 2)\n",
    "        \n",
    "        # Initialize MLP embeddings\n",
    "        nn.init.normal_(self.mlp_user_embedding.weight, mean=0, std=0.01)\n",
    "        nn.init.normal_(self.mlp_item_embedding.weight, mean=0, std=0.01)\n",
    "        \n",
    "        # MLP layers\n",
    "        mlp_layers = []\n",
    "        for i in range(1, self.num_layer):\n",
    "            mlp_layers.append(nn.Linear(layers[i-1], layers[i]))\n",
    "            mlp_layers.append(nn.ReLU())\n",
    "            if reg_layers[i] > 0:\n",
    "                mlp_layers.append(nn.Dropout(reg_layers[i]))\n",
    "        self.mlp_layers = nn.Sequential(*mlp_layers)\n",
    "        \n",
    "        # Final prediction layer\n",
    "        predict_size = mf_dim + layers[-1]\n",
    "        self.output = nn.Linear(predict_size, 1)\n",
    "        nn.init.kaiming_uniform_(self.output.weight, nonlinearity='sigmoid')\n",
    "\n",
    "    def forward(self, user_input, item_input):\n",
    "        # MF part\n",
    "        mf_user_latent = self.mf_user_embedding(user_input)\n",
    "        mf_item_latent = self.mf_item_embedding(item_input)\n",
    "        mf_vector = mf_user_latent * mf_item_latent\n",
    "        \n",
    "        # MLP part\n",
    "        mlp_user_latent = self.mlp_user_embedding(user_input)\n",
    "        mlp_item_latent = self.mlp_item_embedding(item_input)\n",
    "        mlp_vector = torch.cat((mlp_user_latent, mlp_item_latent), dim=-1)\n",
    "        mlp_vector = self.mlp_layers(mlp_vector)\n",
    "        \n",
    "        # Concatenate MF and MLP parts\n",
    "        predict_vector = torch.cat((mf_vector, mlp_vector), dim=-1)\n",
    "        \n",
    "        # Final prediction layer\n",
    "        prediction = torch.sigmoid(self.output(predict_vector))\n",
    "        return prediction.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d821db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NeuMF(num_users, num_items, mf_dim, layers, reg_layers, reg_mf).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "if learner.lower() == \"adagrad\": \n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "elif learner.lower() == \"rmsprop\":\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "elif learner.lower() == \"adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "else:\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "print(model)\n",
    "\n",
    "t1 = time()\n",
    "(hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads, device)\n",
    "hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "print('Init: HR = %.4f, NDCG = %.4f\\t [%.1f s]' % (hr, ndcg, time()-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f80b11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "best_hr, best_ndcg, best_iter = -1, -1, -1\n",
    "for epoch in range(epochs):\n",
    "    t1 = time()\n",
    "    # Generate training instances\n",
    "    user_input, item_input, labels = get_train_instances(train, num_negatives)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(torch.LongTensor(user_input), torch.LongTensor(item_input), torch.FloatTensor(labels))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_user, batch_item, batch_label in train_loader:\n",
    "        batch_user, batch_item, batch_label = batch_user.to(device), batch_item.to(device), batch_label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_user, batch_item).squeeze() \n",
    "        loss = criterion(output, batch_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    t2 = time()\n",
    "    \n",
    "    # Evaluation\n",
    "    if epoch % verbose == 0:\n",
    "        model.eval()\n",
    "        hits, ndcgs = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads,device)\n",
    "        hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "        print(f'Iteration {epoch} [{t2-t1:.1f} s]: HR = {hr:.4f}, NDCG = {ndcg:.4f}, loss = {total_loss/len(train_loader):.4f} [{time()-t2:.1f} s]')\n",
    "        if hr > best_hr:\n",
    "            best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
    "\n",
    "print(f\"End. Best Iteration {best_iter}:  HR = {best_hr:.4f}, NDCG = {best_ndcg:.4f}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
