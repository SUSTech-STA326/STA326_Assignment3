{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from tqdm import  tqdm\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Pytorch's version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0a0+6ddf5cf85e.nv24.04\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a quick look of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>978824330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>978824330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1  2          3\n",
       "0  0  32  4  978824330\n",
       "1  0  34  4  978824330\n",
       "2  0   4  5  978824291\n",
       "3  0  35  4  978824291\n",
       "4  0  30  4  978824291"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_table(\"Data/ml-1m.train.rating\",sep=\"\\t\",header=None)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define NCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self._num_users = config['num_users']\n",
    "        self._num_items = config['num_items']\n",
    "        self._X = config['layer_X']\n",
    "        self._factor = config['factor']\n",
    "        self._embedding_size_gmf = self._factor\n",
    "        self._embedding_size_mlp = self._factor*(2**(self._X-1))\n",
    "\n",
    "        self._embedding__user_gmf = nn.Embedding(self._num_users, self._embedding_size_gmf)\n",
    "        self._embedding__item_gmf = nn.Embedding(self._num_items, self._embedding_size_gmf)\n",
    "\n",
    "        if self._X > 0:\n",
    "            self._embedding__user_mlp = nn.Embedding(self._num_users, self._embedding_size_mlp)\n",
    "            self._embedding__item_mlp = nn.Embedding(self._num_items, self._embedding_size_mlp)\n",
    "\n",
    "            self._fc_layers = nn.ModuleList()\n",
    "            for idx in range(self._X-1, -1, -1):\n",
    "                in_size = self._factor*(2**(idx+1))\n",
    "                out_size = self._factor*(2**idx)\n",
    "                self._fc_layers.append(nn.Linear(in_size, out_size))\n",
    "        self._out_fc = nn.Linear(self._factor, 1, bias=False)\n",
    "        \n",
    "        self._activate1 = nn.Sigmoid()\n",
    "        self._activate2 = nn.ReLU()\n",
    "    def __repr__(self):\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMF(NCF,nn.Module):\n",
    "    def __init__(self, config):\n",
    "        nn.Module.__init__(self)\n",
    "        NCF.__init__(self, config)\n",
    "\n",
    "    def forward(self, user_idx, item_idx):\n",
    "        user_embedding = self._embedding__user_gmf(user_idx)\n",
    "        item_embedding = self._embedding__item_gmf(item_idx)\n",
    "        pointwise_vector = torch.mul(user_embedding, item_embedding)\n",
    "        logit = self._out_fc(pointwise_vector)\n",
    "        prob = self._activate1(logit)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(NCF,nn.Module):\n",
    "    def __init__(self, config):\n",
    "        nn.Module.__init__(self)\n",
    "        NCF.__init__(self, config)\n",
    "\n",
    "    def forward(self, user_idx, item_idx):\n",
    "        user_embedding = self._embedding__user_mlp(user_idx)\n",
    "        item_embedding = self._embedding__item_mlp(item_idx)\n",
    "        vector = torch.cat([user_embedding, item_embedding], dim=-1)\n",
    "        for _, layer in enumerate(self._fc_layers):\n",
    "            vector = layer(vector)\n",
    "            vector = self._activate2(vector)\n",
    "        logit = self._out_fc(vector)\n",
    "        prob = self._activate1(logit)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuMF(NCF,nn.Module):\n",
    "    def __init__(self, config):\n",
    "        nn.Module.__init__(self)\n",
    "        NCF.__init__(self, config)\n",
    "        self._neumf_fc = nn.Linear(self._factor*2, 1, bias=False)\n",
    "    \n",
    "    def forward(self, user_idx, item_idx):\n",
    "        user_embedding_gmf = self._embedding__user_gmf(user_idx)\n",
    "        item_embedding_gmf = self._embedding__item_gmf(item_idx)\n",
    "        pointwise_vector_gmf = torch.mul(user_embedding_gmf, item_embedding_gmf)\n",
    "\n",
    "        user_embedding_mlp = self._embedding__user_mlp(user_idx)\n",
    "        item_embedding_mlp = self._embedding__item_mlp(item_idx)\n",
    "        vector_mlp = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1)\n",
    "        for _, layer in enumerate(self._fc_layers):\n",
    "            vector_mlp = layer(vector_mlp)\n",
    "            vector_mlp = self._activate2(vector_mlp)\n",
    "\n",
    "        vector_neumf = torch.cat([pointwise_vector_gmf, vector_mlp], dim=-1)\n",
    "        logit = self._neumf_fc(vector_neumf)\n",
    "        prob = self._activate1(logit)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMF_config = {'num_users': 6040, 'num_items': 3706, 'factor': 8, 'layer_X': 0}\n",
    "MLP_config = {'num_users': 6040, 'num_items': 3706, 'factor': 8, 'layer_X': 3}\n",
    "NeuMF_config = {'num_users': 6040, 'num_items': 3706, 'factor': 8, 'layer_X': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GMF_model = GMF(GMF_config)\n",
    "GMF_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%self.1 : __torch__.GMF,\n",
      "      %user_idx : Long(1, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %item_idx : Long(1, strides=[1], requires_grad=0, device=cuda:0)):\n",
      "  %_activate1 : __torch__.torch.nn.modules.activation.Sigmoid = prim::GetAttr[name=\"_activate1\"](%self.1)\n",
      "  %_out_fc : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"_out_fc\"](%self.1)\n",
      "  %_embedding__item_gmf : __torch__.torch.nn.modules.sparse.___torch_mangle_0.Embedding = prim::GetAttr[name=\"_embedding__item_gmf\"](%self.1)\n",
      "  %_embedding__user_gmf : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name=\"_embedding__user_gmf\"](%self.1)\n",
      "  %50 : Tensor = prim::CallMethod[name=\"forward\"](%_embedding__user_gmf, %user_idx)\n",
      "  %51 : Tensor = prim::CallMethod[name=\"forward\"](%_embedding__item_gmf, %item_idx)\n",
      "  %input.1 : Float(1, 8, strides=[8, 1], requires_grad=1, device=cuda:0) = aten::mul(%50, %51) # /tmp/ipykernel_35005/167587794.py:9:0\n",
      "  %52 : Tensor = prim::CallMethod[name=\"forward\"](%_out_fc, %input.1)\n",
      "  %53 : Tensor = prim::CallMethod[name=\"forward\"](%_activate1, %52)\n",
      "  return (%53)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate random user_idx and item_idx\n",
    "user_idx = torch.randint(0, GMF_config['num_users'], (1,)).to(device)\n",
    "item_idx = torch.randint(0, GMF_config['num_items'], (1,)).to(device)\n",
    "\n",
    "# using torch.jit.trace to trace model\n",
    "traced_model = torch.jit.trace(GMF_model, (user_idx, item_idx))\n",
    "\n",
    "print(traced_model.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_model = MLP(MLP_config)\n",
    "MLP_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NeuMF_model = NeuMF(NeuMF_config)\n",
    "NeuMF_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, path):\n",
    "        self.trainMatrix = self.load_rating_file_as_matrix(path + \".train.rating\")\n",
    "        self.testRatings = self.load_rating_file_as_list(path + \".test.rating\")\n",
    "        self.testNegatives = self.load_negative_file(path + \".test.negative\")\n",
    "        assert len(self.testRatings) == len(self.testNegatives)\n",
    "        self.num_users, self.num_items = self.trainMatrix.shape\n",
    "        \n",
    "    def load_rating_file_as_list(self, filename):\n",
    "        ratingList = []\n",
    "        with open(filename, \"r\") as f:\n",
    "            line = f.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\"\\t\")\n",
    "                user, item = int(arr[0]), int(arr[1])\n",
    "                ratingList.append([user, item])\n",
    "                line = f.readline()\n",
    "        return ratingList\n",
    "    \n",
    "    def load_negative_file(self, filename):\n",
    "        negativeList = []\n",
    "        with open(filename, \"r\") as f:\n",
    "            line = f.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\"\\t\")\n",
    "                negatives = []\n",
    "                for x in arr[1: ]:\n",
    "                    negatives.append(int(x))\n",
    "                negativeList.append(negatives)\n",
    "                line = f.readline()\n",
    "        return negativeList\n",
    "    \n",
    "    def load_rating_file_as_matrix(self, filename):\n",
    "        num_users, num_items = 0, 0\n",
    "        with open(filename, \"r\") as f:\n",
    "            line = f.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\"\\t\")\n",
    "                u, i = int(arr[0]), int(arr[1])\n",
    "                num_users = max(num_users, u)\n",
    "                num_items = max(num_items, i)\n",
    "                line = f.readline()\n",
    "        mat = sp.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n",
    "        with open(filename, \"r\") as f:\n",
    "            line = f.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\"\\t\")\n",
    "                user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
    "                if (rating > 0):\n",
    "                    mat[user, item] = 1.0\n",
    "                line = f.readline()    \n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(\"./Data/\"+\"ml-1m\")\n",
    "train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "num_users, num_items = train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding negative samples to trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_instances(train, num_negatives):\n",
    "    user_input, item_input, labels = [],[],[]\n",
    "    for (u, i) in train.keys():\n",
    "        # positive instance\n",
    "        user_input.append(u)\n",
    "        item_input.append(i)\n",
    "        labels.append(1)\n",
    "        # negative instances\n",
    "        for t in range(num_negatives):\n",
    "            j = np.random.randint(num_items)\n",
    "            while (u, j) in train:\n",
    "                j = np.random.randint(num_items)\n",
    "            user_input.append(u)\n",
    "            item_input.append(j)\n",
    "            labels.append(0)\n",
    "    return user_input, item_input, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create traindataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class mlDataset(Dataset):\n",
    "    def __init__(self, user_input, item_input, labels):\n",
    "        self.user_input = user_input\n",
    "        self.item_input = item_input\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, index):\n",
    "        return self.user_input[index], self.item_input[index], self.labels[index]\n",
    "user_input, item_input, labels = get_train_instances(train, num_negatives=4)\n",
    "train_dataset = mlDataset(user_input, item_input, labels)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,topk):\n",
    "    class testDataset(Dataset):\n",
    "        def __init__(self, rating, negative_lists):\n",
    "            self.rating = rating\n",
    "            self.negative_lists = negative_lists\n",
    "        def __len__(self):\n",
    "            return len(self.rating)\n",
    "        def __getitem__(self, index):\n",
    "            return self.rating[index], self.negative_lists[index]\n",
    "\n",
    "    def HR_NDCG(testloader):\n",
    "        model.eval()\n",
    "        ht = 0; ndcg = 0\n",
    "        with torch.no_grad():\n",
    "            for rating, negatives in testloader:\n",
    "                user_idxs = torch.tensor([x[0] for x in rating], dtype=torch.long).to(device)\n",
    "                pos_item_idxs = torch.tensor([x[1] for x in rating], dtype=torch.long).to(device)\n",
    "                neg_item_idxs = torch.stack([x.clone().detach() for x in negatives]).to(device)\n",
    "                print(pos_item_idxs.size())\n",
    "                print(neg_item_idxs.size())\n",
    "\n",
    "                pos_scores = model(user_idxs, pos_item_idxs).unsqueeze(1)  # (batch_size, 1)\n",
    "                neg_scores = model(user_idxs.unsqueeze(1).repeat(1, neg_item_idxs.size(1)), neg_item_idxs)  # (batch_size, num_negatives)\n",
    "                all_scores = torch.cat((pos_scores, neg_scores), dim=1)  # (batch_size, num_negatives+1)\n",
    "\n",
    "                # calculate HR\n",
    "                _, topk_indices = torch.topk(all_scores, topk, dim=1, largest=True, sorted=True)\n",
    "                ht += torch.sum((topk_indices == 0).int()).item()  # 0 is the index of positive example in concatenated scores\n",
    "\n",
    "                # calculate NDCG\n",
    "                sorted_scores, _ = torch.sort(all_scores, dim=1, descending=True)\n",
    "                _, rankings = torch.where(pos_scores == sorted_scores)\n",
    "                ndcg += torch.sum(1 / torch.log2(rankings + 2)).item()\n",
    "\n",
    "        return ht / len(testloader.dataset), ndcg / len(testloader.dataset)\n",
    "\n",
    "    test_dataset = testDataset(testRatings, testNegatives)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=256, shuffle=False)\n",
    "    hr, ndcg = HR_NDCG(test_loader)\n",
    "    return hr, ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(GMF_model.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(GMF_model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss() #期望的输入是经过sigmoid函数处理的，此处应该选用BCELoss而不是BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:41<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.6930796504020691\n",
      "torch.Size([2])\n",
      "torch.Size([99, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (99) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m HR, NDCG \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGMF_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHR@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, NDCG@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNDCG\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 39\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, topk)\u001b[0m\n\u001b[1;32m     37\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m testDataset(testRatings, testNegatives)\n\u001b[1;32m     38\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset\u001b[38;5;241m=\u001b[39mtest_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 39\u001b[0m hr, ndcg \u001b[38;5;241m=\u001b[39m \u001b[43mHR_NDCG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hr, ndcg\n",
      "Cell \u001b[0;32mIn[17], line 23\u001b[0m, in \u001b[0;36mevaluate.<locals>.HR_NDCG\u001b[0;34m(testloader)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(neg_item_idxs\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     22\u001b[0m pos_scores \u001b[38;5;241m=\u001b[39m model(user_idxs, pos_item_idxs)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, 1)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m neg_scores \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_idxs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_item_idxs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_item_idxs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch_size, num_negatives)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m all_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((pos_scores, neg_scores), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, num_negatives+1)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# calculate HR\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1536\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1534\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1535\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1539\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36mGMF.forward\u001b[0;34m(self, user_idx, item_idx)\u001b[0m\n\u001b[1;32m      7\u001b[0m user_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding__user_gmf(user_idx)\n\u001b[1;32m      8\u001b[0m item_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding__item_gmf(item_idx)\n\u001b[0;32m----> 9\u001b[0m pointwise_vector \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_embedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m logit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_out_fc(pointwise_vector)\n\u001b[1;32m     11\u001b[0m prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activate1(logit)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (99) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    GMF_model.train()\n",
    "    running_loss = 0\n",
    "    for user_idxs, item_idxs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        user_idxs = user_idxs.to(device)\n",
    "        item_idxs = item_idxs.to(device)\n",
    "        labels = labels.to(device).to(torch.float32)\n",
    "\n",
    "        outputs = GMF_model(user_idxs, item_idxs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "    HR, NDCG = evaluate(GMF_model, topk)\n",
    "    print(f'HR@{topk}: {HR}, NDCG@{topk}: {NDCG}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
