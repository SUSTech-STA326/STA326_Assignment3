{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f54f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c88196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Embedding):\n",
    "        torch.nn.init.normal_(m.weight.data, mean=0.0, std=0.01)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        torch.nn.init.normal_(m.weight.data, mean=0.0, std=0.01)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "class GMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_size):\n",
    "        super(GMF, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_size)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_size)\n",
    "        self.fc = nn.Linear(embedding_size, 1)\n",
    "        \n",
    "    def forward(self, user_ids, item_ids):\n",
    "        user_embedding = self.user_embedding(user_ids)\n",
    "        item_embedding = self.item_embedding(item_ids)\n",
    "        element_product = user_embedding * item_embedding\n",
    "        logits = self.fc(element_product)\n",
    "        return logits.view(-1)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_users, num_items, hidden_sizes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        \n",
    "        layers = []\n",
    "        input_size = num_users + num_items\n",
    "        layer_sizes = [input_size] + hidden_sizes + [1] \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            if i < len(layer_sizes) - 2:\n",
    "                layers.append(nn.ReLU())  \n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, user_ids, item_ids):\n",
    "        user_onehot = F.one_hot(user_ids, num_classes=self.num_users).float()\n",
    "        item_onehot = F.one_hot(item_ids, num_classes=self.num_items).float()\n",
    "        input_vec = torch.cat([user_onehot, item_onehot], dim=1)\n",
    "        return self.mlp(input_vec)\n",
    "\n",
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, mf_dim=10, mlp_layers=[64, 32], dropout=0.2):\n",
    "        super(NeuMF, self).__init__()\n",
    "        self.user_embedding_mf = nn.Embedding(num_users, mf_dim)\n",
    "        self.item_embedding_mf = nn.Embedding(num_items, mf_dim)\n",
    "        layers = []\n",
    "        input_size = mf_dim * 2\n",
    "        layer_sizes = [input_size] + mlp_layers\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.output_layer = nn.Linear(mlp_layers[-1] + mf_dim, 1)\n",
    "\n",
    "    def forward(self, user_ids, item_ids):\n",
    "        user_embedding_mf = self.user_embedding_mf(user_ids)\n",
    "        item_embedding_mf = self.item_embedding_mf(item_ids)\n",
    "        mf_output = torch.mul(user_embedding_mf, item_embedding_mf)  # Element-wise multiplication\n",
    "\n",
    "        user_embedding_mlp = self.user_embedding_mf(user_ids)\n",
    "        item_embedding_mlp = self.item_embedding_mf(item_ids)\n",
    "        mlp_input = torch.cat((user_embedding_mlp, item_embedding_mlp), dim=1)\n",
    "        mlp_output = self.mlp(mlp_input)\n",
    "\n",
    "        concat_output = torch.cat((mf_output, mlp_output), dim=1)\n",
    "        output = self.output_layer(concat_output).squeeze()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6f93f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from model import GMF, MLP, NeuMF\n",
    "# from evaluation import evaluate_model\n",
    "import re \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "data_dir = 'ml-1m'\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "class NegativeDataset(Dataset):\n",
    "    def __init__(self, negative_file):\n",
    "        self.negatives = self._load_negative(negative_file)\n",
    "\n",
    "    def _load_negative(self, negative_file):\n",
    "        negatives = {}\n",
    "        with open(negative_file, 'r') as file:\n",
    "            for line in file:\n",
    "                line = line.strip().split('\\t')\n",
    "                user, item = eval(line[0])\n",
    "                negatives[(user, item)] = list(map(int, line[1:]))\n",
    "        return negatives\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.negatives)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_item = list(self.negatives.keys())[idx]\n",
    "        user_id = torch.tensor(user_item[0], dtype=torch.int64)\n",
    "        item_id = torch.tensor(user_item[1], dtype=torch.int64)\n",
    "        negative_ids = torch.tensor(self.negatives[user_item], dtype=torch.int64)\n",
    "        return user_id, item_id, negative_ids\n",
    "\n",
    "\n",
    "train_data = pd.read_csv(f'{data_dir}/ml-1m.train.rating', sep='\\t', header=None, names=['user', 'item', 'rating', 'timestamp'])\n",
    "train_data = train_data.drop(columns='timestamp')\n",
    "test_data = pd.read_csv(f'{data_dir}/ml-1m.test.rating', sep='\\t', header=None, names=['user', 'item', 'rating', 'timestamp'])\n",
    "test_data = test_data.drop(columns='timestamp')\n",
    "negative_dataset = NegativeDataset('./ml-1m/ml-1m.test.negative')\n",
    "negative_loader = DataLoader(negative_dataset, batch_size=batch_size, shuffle=False)\n",
    "train_dataset = TensorDataset(torch.tensor(train_data['user'].values),\n",
    "                          torch.tensor(train_data['item'].values),\n",
    "                          torch.tensor(train_data['rating'].values))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = TensorDataset(torch.tensor(test_data['user'].values),\n",
    "                          torch.tensor(test_data['item'].values),\n",
    "                          torch.tensor(test_data['rating'].values))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_users, test_items = torch.tensor(test_data['user'].values), torch.tensor(test_data['item'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d3b56b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040\n",
      "3706\n"
     ]
    }
   ],
   "source": [
    "num_users = max(train_data['user'].max(), test_data['user'].max()) + 1\n",
    "print(num_users)\n",
    "num_items = max(train_data['item'].max(), test_data['item'].max()) + 1\n",
    "print(num_items)\n",
    "\n",
    "embedding_size = 64\n",
    "gmf_model = GMF(num_users, num_items, embedding_size)\n",
    "hidden_sizes = [16]  \n",
    "mlp_model_1 = MLP(num_users, num_items, hidden_sizes)\n",
    "hidden_sizes = [16, 8] \n",
    "mlp_model_2 = MLP(num_users, num_items, hidden_sizes)\n",
    "hidden_sizes = [16, 64, 8] \n",
    "mlp_model_3 = MLP(num_users, num_items, hidden_sizes)\n",
    "hidden_sizes = [16, 64, 16, 8] \n",
    "mlp_model_4 = MLP(num_users, num_items, hidden_sizes)\n",
    "neumf_model = NeuMF(num_users, num_items, mf_dim=10, mlp_layers=[64, 32], dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "093fce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, negative_loader, top_k=10):\n",
    "    model.eval()\n",
    "    HR_list = []\n",
    "    NDCG_list = []\n",
    "\n",
    "    for (user_ids, pos_item_ids, _), (neg_user_ids, _, neg_item_ids) in zip(test_loader, negative_loader):\n",
    "        items = torch.cat([pos_item_ids.unsqueeze(1), neg_item_ids], dim=1)\n",
    "        user_ids = user_ids.unsqueeze(1).expand(-1, items.size(1))\n",
    "\n",
    "        user_ids = user_ids.reshape(-1)\n",
    "        item_ids = items.reshape(-1)\n",
    "        predictions = model(user_ids, item_ids).squeeze()\n",
    "        predictions = predictions.reshape(-1, 100)\n",
    "        _, indices = torch.topk(predictions, k=top_k, dim=1)\n",
    "        recommended_items = items.gather(1, indices)\n",
    "\n",
    "        HR = (recommended_items == pos_item_ids.unsqueeze(1)).any(dim=1).float()\n",
    "        HR_list.append(HR.mean().item())\n",
    "\n",
    "        relevant = (recommended_items == pos_item_ids.unsqueeze(1))\n",
    "        rank = relevant.nonzero(as_tuple=True)[1]\n",
    "        NDCG = (1 / torch.log2(rank.float() + 2)).mean().item()\n",
    "        NDCG_list.append(NDCG)\n",
    "\n",
    "    mean_HR = np.mean(HR_list)\n",
    "    mean_NDCG = np.mean(NDCG_list)\n",
    "    \n",
    "    return mean_HR, mean_NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8084cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "results = []\n",
    "model = gmf_model\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "for epoch in range(30):\n",
    "    for users, items, ratings in train_loader:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        predictions = model(users, items).squeeze()\n",
    "        predictions = predictions.view(-1)\n",
    "        loss = criterion(predictions, ratings.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{30}')\n",
    "    hr, ndcg = evaluate(model, test_loader, negative_loader, top_k=10)\n",
    "    results.append({\n",
    "        'Model': 'GMF',\n",
    "        'HR@10': hr,\n",
    "        'NDCG@10': ndcg\n",
    "    })\n",
    "    print(f\"Model: {model.__class__.__name__}, HR@10: {hr}, NDCG@10: {ndcg}\")\n",
    "    \n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('GMF.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f667b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "count = 0\n",
    "for model in [mlp_model_1, mlp_model_2, mlp_model_3, mlp_model_4]:\n",
    "    count = count + 1\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    for epoch in range(30):\n",
    "        for users, items, ratings in train_loader:\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            predictions = model(users, items).squeeze()\n",
    "            predictions = predictions.view(-1)\n",
    "            loss = criterion(predictions, ratings.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{30}')\n",
    "        hr, ndcg = evaluate(model, test_loader, negative_loader, top_k=10)\n",
    "        results.append({\n",
    "            'Model': 'MLP_' + str(count),\n",
    "            'HR@10': hr,\n",
    "            'NDCG@10': ndcg\n",
    "        })\n",
    "        print(f\"Model: MLP_{count}, HR@10: {hr}, NDCG@10: {ndcg}\")\n",
    "        \n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('MLP.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96a165da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Model: NeuMF, HR@10: 0.4447916667712362, NDCG@10: 0.5538063309694591\n",
      "Epoch 2/30\n",
      "Model: NeuMF, HR@10: 0.4459429825607099, NDCG@10: 0.5587665598643453\n",
      "Epoch 3/30\n",
      "Model: NeuMF, HR@10: 0.44544956150807835, NDCG@10: 0.559753398832522\n",
      "Epoch 4/30\n",
      "Model: NeuMF, HR@10: 0.4452850878238678, NDCG@10: 0.560041973779076\n",
      "Epoch 5/30\n",
      "Model: NeuMF, HR@10: 0.446600877297552, NDCG@10: 0.558251712824169\n",
      "Epoch 6/30\n",
      "Model: NeuMF, HR@10: 0.44627192992913095, NDCG@10: 0.5578570930581344\n",
      "Epoch 7/30\n",
      "Model: NeuMF, HR@10: 0.4457785088764994, NDCG@10: 0.5586960654509695\n",
      "Epoch 8/30\n",
      "Model: NeuMF, HR@10: 0.44610745624492043, NDCG@10: 0.5584603849210237\n",
      "Epoch 9/30\n",
      "Model: NeuMF, HR@10: 0.4459429825607099, NDCG@10: 0.5583915290079619\n",
      "Epoch 10/30\n",
      "Model: NeuMF, HR@10: 0.4459429825607099, NDCG@10: 0.5577109474884836\n",
      "Epoch 11/30\n",
      "Model: NeuMF, HR@10: 0.44643640361334147, NDCG@10: 0.5573945196051346\n",
      "Epoch 12/30\n",
      "Model: NeuMF, HR@10: 0.446600877297552, NDCG@10: 0.5573740720748901\n",
      "Epoch 13/30\n",
      "Model: NeuMF, HR@10: 0.446600877297552, NDCG@10: 0.5575544570621691\n",
      "Epoch 14/30\n",
      "Model: NeuMF, HR@10: 0.44643640361334147, NDCG@10: 0.5576567863163195\n",
      "Epoch 15/30\n",
      "Model: NeuMF, HR@10: 0.4467653509817625, NDCG@10: 0.5572417202748751\n",
      "Epoch 16/30\n",
      "Model: NeuMF, HR@10: 0.44643640361334147, NDCG@10: 0.5573038016494952\n",
      "Epoch 17/30\n",
      "Model: NeuMF, HR@10: 0.44627192992913095, NDCG@10: 0.5573519320864426\n",
      "Epoch 18/30\n",
      "Model: NeuMF, HR@10: 0.44627192992913095, NDCG@10: 0.5573318202244608\n",
      "Epoch 19/30\n",
      "Model: NeuMF, HR@10: 0.446600877297552, NDCG@10: 0.5572456243791079\n",
      "Epoch 20/30\n",
      "Model: NeuMF, HR@10: 0.446600877297552, NDCG@10: 0.5572540160856749\n",
      "Epoch 21/30\n",
      "Model: NeuMF, HR@10: 0.44643640361334147, NDCG@10: 0.5572758991467325\n",
      "Epoch 22/30\n",
      "Model: NeuMF, HR@10: 0.44627192992913095, NDCG@10: 0.5576106011867523\n",
      "Epoch 23/30\n",
      "Model: NeuMF, HR@10: 0.44627192992913095, NDCG@10: 0.5577009856700897\n",
      "Epoch 24/30\n",
      "Model: NeuMF, HR@10: 0.44643640361334147, NDCG@10: 0.5576980418280552\n",
      "Epoch 25/30\n",
      "Model: NeuMF, HR@10: 0.44610745624492043, NDCG@10: 0.5577347836996379\n",
      "Epoch 26/30\n",
      "Model: NeuMF, HR@10: 0.44610745624492043, NDCG@10: 0.5576465813737167\n",
      "Epoch 27/30\n",
      "Model: NeuMF, HR@10: 0.44643640361334147, NDCG@10: 0.557455523390519\n",
      "Epoch 28/30\n",
      "Model: NeuMF, HR@10: 0.44643640361334147, NDCG@10: 0.557432362280394\n",
      "Epoch 29/30\n",
      "Model: NeuMF, HR@10: 0.44643640361334147, NDCG@10: 0.5573960552090093\n",
      "Epoch 30/30\n",
      "Model: NeuMF, HR@10: 0.44643640361334147, NDCG@10: 0.5575231548986936\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "model = neumf_model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "for epoch in range(30):\n",
    "    for users, items, ratings in train_loader:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        predictions = model(users, items).squeeze()\n",
    "        predictions = predictions.view(-1)\n",
    "        loss = criterion(predictions, ratings.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{30}')\n",
    "    hr, ndcg = evaluate(model, test_loader, negative_loader, top_k=10)\n",
    "    results.append({\n",
    "        'Model': model.__class__.__name__,\n",
    "#             'Loss': loss.item(),\n",
    "        'HR@10': hr,\n",
    "        'NDCG@10': ndcg\n",
    "    })\n",
    "    print(f\"Model: {model.__class__.__name__}, HR@10: {hr}, NDCG@10: {ndcg}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('NeuMF.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9296263a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a5e9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4fecd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efea6544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
