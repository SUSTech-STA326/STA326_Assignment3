{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T11:32:49.127962Z",
     "iopub.status.busy": "2024-05-22T11:32:49.127492Z",
     "iopub.status.idle": "2024-05-22T11:32:50.669389Z",
     "shell.execute_reply": "2024-05-22T11:32:50.668447Z",
     "shell.execute_reply.started": "2024-05-22T11:32:49.127896Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T11:32:50.671069Z",
     "iopub.status.busy": "2024-05-22T11:32:50.670822Z",
     "iopub.status.idle": "2024-05-22T11:32:50.683847Z",
     "shell.execute_reply": "2024-05-22T11:32:50.683281Z",
     "shell.execute_reply.started": "2024-05-22T11:32:50.671049Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "class UserItemRatingDataset(Dataset):\n",
    "    \"\"\"Wrapper, convert <user, item, rating> Tensor into Pytorch Dataset\"\"\"\n",
    "    def __init__(self, user_tensor, item_tensor, target_tensor):\n",
    "        self.user_tensor = user_tensor\n",
    "        self.item_tensor = item_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.user_tensor[index], self.item_tensor[index], self.target_tensor[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.user_tensor.size(0)\n",
    "\n",
    "\n",
    "class SampleGenerator(object):\n",
    "    def __init__(self, ratings):\n",
    "        assert 'userId' in ratings.columns\n",
    "        assert 'itemId' in ratings.columns\n",
    "        assert 'rating' in ratings.columns\n",
    "\n",
    "        self.ratings = ratings\n",
    "        # explicit feedback using _normalize and implicit using _binarize\n",
    "        # self.preprocess_ratings = self._normalize(ratings)\n",
    "        self.preprocess_ratings = self._binarize(ratings)\n",
    "        self.user_pool = set(self.ratings['userId'].unique())\n",
    "        self.item_pool = set(self.ratings['itemId'].unique())\n",
    "        # create negative item samples for NCF learning\n",
    "        self.negatives = self._sample_negative(ratings)\n",
    "        self.train_ratings, self.test_ratings = self._split_loo(self.preprocess_ratings)\n",
    "\n",
    "    def _normalize(self, ratings):\n",
    "        \"\"\"normalize into [0, 1] from [0, max_rating], explicit feedback\"\"\"\n",
    "        ratings = deepcopy(ratings)\n",
    "        max_rating = ratings.rating.max()\n",
    "        ratings['rating'] = ratings.rating * 1.0 / max_rating\n",
    "        return ratings\n",
    "    \n",
    "    def _binarize(self, ratings):\n",
    "        \"\"\"binarize into 0 or 1, imlicit feedback\"\"\"\n",
    "        ratings = deepcopy(ratings)\n",
    "        ratings['rating'][ratings['rating'] > 0] = 1.0\n",
    "        return ratings\n",
    "\n",
    "    def _split_loo(self, ratings):\n",
    "        \"\"\"leave one out train/test split \"\"\"\n",
    "        ratings['rank_latest'] = ratings.groupby(['userId'])['timestamp'].rank(method='first', ascending=False)\n",
    "        test = ratings[ratings['rank_latest'] == 1]\n",
    "        train = ratings[ratings['rank_latest'] > 1]\n",
    "        assert train['userId'].nunique() == test['userId'].nunique()\n",
    "        return train[['userId', 'itemId', 'rating']], test[['userId', 'itemId', 'rating']]\n",
    "\n",
    "    def _sample_negative(self, ratings):\n",
    "        \"\"\"return all negative items & 100 sampled negative items\"\"\"\n",
    "        interact_status = ratings.groupby('userId')['itemId'].apply(set).reset_index().rename(\n",
    "            columns={'itemId': 'interacted_items'})\n",
    "        interact_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: self.item_pool - x)\n",
    "        interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, 99))\n",
    "        return interact_status[['userId', 'negative_items', 'negative_samples']]\n",
    "\n",
    "    def instance_a_train_loader(self, num_negatives, batch_size):\n",
    "        \"\"\"instance train loader for one training epoch\"\"\"\n",
    "        users, items, ratings = [], [], []\n",
    "        train_ratings = pd.merge(self.train_ratings, self.negatives[['userId', 'negative_items']], on='userId')\n",
    "        train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, num_negatives))\n",
    "        for row in train_ratings.itertuples():\n",
    "            users.append(int(row.userId))\n",
    "            items.append(int(row.itemId))\n",
    "            ratings.append(float(row.rating))\n",
    "            for i in range(num_negatives):\n",
    "                users.append(int(row.userId))\n",
    "                items.append(int(row.negatives[i]))\n",
    "                ratings.append(float(0))  # negative samples get 0 rating\n",
    "        dataset = UserItemRatingDataset(user_tensor=torch.LongTensor(users),\n",
    "                                        item_tensor=torch.LongTensor(items),\n",
    "                                        target_tensor=torch.FloatTensor(ratings))\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    @property\n",
    "    def evaluate_data(self):\n",
    "        \"\"\"create evaluate data\"\"\"\n",
    "        test_ratings = pd.merge(self.test_ratings, self.negatives[['userId', 'negative_samples']], on='userId')\n",
    "        test_users, test_items, negative_users, negative_items = [], [], [], []\n",
    "        for row in test_ratings.itertuples():\n",
    "            test_users.append(int(row.userId))\n",
    "            test_items.append(int(row.itemId))\n",
    "            for i in range(len(row.negative_samples)):\n",
    "                negative_users.append(int(row.userId))\n",
    "                negative_items.append(int(row.negative_samples[i]))\n",
    "        return [torch.LongTensor(test_users), torch.LongTensor(test_items), torch.LongTensor(negative_users),\n",
    "                torch.LongTensor(negative_items)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T11:32:50.684680Z",
     "iopub.status.busy": "2024-05-22T11:32:50.684524Z",
     "iopub.status.idle": "2024-05-22T11:32:50.690161Z",
     "shell.execute_reply": "2024-05-22T11:32:50.689551Z",
     "shell.execute_reply.started": "2024-05-22T11:32:50.684663Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checkpoints\n",
    "def save_checkpoint(model, model_dir):\n",
    "    torch.save(model.state_dict(), model_dir)\n",
    "\n",
    "\n",
    "def resume_checkpoint(model, model_dir, device_id):\n",
    "    state_dict = torch.load(model_dir,\n",
    "                            map_location=lambda storage, loc: storage.cuda(device=device_id))  # ensure all storage are on gpu\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "# Hyper params\n",
    "def use_cuda(enabled, device_id=0):\n",
    "    if enabled:\n",
    "        assert torch.cuda.is_available(), 'CUDA is not available'\n",
    "        torch.cuda.set_device(device_id)\n",
    "\n",
    "\n",
    "def use_optimizer(network, params):\n",
    "    if params['optimizer'] == 'sgd':\n",
    "        optimizer = torch.optim.SGD(network.parameters(),\n",
    "                                    lr=params['sgd_lr'],\n",
    "                                    momentum=params['sgd_momentum'],\n",
    "                                    weight_decay=params['l2_regularization'])\n",
    "    elif params['optimizer'] == 'adam':\n",
    "        optimizer = torch.optim.Adam(network.parameters(), \n",
    "                                                          lr=params['adam_lr'],\n",
    "                                                          weight_decay=params['l2_regularization'])\n",
    "    elif params['optimizer'] == 'rmsprop':\n",
    "        optimizer = torch.optim.RMSprop(network.parameters(),\n",
    "                                        lr=params['rmsprop_lr'],\n",
    "                                        alpha=params['rmsprop_alpha'],\n",
    "                                        momentum=params['rmsprop_momentum'])\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T11:32:50.690951Z",
     "iopub.status.busy": "2024-05-22T11:32:50.690803Z",
     "iopub.status.idle": "2024-05-22T11:32:50.698406Z",
     "shell.execute_reply": "2024-05-22T11:32:50.697810Z",
     "shell.execute_reply.started": "2024-05-22T11:32:50.690934Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MetronAtK(object):\n",
    "    def __init__(self, top_k):\n",
    "        self._top_k = top_k\n",
    "        self._subjects = None  # Subjects which we ran evaluation on\n",
    "\n",
    "    @property\n",
    "    def top_k(self):\n",
    "        return self._top_k\n",
    "\n",
    "    @top_k.setter\n",
    "    def top_k(self, top_k):\n",
    "        self._top_k = top_k\n",
    "\n",
    "    @property\n",
    "    def subjects(self):\n",
    "        return self._subjects\n",
    "\n",
    "    @subjects.setter\n",
    "    def subjects(self, subjects):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            subjects: list, [test_users, test_items, test_scores, negative users, negative items, negative scores]\n",
    "        \"\"\"\n",
    "        assert isinstance(subjects, list)\n",
    "        test_users, test_items, test_scores = subjects[0], subjects[1], subjects[2]\n",
    "        neg_users, neg_items, neg_scores = subjects[3], subjects[4], subjects[5]\n",
    "        # the golden set\n",
    "        test = pd.DataFrame({'user': test_users,\n",
    "                             'test_item': test_items,\n",
    "                             'test_score': test_scores})\n",
    "        # the full set\n",
    "        full = pd.DataFrame({'user': neg_users + test_users,\n",
    "                            'item': neg_items + test_items,\n",
    "                            'score': neg_scores + test_scores})\n",
    "        full = pd.merge(full, test, on=['user'], how='left')\n",
    "        # rank the items according to the scores for each user\n",
    "        full['rank'] = full.groupby('user')['score'].rank(method='first', ascending=False)\n",
    "        full.sort_values(['user', 'rank'], inplace=True)\n",
    "        self._subjects = full\n",
    "\n",
    "    def cal_hit_ratio(self):\n",
    "        \"\"\"Hit Ratio @ top_K\"\"\"\n",
    "        full, top_k = self._subjects, self._top_k\n",
    "        top_k = full[full['rank']<=top_k]\n",
    "        test_in_top_k =top_k[top_k['test_item'] == top_k['item']]  # golden items hit in the top_K items\n",
    "        return len(test_in_top_k) * 1.0 / full['user'].nunique()\n",
    "\n",
    "    def cal_ndcg(self):\n",
    "        full, top_k = self._subjects, self._top_k\n",
    "        top_k = full[full['rank']<=top_k].copy()\n",
    "        test_in_top_k =top_k[top_k['test_item'] == top_k['item']].copy()\n",
    "        test_in_top_k.loc[:, 'ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x)) # the rank starts from 1\n",
    "        return test_in_top_k['ndcg'].sum() * 1.0 / full['user'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T11:32:50.699219Z",
     "iopub.status.busy": "2024-05-22T11:32:50.699047Z",
     "iopub.status.idle": "2024-05-22T11:32:50.709973Z",
     "shell.execute_reply": "2024-05-22T11:32:50.709246Z",
     "shell.execute_reply.started": "2024-05-22T11:32:50.699180Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Engine(object):\n",
    "    \"\"\"Meta Engine for training & evaluating NCF model\n",
    "\n",
    "    Note: Subclass should implement self.model !\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config  # model configuration\n",
    "        self._metron = MetronAtK(top_k=10)\n",
    "        self._writer = SummaryWriter(log_dir='runs/{}'.format(config['alias']))  # tensorboard writer\n",
    "        self._writer.add_text('config', str(config), 0)\n",
    "        self.opt = use_optimizer(self.model, config)\n",
    "        # explicit feedback\n",
    "        # self.crit = torch.nn.MSELoss()\n",
    "        # implicit feedback\n",
    "        self.crit = torch.nn.BCELoss()\n",
    "\n",
    "    def train_single_batch(self, users, items, ratings):\n",
    "        assert hasattr(self, 'model'), 'Please specify the exact model !'\n",
    "        if self.config['use_cuda'] is True:\n",
    "            users, items, ratings = users.cuda(), items.cuda(), ratings.cuda()\n",
    "        self.opt.zero_grad()\n",
    "        ratings_pred = self.model(users, items)\n",
    "        loss = self.crit(ratings_pred.view(-1), ratings)\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "        loss = loss.item()\n",
    "        return loss\n",
    "\n",
    "    def train_an_epoch(self, train_loader, epoch_id):\n",
    "        assert hasattr(self, 'model'), 'Please specify the exact model !'\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for batch_id, batch in enumerate(train_loader):\n",
    "            assert isinstance(batch[0], torch.LongTensor)\n",
    "            user, item, rating = batch[0], batch[1], batch[2]\n",
    "            rating = rating.float()\n",
    "            loss = self.train_single_batch(user, item, rating)\n",
    "            if batch_id % 2500 == 0:\n",
    "                print('[Training Epoch {}] Batch {}, Loss {}'.format(epoch_id, batch_id, loss))\n",
    "            total_loss += loss\n",
    "        self._writer.add_scalar('model/loss', total_loss, epoch_id)\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, evaluate_data, epoch_id):\n",
    "        assert hasattr(self, 'model'), 'Please specify the exact model !'\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_users, test_items = evaluate_data[0], evaluate_data[1]\n",
    "            negative_users, negative_items = evaluate_data[2], evaluate_data[3]\n",
    "            if self.config['use_cuda'] is True:\n",
    "                test_users = test_users.cuda()\n",
    "                test_items = test_items.cuda()\n",
    "                negative_users = negative_users.cuda()\n",
    "                negative_items = negative_items.cuda()\n",
    "            test_scores = self.model(test_users, test_items)\n",
    "            negative_scores = self.model(negative_users, negative_items)\n",
    "            if self.config['use_cuda'] is True:\n",
    "                test_users = test_users.cpu()\n",
    "                test_items = test_items.cpu()\n",
    "                test_scores = test_scores.cpu()\n",
    "                negative_users = negative_users.cpu()\n",
    "                negative_items = negative_items.cpu()\n",
    "                negative_scores = negative_scores.cpu()\n",
    "            self._metron.subjects = [test_users.data.view(-1).tolist(),\n",
    "                                 test_items.data.view(-1).tolist(),\n",
    "                                 test_scores.data.view(-1).tolist(),\n",
    "                                 negative_users.data.view(-1).tolist(),\n",
    "                                 negative_items.data.view(-1).tolist(),\n",
    "                                 negative_scores.data.view(-1).tolist()]\n",
    "        hit_ratio, ndcg = self._metron.cal_hit_ratio(), self._metron.cal_ndcg()\n",
    "        self._writer.add_scalar('performance/HR', hit_ratio, epoch_id)\n",
    "        self._writer.add_scalar('performance/NDCG', ndcg, epoch_id)\n",
    "        print('[Evluating Epoch {}] HR = {:.4f}, NDCG = {:.4f}'.format(epoch_id, hit_ratio, ndcg))\n",
    "        return hit_ratio, ndcg\n",
    "\n",
    "    def save(self, alias, epoch_id, hit_ratio, ndcg):\n",
    "        assert hasattr(self, 'model'), 'Please specify the exact model !'\n",
    "        model_dir = self.config['model_dir'].format(alias, epoch_id, hit_ratio, ndcg)\n",
    "        save_checkpoint(self.model, model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import GMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T11:32:50.710857Z",
     "iopub.status.busy": "2024-05-22T11:32:50.710707Z",
     "iopub.status.idle": "2024-05-22T11:32:50.716609Z",
     "shell.execute_reply": "2024-05-22T11:32:50.715921Z",
     "shell.execute_reply.started": "2024-05-22T11:32:50.710841Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GMF(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GMF, self).__init__()\n",
    "        self.num_users = config['num_users']\n",
    "        self.num_items = config['num_items']\n",
    "        self.latent_dim = config['latent_dim']\n",
    "\n",
    "        self.embedding_user = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim)\n",
    "        self.embedding_item = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim)\n",
    "\n",
    "        self.affine_output = torch.nn.Linear(in_features=self.latent_dim, out_features=1)\n",
    "        self.logistic = torch.nn.Sigmoid()\n",
    "\n",
    "        # Initialize model parameters with a Gaussian distribution (with a mean of 0 and standard deviation of 0.01)\n",
    "        if config['weight_init_gaussian']:\n",
    "            for sm in self.modules():\n",
    "                if isinstance(sm, (nn.Embedding, nn.Linear)):\n",
    "                    print(sm)\n",
    "                    torch.nn.init.normal_(sm.weight.data, 0.0, 0.01)\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embedding = self.embedding_user(user_indices)\n",
    "        item_embedding = self.embedding_item(item_indices)\n",
    "        element_product = torch.mul(user_embedding, item_embedding)\n",
    "        logits = self.affine_output(element_product)\n",
    "        rating = self.logistic(logits)\n",
    "        return rating\n",
    "\n",
    "    def init_weight(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T11:32:50.718445Z",
     "iopub.status.busy": "2024-05-22T11:32:50.718291Z",
     "iopub.status.idle": "2024-05-22T11:32:50.722070Z",
     "shell.execute_reply": "2024-05-22T11:32:50.721470Z",
     "shell.execute_reply.started": "2024-05-22T11:32:50.718428Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GMFEngine(Engine):\n",
    "    \"\"\"Engine for training & evaluating GMF model\"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.model = GMF(config)\n",
    "        if config['use_cuda'] is True:\n",
    "            use_cuda(True, config['device_id'])\n",
    "            self.model.cuda()\n",
    "        super(GMFEngine, self).__init__(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T11:32:50.722835Z",
     "iopub.status.busy": "2024-05-22T11:32:50.722689Z",
     "iopub.status.idle": "2024-05-22T11:32:50.730573Z",
     "shell.execute_reply": "2024-05-22T11:32:50.729973Z",
     "shell.execute_reply.started": "2024-05-22T11:32:50.722819Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MLP, self).__init__()\n",
    "        self.config = config\n",
    "        self.num_users = config['num_users']\n",
    "        self.num_items = config['num_items']\n",
    "        self.latent_dim = config['latent_dim']\n",
    "\n",
    "        self.embedding_user = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim)\n",
    "        self.embedding_item = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim)\n",
    "\n",
    "        self.fc_layers = torch.nn.ModuleList()\n",
    "        for idx, (in_size, out_size) in enumerate(zip(config['layers'][:-1], config['layers'][1:])):\n",
    "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
    "\n",
    "        self.affine_output = torch.nn.Linear(in_features=config['layers'][-1], out_features=1)\n",
    "        self.logistic = torch.nn.Sigmoid()\n",
    "\n",
    "        # Initialize model parameters with a Gaussian distribution (with a mean of 0 and standard deviation of 0.01)\n",
    "        if config['weight_init_gaussian']:\n",
    "            for sm in self.modules():\n",
    "                if isinstance(sm, (nn.Embedding, nn.Linear)):\n",
    "                    print(sm)\n",
    "                    torch.nn.init.normal_(sm.weight.data, 0.0, 0.01)\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embedding = self.embedding_user(user_indices)\n",
    "        item_embedding = self.embedding_item(item_indices)\n",
    "        vector = torch.cat([user_embedding, item_embedding], dim=-1)  # the concat latent vector\n",
    "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
    "            vector = self.fc_layers[idx](vector)\n",
    "            vector = torch.nn.ReLU()(vector)\n",
    "            # vector = torch.nn.BatchNorm1d()(vector)\n",
    "            # vector = torch.nn.Dropout(p=0.5)(vector)\n",
    "        logits = self.affine_output(vector)\n",
    "        rating = self.logistic(logits)\n",
    "        return rating\n",
    "\n",
    "    def init_weight(self):\n",
    "        pass\n",
    "\n",
    "    def load_pretrain_weights(self):\n",
    "        \"\"\"Loading weights from trained GMF model\"\"\"\n",
    "        config = self.config\n",
    "        gmf_model = GMF(config)\n",
    "        if config['use_cuda'] is True:\n",
    "            gmf_model.cuda()\n",
    "        resume_checkpoint(gmf_model, model_dir=config['pretrain_mf'], device_id=config['device_id'])\n",
    "        self.embedding_user.weight.data = gmf_model.embedding_user.weight.data\n",
    "        self.embedding_item.weight.data = gmf_model.embedding_item.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T11:32:50.731425Z",
     "iopub.status.busy": "2024-05-22T11:32:50.731269Z",
     "iopub.status.idle": "2024-05-22T11:32:50.735222Z",
     "shell.execute_reply": "2024-05-22T11:32:50.734619Z",
     "shell.execute_reply.started": "2024-05-22T11:32:50.731408Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLPEngine(Engine):\n",
    "    \"\"\"Engine for training & evaluating GMF model\"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.model = MLP(config)\n",
    "        if config['use_cuda'] is True:\n",
    "            use_cuda(True, config['device_id'])\n",
    "            self.model.cuda()\n",
    "        super(MLPEngine, self).__init__(config)\n",
    "        print(self.model)\n",
    "\n",
    "        if config['pretrain']:\n",
    "            self.model.load_pretrain_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T11:32:50.736006Z",
     "iopub.status.busy": "2024-05-22T11:32:50.735862Z",
     "iopub.status.idle": "2024-05-22T11:32:50.747171Z",
     "shell.execute_reply": "2024-05-22T11:32:50.746718Z",
     "shell.execute_reply.started": "2024-05-22T11:32:50.735989Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuMF(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(NeuMF, self).__init__()\n",
    "        self.config = config\n",
    "        self.num_users = config['num_users']\n",
    "        self.num_items = config['num_items']\n",
    "        self.latent_dim_mf = config['latent_dim_mf']\n",
    "        self.latent_dim_mlp = config['latent_dim_mlp']\n",
    "\n",
    "        self.embedding_user_mlp = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim_mlp)\n",
    "        self.embedding_item_mlp = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim_mlp)\n",
    "        self.embedding_user_mf = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim_mf)\n",
    "        self.embedding_item_mf = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim_mf)\n",
    "\n",
    "        self.fc_layers = torch.nn.ModuleList()\n",
    "        for idx, (in_size, out_size) in enumerate(zip(config['layers'][:-1], config['layers'][1:])):\n",
    "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
    "\n",
    "        self.affine_output = torch.nn.Linear(in_features=config['layers'][-1] + config['latent_dim_mf'], out_features=1)\n",
    "        self.logistic = torch.nn.Sigmoid()\n",
    "\n",
    "        # Initialize model parameters with a Gaussian distribution (with a mean of 0 and standard deviation of 0.01)\n",
    "        if config['weight_init_gaussian']:\n",
    "            for sm in self.modules():\n",
    "                if isinstance(sm, (nn.Embedding, nn.Linear)):\n",
    "                    print(sm)\n",
    "                    torch.nn.init.normal_(sm.weight.data, 0.0, 0.01)\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embedding_mlp = self.embedding_user_mlp(user_indices)\n",
    "        item_embedding_mlp = self.embedding_item_mlp(item_indices)\n",
    "        user_embedding_mf = self.embedding_user_mf(user_indices)\n",
    "        item_embedding_mf = self.embedding_item_mf(item_indices)\n",
    "\n",
    "        mlp_vector = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1)  # the concat latent vector\n",
    "        mf_vector =torch.mul(user_embedding_mf, item_embedding_mf)\n",
    "\n",
    "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
    "            mlp_vector = self.fc_layers[idx](mlp_vector)\n",
    "            mlp_vector = torch.nn.ReLU()(mlp_vector)\n",
    "\n",
    "        vector = torch.cat([mlp_vector, mf_vector], dim=-1)\n",
    "        logits = self.affine_output(vector)\n",
    "        rating = self.logistic(logits)\n",
    "        return rating\n",
    "\n",
    "    def init_weight(self):\n",
    "        pass\n",
    "\n",
    "    def load_pretrain_weights(self):\n",
    "        \"\"\"Loading weights from trained MLP model & GMF model\"\"\"\n",
    "        config = self.config\n",
    "        config['latent_dim'] = config['latent_dim_mlp']\n",
    "        mlp_model = MLP(config)\n",
    "        if config['use_cuda'] is True:\n",
    "            mlp_model.cuda()\n",
    "        resume_checkpoint(mlp_model, model_dir=config['pretrain_mlp'], device_id=config['device_id'])\n",
    "\n",
    "        self.embedding_user_mlp.weight.data = mlp_model.embedding_user.weight.data\n",
    "        self.embedding_item_mlp.weight.data = mlp_model.embedding_item.weight.data\n",
    "        for idx in range(len(self.fc_layers)):\n",
    "            self.fc_layers[idx].weight.data = mlp_model.fc_layers[idx].weight.data\n",
    "\n",
    "        config['latent_dim'] = config['latent_dim_mf']\n",
    "        gmf_model = GMF(config)\n",
    "        if config['use_cuda'] is True:\n",
    "            gmf_model.cuda()\n",
    "        resume_checkpoint(gmf_model, model_dir=config['pretrain_mf'], device_id=config['device_id'])\n",
    "        self.embedding_user_mf.weight.data = gmf_model.embedding_user.weight.data\n",
    "        self.embedding_item_mf.weight.data = gmf_model.embedding_item.weight.data\n",
    "\n",
    "        self.affine_output.weight.data = 0.5 * torch.cat([mlp_model.affine_output.weight.data, gmf_model.affine_output.weight.data], dim=-1)\n",
    "        self.affine_output.bias.data = 0.5 * (mlp_model.affine_output.bias.data + gmf_model.affine_output.bias.data)\n",
    "\n",
    "\n",
    "class NeuMFEngine(Engine):\n",
    "    \"\"\"Engine for training & evaluating GMF model\"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.model = NeuMF(config)\n",
    "        if config['use_cuda'] is True:\n",
    "            use_cuda(True, config['device_id'])\n",
    "            self.model.cuda()\n",
    "        super(NeuMFEngine, self).__init__(config)\n",
    "        print(self.model)\n",
    "\n",
    "        if config['pretrain']:\n",
    "            self.model.load_pretrain_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T11:32:50.748027Z",
     "iopub.status.busy": "2024-05-22T11:32:50.747871Z",
     "iopub.status.idle": "2024-05-22T11:32:50.752904Z",
     "shell.execute_reply": "2024-05-22T11:32:50.752460Z",
     "shell.execute_reply.started": "2024-05-22T11:32:50.748011Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gmf_config = {'alias': 'gmf_factor8neg4-implict',\n",
    "              'num_epoch': 20,\n",
    "              'batch_size': 1024,\n",
    "              'optimizer': 'adam',\n",
    "              'adam_lr': 1e-3,\n",
    "              'num_users': 6040,\n",
    "              'num_items': 3706,\n",
    "              'latent_dim': 8,\n",
    "              'num_negative': 4,\n",
    "              'l2_regularization': 0,  # 0.01\n",
    "              'weight_init_gaussian': True,\n",
    "              'use_cuda': True,\n",
    "              'device_id': 0,\n",
    "              'model_dir': 'checkpoints/{}_Epoch{}_HR{:.4f}_NDCG{:.4f}.model'}\n",
    "\n",
    "mlp_config = {'alias': 'mlp_factor8neg4_bz256_166432168_pretrain_reg_0.0000001',\n",
    "              'num_epoch': 20,\n",
    "              'batch_size': 256,  # 1024,\n",
    "              'optimizer': 'adam',\n",
    "              'adam_lr': 1e-3,\n",
    "              'num_users': 6040,\n",
    "              'num_items': 3706,\n",
    "              'latent_dim': 8,\n",
    "              'num_negative': 4,\n",
    "              'layers': [16, 64, 32, 16, 8], \n",
    "              'l2_regularization': 0.0000001,  # MLP model is sensitive to hyper params\n",
    "              'weight_init_gaussian': True,\n",
    "              'use_cuda': True,\n",
    "              'device_id': 0,\n",
    "              'pretrain': False,\n",
    "              'pretrain_mf': 'checkpoints/{}'.format('gmf_factor8neg4_Epoch100_HR0.6391_NDCG0.2852.model'),\n",
    "              'model_dir': 'checkpoints/{}_Epoch{}_HR{:.4f}_NDCG{:.4f}.model'}\n",
    "\n",
    "neumf_config = {'alias': 'neumf_factor8neg4',\n",
    "                'num_epoch': 20,\n",
    "                'batch_size': 1024,\n",
    "                'optimizer': 'adam',\n",
    "                'adam_lr': 1e-3,\n",
    "                'num_users': 6040,\n",
    "                'num_items': 3706,\n",
    "                'latent_dim_mf': 8,\n",
    "                'latent_dim_mlp': 8,\n",
    "                'num_negative': 4,\n",
    "                'layers': [16, 64, 32, 16, 8],  \n",
    "                'l2_regularization': 0.0000001,\n",
    "                'weight_init_gaussian': True,\n",
    "                'use_cuda': True,\n",
    "                'device_id': 0,\n",
    "                'pretrain': False,\n",
    "                'pretrain_mf': 'checkpoints/{}'.format('gmf_factor8neg4_Epoch100_HR0.6391_NDCG0.2852.model'),\n",
    "                'pretrain_mlp': 'checkpoints/{}'.format('mlp_factor8neg4_Epoch100_HR0.5606_NDCG0.2463.model'),\n",
    "                'model_dir': 'checkpoints/{}_Epoch{}_HR{:.4f}_NDCG{:.4f}.model'\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T11:32:50.753593Z",
     "iopub.status.busy": "2024-05-22T11:32:50.753448Z",
     "iopub.status.idle": "2024-05-22T11:32:55.518979Z",
     "shell.execute_reply": "2024-05-22T11:32:55.518082Z",
     "shell.execute_reply.started": "2024-05-22T11:32:50.753577Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of userId is [0, 6039]\n",
      "Range of itemId is [0, 3705]\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "ml1m_dir = '/openbayes/input/input0/ratings.dat'\n",
    "ml1m_rating = pd.read_csv(ml1m_dir, sep='::', header=None, names=['uid', 'mid', 'rating', 'timestamp'], engine='python')\n",
    "# Reindex\n",
    "user_id = ml1m_rating[['uid']].drop_duplicates().reindex()\n",
    "user_id['userId'] = np.arange(len(user_id))\n",
    "ml1m_rating = pd.merge(ml1m_rating, user_id, on=['uid'], how='left')\n",
    "item_id = ml1m_rating[['mid']].drop_duplicates()\n",
    "item_id['itemId'] = np.arange(len(item_id))\n",
    "ml1m_rating = pd.merge(ml1m_rating, item_id, on=['mid'], how='left')\n",
    "ml1m_rating = ml1m_rating[['userId', 'itemId', 'rating', 'timestamp']]\n",
    "print('Range of userId is [{}, {}]'.format(ml1m_rating.userId.min(), ml1m_rating.userId.max()))\n",
    "print('Range of itemId is [{}, {}]'.format(ml1m_rating.itemId.min(), ml1m_rating.itemId.max()))\n",
    "# DataLoader for training\n",
    "sample_generator = SampleGenerator(ratings=ml1m_rating)\n",
    "evaluate_data = sample_generator.evaluate_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the exact model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T11:32:55.520152Z",
     "iopub.status.busy": "2024-05-22T11:32:55.519979Z",
     "iopub.status.idle": "2024-05-22T11:32:55.523200Z",
     "shell.execute_reply": "2024-05-22T11:32:55.522664Z",
     "shell.execute_reply.started": "2024-05-22T11:32:55.520133Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = range(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T11:32:55.524248Z",
     "iopub.status.busy": "2024-05-22T11:32:55.523880Z",
     "iopub.status.idle": "2024-05-22T11:59:10.184857Z",
     "shell.execute_reply": "2024-05-22T11:59:10.183831Z",
     "shell.execute_reply.started": "2024-05-22T11:32:55.524229Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=8, out_features=1, bias=True)\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.7182995080947876\n",
      "[Training Epoch 0] Batch 2500, Loss 0.3785736560821533\n",
      "[Evluating Epoch 0] HR = 0.4502, NDCG = 0.2522\n",
      "After epoch 0:\n",
      "Hit Ratio: 0.4502\n",
      "NDCG: 0.2522\n",
      "Training Loss: 0.3709\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.31364208459854126\n",
      "[Training Epoch 1] Batch 2500, Loss 0.34285491704940796\n",
      "[Evluating Epoch 1] HR = 0.4925, NDCG = 0.2713\n",
      "After epoch 1:\n",
      "Hit Ratio: 0.4925\n",
      "NDCG: 0.2713\n",
      "Training Loss: 0.3233\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.34381455183029175\n",
      "[Training Epoch 2] Batch 2500, Loss 0.32451707124710083\n",
      "[Evluating Epoch 2] HR = 0.5341, NDCG = 0.2998\n",
      "After epoch 2:\n",
      "Hit Ratio: 0.5341\n",
      "NDCG: 0.2998\n",
      "Training Loss: 0.3029\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.31199246644973755\n",
      "[Training Epoch 3] Batch 2500, Loss 0.2761942148208618\n",
      "[Evluating Epoch 3] HR = 0.5745, NDCG = 0.3239\n",
      "After epoch 3:\n",
      "Hit Ratio: 0.5745\n",
      "NDCG: 0.3239\n",
      "Training Loss: 0.2798\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.28892451524734497\n",
      "[Training Epoch 4] Batch 2500, Loss 0.2666233479976654\n",
      "[Evluating Epoch 4] HR = 0.5970, NDCG = 0.3388\n",
      "After epoch 4:\n",
      "Hit Ratio: 0.5970\n",
      "NDCG: 0.3388\n",
      "Training Loss: 0.3338\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.28633546829223633\n",
      "[Training Epoch 5] Batch 2500, Loss 0.2739698886871338\n",
      "[Evluating Epoch 5] HR = 0.6036, NDCG = 0.3440\n",
      "After epoch 5:\n",
      "Hit Ratio: 0.6036\n",
      "NDCG: 0.3440\n",
      "Training Loss: 0.2735\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.2769215703010559\n",
      "[Training Epoch 6] Batch 2500, Loss 0.26509857177734375\n",
      "[Evluating Epoch 6] HR = 0.6096, NDCG = 0.3468\n",
      "After epoch 6:\n",
      "Hit Ratio: 0.6096\n",
      "NDCG: 0.3468\n",
      "Training Loss: 0.2956\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.2872273921966553\n",
      "[Training Epoch 7] Batch 2500, Loss 0.28001612424850464\n",
      "[Evluating Epoch 7] HR = 0.6189, NDCG = 0.3504\n",
      "After epoch 7:\n",
      "Hit Ratio: 0.6189\n",
      "NDCG: 0.3504\n",
      "Training Loss: 0.3014\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.263732373714447\n",
      "[Training Epoch 8] Batch 2500, Loss 0.3013983964920044\n",
      "[Evluating Epoch 8] HR = 0.6328, NDCG = 0.3590\n",
      "After epoch 8:\n",
      "Hit Ratio: 0.6328\n",
      "NDCG: 0.3590\n",
      "Training Loss: 0.2375\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.2698872685432434\n",
      "[Training Epoch 9] Batch 2500, Loss 0.26284512877464294\n",
      "[Evluating Epoch 9] HR = 0.6358, NDCG = 0.3601\n",
      "After epoch 9:\n",
      "Hit Ratio: 0.6358\n",
      "NDCG: 0.3601\n",
      "Training Loss: 0.2908\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.26121988892555237\n",
      "[Training Epoch 10] Batch 2500, Loss 0.30853569507598877\n",
      "[Evluating Epoch 10] HR = 0.6391, NDCG = 0.3645\n",
      "After epoch 10:\n",
      "Hit Ratio: 0.6391\n",
      "NDCG: 0.3645\n",
      "Training Loss: 0.3035\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.250521183013916\n",
      "[Training Epoch 11] Batch 2500, Loss 0.2682413160800934\n",
      "[Evluating Epoch 11] HR = 0.6425, NDCG = 0.3670\n",
      "After epoch 11:\n",
      "Hit Ratio: 0.6425\n",
      "NDCG: 0.3670\n",
      "Training Loss: 0.2508\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.24908578395843506\n",
      "[Training Epoch 12] Batch 2500, Loss 0.2637776732444763\n",
      "[Evluating Epoch 12] HR = 0.6442, NDCG = 0.3675\n",
      "After epoch 12:\n",
      "Hit Ratio: 0.6442\n",
      "NDCG: 0.3675\n",
      "Training Loss: 0.2843\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.2600674331188202\n",
      "[Training Epoch 13] Batch 2500, Loss 0.2776997685432434\n",
      "[Evluating Epoch 13] HR = 0.6444, NDCG = 0.3702\n",
      "After epoch 13:\n",
      "Hit Ratio: 0.6444\n",
      "NDCG: 0.3702\n",
      "Training Loss: 0.2587\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.2751123011112213\n",
      "[Training Epoch 14] Batch 2500, Loss 0.2990480065345764\n",
      "[Evluating Epoch 14] HR = 0.6414, NDCG = 0.3670\n",
      "After epoch 14:\n",
      "Hit Ratio: 0.6414\n",
      "NDCG: 0.3670\n",
      "Training Loss: 0.3227\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.24657993018627167\n",
      "[Training Epoch 15] Batch 2500, Loss 0.2473677396774292\n",
      "[Evluating Epoch 15] HR = 0.6429, NDCG = 0.3685\n",
      "After epoch 15:\n",
      "Hit Ratio: 0.6429\n",
      "NDCG: 0.3685\n",
      "Training Loss: 0.2760\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.27388232946395874\n",
      "[Training Epoch 16] Batch 2500, Loss 0.27876734733581543\n",
      "[Evluating Epoch 16] HR = 0.6452, NDCG = 0.3701\n",
      "After epoch 16:\n",
      "Hit Ratio: 0.6452\n",
      "NDCG: 0.3701\n",
      "Training Loss: 0.2358\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.23749256134033203\n",
      "[Training Epoch 17] Batch 2500, Loss 0.25667130947113037\n",
      "[Evluating Epoch 17] HR = 0.6439, NDCG = 0.3713\n",
      "After epoch 17:\n",
      "Hit Ratio: 0.6439\n",
      "NDCG: 0.3713\n",
      "Training Loss: 0.2864\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.25773292779922485\n",
      "[Training Epoch 18] Batch 2500, Loss 0.2856920659542084\n",
      "[Evluating Epoch 18] HR = 0.6503, NDCG = 0.3709\n",
      "After epoch 18:\n",
      "Hit Ratio: 0.6503\n",
      "NDCG: 0.3709\n",
      "Training Loss: 0.2811\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.2935415506362915\n",
      "[Training Epoch 19] Batch 2500, Loss 0.2543186545372009\n",
      "[Evluating Epoch 19] HR = 0.6483, NDCG = 0.3701\n",
      "After epoch 19:\n",
      "Hit Ratio: 0.6483\n",
      "NDCG: 0.3701\n",
      "Training Loss: 0.3147\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "config = gmf_config\n",
    "engine = GMFEngine(config)\n",
    "GMF_Model = {'hit_ratio': [], 'ndcg': [], 'training_loss': []}\n",
    "for epoch in range(config['num_epoch']):\n",
    "    print('Epoch {} starts !'.format(epoch))\n",
    "    print('-' * 80)\n",
    "    train_loader = sample_generator.instance_a_train_loader(config['num_negative'], config['batch_size'])\n",
    "    last_batch_loss = engine.train_an_epoch(train_loader, epoch_id=epoch)\n",
    "    \n",
    "    # Evaluate the model after each epoch\n",
    "    hit_ratio, ndcg = engine.evaluate(evaluate_data, epoch_id=epoch)\n",
    "    GMF_Model['hit_ratio'].append(hit_ratio)\n",
    "    GMF_Model['ndcg'].append(ndcg)\n",
    "    GMF_Model['training_loss'].append(last_batch_loss)\n",
    "    # Print the evaluation metrics\n",
    "    print('After epoch {}:'.format(epoch))\n",
    "    print('Hit Ratio: {:.4f}'.format(hit_ratio))\n",
    "    print('NDCG: {:.4f}'.format(ndcg))\n",
    "    print('Training Loss: {:.4f}'.format(last_batch_loss))\n",
    "    print('-' * 80)                                \n",
    "    \n",
    "    # Save the model after each epoch\n",
    "    engine.save(config['alias'], epoch, hit_ratio, ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T11:59:10.186687Z",
     "iopub.status.busy": "2024-05-22T11:59:10.186246Z",
     "iopub.status.idle": "2024-05-22T12:29:49.644637Z",
     "shell.execute_reply": "2024-05-22T12:29:49.644141Z",
     "shell.execute_reply.started": "2024-05-22T11:59:10.186646Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=16, bias=True)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=8, out_features=1, bias=True)\n",
      "MLP(\n",
      "  (embedding_user): Embedding(6040, 8)\n",
      "  (embedding_item): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.637165904045105\n",
      "[Training Epoch 0] Batch 2500, Loss 0.4043534994125366\n",
      "[Training Epoch 0] Batch 5000, Loss 0.3332201838493347\n",
      "[Training Epoch 0] Batch 7500, Loss 0.375190407037735\n",
      "[Training Epoch 0] Batch 10000, Loss 0.33535894751548767\n",
      "[Training Epoch 0] Batch 12500, Loss 0.35061195492744446\n",
      "[Training Epoch 0] Batch 15000, Loss 0.3845532536506653\n",
      "[Training Epoch 0] Batch 17500, Loss 0.37183380126953125\n",
      "[Evluating Epoch 0] HR = 0.4457, NDCG = 0.2413\n",
      "After epoch 0:\n",
      "Hit Ratio: 0.4457\n",
      "NDCG: 0.2413\n",
      "Training Loss: 0.2689\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.3498937785625458\n",
      "[Training Epoch 1] Batch 2500, Loss 0.3038710951805115\n",
      "[Training Epoch 1] Batch 5000, Loss 0.3472401797771454\n",
      "[Training Epoch 1] Batch 7500, Loss 0.39789968729019165\n",
      "[Training Epoch 1] Batch 10000, Loss 0.3346123695373535\n",
      "[Training Epoch 1] Batch 12500, Loss 0.3236373960971832\n",
      "[Training Epoch 1] Batch 15000, Loss 0.3262297511100769\n",
      "[Training Epoch 1] Batch 17500, Loss 0.3293353319168091\n",
      "[Evluating Epoch 1] HR = 0.4853, NDCG = 0.2676\n",
      "After epoch 1:\n",
      "Hit Ratio: 0.4853\n",
      "NDCG: 0.2676\n",
      "Training Loss: 0.2875\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.3220018148422241\n",
      "[Training Epoch 2] Batch 2500, Loss 0.35397636890411377\n",
      "[Training Epoch 2] Batch 5000, Loss 0.3049069941043854\n",
      "[Training Epoch 2] Batch 7500, Loss 0.350869357585907\n",
      "[Training Epoch 2] Batch 10000, Loss 0.4339302182197571\n",
      "[Training Epoch 2] Batch 12500, Loss 0.3285411596298218\n",
      "[Training Epoch 2] Batch 15000, Loss 0.25310853123664856\n",
      "[Training Epoch 2] Batch 17500, Loss 0.3143134117126465\n",
      "[Evluating Epoch 2] HR = 0.5416, NDCG = 0.3019\n",
      "After epoch 2:\n",
      "Hit Ratio: 0.5416\n",
      "NDCG: 0.3019\n",
      "Training Loss: 0.3610\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.3195915222167969\n",
      "[Training Epoch 3] Batch 2500, Loss 0.32859495282173157\n",
      "[Training Epoch 3] Batch 5000, Loss 0.3051845133304596\n",
      "[Training Epoch 3] Batch 7500, Loss 0.3006887137889862\n",
      "[Training Epoch 3] Batch 10000, Loss 0.3435317277908325\n",
      "[Training Epoch 3] Batch 12500, Loss 0.29782140254974365\n",
      "[Training Epoch 3] Batch 15000, Loss 0.2547157406806946\n",
      "[Training Epoch 3] Batch 17500, Loss 0.28938519954681396\n",
      "[Evluating Epoch 3] HR = 0.5674, NDCG = 0.3155\n",
      "After epoch 3:\n",
      "Hit Ratio: 0.5674\n",
      "NDCG: 0.3155\n",
      "Training Loss: 0.2439\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.27427759766578674\n",
      "[Training Epoch 4] Batch 2500, Loss 0.3597770929336548\n",
      "[Training Epoch 4] Batch 5000, Loss 0.28319674730300903\n",
      "[Training Epoch 4] Batch 7500, Loss 0.3500126004219055\n",
      "[Training Epoch 4] Batch 10000, Loss 0.2487385869026184\n",
      "[Training Epoch 4] Batch 12500, Loss 0.3191581666469574\n",
      "[Training Epoch 4] Batch 15000, Loss 0.2742134630680084\n",
      "[Training Epoch 4] Batch 17500, Loss 0.1874777227640152\n",
      "[Evluating Epoch 4] HR = 0.5896, NDCG = 0.3279\n",
      "After epoch 4:\n",
      "Hit Ratio: 0.5896\n",
      "NDCG: 0.3279\n",
      "Training Loss: 0.2673\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.30248236656188965\n",
      "[Training Epoch 5] Batch 2500, Loss 0.2759605050086975\n",
      "[Training Epoch 5] Batch 5000, Loss 0.29013970494270325\n",
      "[Training Epoch 5] Batch 7500, Loss 0.2487841546535492\n",
      "[Training Epoch 5] Batch 10000, Loss 0.28260383009910583\n",
      "[Training Epoch 5] Batch 12500, Loss 0.3041599690914154\n",
      "[Training Epoch 5] Batch 15000, Loss 0.2915340065956116\n",
      "[Training Epoch 5] Batch 17500, Loss 0.21404379606246948\n",
      "[Evluating Epoch 5] HR = 0.5983, NDCG = 0.3364\n",
      "After epoch 5:\n",
      "Hit Ratio: 0.5983\n",
      "NDCG: 0.3364\n",
      "Training Loss: 0.2480\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.2903361916542053\n",
      "[Training Epoch 6] Batch 2500, Loss 0.2523668110370636\n",
      "[Training Epoch 6] Batch 5000, Loss 0.28813350200653076\n",
      "[Training Epoch 6] Batch 7500, Loss 0.3203276991844177\n",
      "[Training Epoch 6] Batch 10000, Loss 0.3139740526676178\n",
      "[Training Epoch 6] Batch 12500, Loss 0.33463817834854126\n",
      "[Training Epoch 6] Batch 15000, Loss 0.2212592214345932\n",
      "[Training Epoch 6] Batch 17500, Loss 0.30335140228271484\n",
      "[Evluating Epoch 6] HR = 0.6056, NDCG = 0.3401\n",
      "After epoch 6:\n",
      "Hit Ratio: 0.6056\n",
      "NDCG: 0.3401\n",
      "Training Loss: 0.3034\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.26521384716033936\n",
      "[Training Epoch 7] Batch 2500, Loss 0.2588372230529785\n",
      "[Training Epoch 7] Batch 5000, Loss 0.2857285439968109\n",
      "[Training Epoch 7] Batch 7500, Loss 0.25643694400787354\n",
      "[Training Epoch 7] Batch 10000, Loss 0.2774106562137604\n",
      "[Training Epoch 7] Batch 12500, Loss 0.3220602571964264\n",
      "[Training Epoch 7] Batch 15000, Loss 0.2577902674674988\n",
      "[Training Epoch 7] Batch 17500, Loss 0.286490797996521\n",
      "[Evluating Epoch 7] HR = 0.6147, NDCG = 0.3477\n",
      "After epoch 7:\n",
      "Hit Ratio: 0.6147\n",
      "NDCG: 0.3477\n",
      "Training Loss: 0.3277\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.3402893543243408\n",
      "[Training Epoch 8] Batch 2500, Loss 0.25490817427635193\n",
      "[Training Epoch 8] Batch 5000, Loss 0.2911131978034973\n",
      "[Training Epoch 8] Batch 7500, Loss 0.30274802446365356\n",
      "[Training Epoch 8] Batch 10000, Loss 0.2752953767776489\n",
      "[Training Epoch 8] Batch 12500, Loss 0.28907695412635803\n",
      "[Training Epoch 8] Batch 15000, Loss 0.2882154583930969\n",
      "[Training Epoch 8] Batch 17500, Loss 0.3139110803604126\n",
      "[Evluating Epoch 8] HR = 0.6156, NDCG = 0.3507\n",
      "After epoch 8:\n",
      "Hit Ratio: 0.6156\n",
      "NDCG: 0.3507\n",
      "Training Loss: 0.3340\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.2850963771343231\n",
      "[Training Epoch 9] Batch 2500, Loss 0.2899821400642395\n",
      "[Training Epoch 9] Batch 5000, Loss 0.28814947605133057\n",
      "[Training Epoch 9] Batch 7500, Loss 0.2556682527065277\n",
      "[Training Epoch 9] Batch 10000, Loss 0.2705134153366089\n",
      "[Training Epoch 9] Batch 12500, Loss 0.22133135795593262\n",
      "[Training Epoch 9] Batch 15000, Loss 0.24473339319229126\n",
      "[Training Epoch 9] Batch 17500, Loss 0.2567383944988251\n",
      "[Evluating Epoch 9] HR = 0.6247, NDCG = 0.3604\n",
      "After epoch 9:\n",
      "Hit Ratio: 0.6247\n",
      "NDCG: 0.3604\n",
      "Training Loss: 0.2799\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.2972124218940735\n",
      "[Training Epoch 10] Batch 2500, Loss 0.32534441351890564\n",
      "[Training Epoch 10] Batch 5000, Loss 0.2344575971364975\n",
      "[Training Epoch 10] Batch 7500, Loss 0.28498411178588867\n",
      "[Training Epoch 10] Batch 10000, Loss 0.26354464888572693\n",
      "[Training Epoch 10] Batch 12500, Loss 0.29620665311813354\n",
      "[Training Epoch 10] Batch 15000, Loss 0.25040432810783386\n",
      "[Training Epoch 10] Batch 17500, Loss 0.24079233407974243\n",
      "[Evluating Epoch 10] HR = 0.6298, NDCG = 0.3583\n",
      "After epoch 10:\n",
      "Hit Ratio: 0.6298\n",
      "NDCG: 0.3583\n",
      "Training Loss: 0.2916\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.20055292546749115\n",
      "[Training Epoch 11] Batch 2500, Loss 0.2693654000759125\n",
      "[Training Epoch 11] Batch 5000, Loss 0.2569495737552643\n",
      "[Training Epoch 11] Batch 7500, Loss 0.29155853390693665\n",
      "[Training Epoch 11] Batch 10000, Loss 0.24031081795692444\n",
      "[Training Epoch 11] Batch 12500, Loss 0.28246787190437317\n",
      "[Training Epoch 11] Batch 15000, Loss 0.23448313772678375\n",
      "[Training Epoch 11] Batch 17500, Loss 0.28869858384132385\n",
      "[Evluating Epoch 11] HR = 0.6325, NDCG = 0.3633\n",
      "After epoch 11:\n",
      "Hit Ratio: 0.6325\n",
      "NDCG: 0.3633\n",
      "Training Loss: 0.2977\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.28768283128738403\n",
      "[Training Epoch 12] Batch 2500, Loss 0.3121185898780823\n",
      "[Training Epoch 12] Batch 5000, Loss 0.26556092500686646\n",
      "[Training Epoch 12] Batch 7500, Loss 0.2553293704986572\n",
      "[Training Epoch 12] Batch 10000, Loss 0.2977692186832428\n",
      "[Training Epoch 12] Batch 12500, Loss 0.2126617431640625\n",
      "[Training Epoch 12] Batch 15000, Loss 0.2822609841823578\n",
      "[Training Epoch 12] Batch 17500, Loss 0.2731492519378662\n",
      "[Evluating Epoch 12] HR = 0.6411, NDCG = 0.3687\n",
      "After epoch 12:\n",
      "Hit Ratio: 0.6411\n",
      "NDCG: 0.3687\n",
      "Training Loss: 0.3121\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.2703411281108856\n",
      "[Training Epoch 13] Batch 2500, Loss 0.27270951867103577\n",
      "[Training Epoch 13] Batch 5000, Loss 0.2555219233036041\n",
      "[Training Epoch 13] Batch 7500, Loss 0.2854188084602356\n",
      "[Training Epoch 13] Batch 10000, Loss 0.2700873613357544\n",
      "[Training Epoch 13] Batch 12500, Loss 0.34640055894851685\n",
      "[Training Epoch 13] Batch 15000, Loss 0.25138556957244873\n",
      "[Training Epoch 13] Batch 17500, Loss 0.26908978819847107\n",
      "[Evluating Epoch 13] HR = 0.6399, NDCG = 0.3692\n",
      "After epoch 13:\n",
      "Hit Ratio: 0.6399\n",
      "NDCG: 0.3692\n",
      "Training Loss: 0.2958\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.26890385150909424\n",
      "[Training Epoch 14] Batch 2500, Loss 0.25958704948425293\n",
      "[Training Epoch 14] Batch 5000, Loss 0.2709149122238159\n",
      "[Training Epoch 14] Batch 7500, Loss 0.2879720628261566\n",
      "[Training Epoch 14] Batch 10000, Loss 0.2764701843261719\n",
      "[Training Epoch 14] Batch 12500, Loss 0.24767810106277466\n",
      "[Training Epoch 14] Batch 15000, Loss 0.24712921679019928\n",
      "[Training Epoch 14] Batch 17500, Loss 0.3344734013080597\n",
      "[Evluating Epoch 14] HR = 0.6404, NDCG = 0.3707\n",
      "After epoch 14:\n",
      "Hit Ratio: 0.6404\n",
      "NDCG: 0.3707\n",
      "Training Loss: 0.1821\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.201736181974411\n",
      "[Training Epoch 15] Batch 2500, Loss 0.24247345328330994\n",
      "[Training Epoch 15] Batch 5000, Loss 0.29830417037010193\n",
      "[Training Epoch 15] Batch 7500, Loss 0.258556604385376\n",
      "[Training Epoch 15] Batch 10000, Loss 0.2789195775985718\n",
      "[Training Epoch 15] Batch 12500, Loss 0.23179569840431213\n",
      "[Training Epoch 15] Batch 15000, Loss 0.2642575800418854\n",
      "[Training Epoch 15] Batch 17500, Loss 0.2735803425312042\n",
      "[Evluating Epoch 15] HR = 0.6396, NDCG = 0.3721\n",
      "After epoch 15:\n",
      "Hit Ratio: 0.6396\n",
      "NDCG: 0.3721\n",
      "Training Loss: 0.2600\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.33053338527679443\n",
      "[Training Epoch 16] Batch 2500, Loss 0.3576597571372986\n",
      "[Training Epoch 16] Batch 5000, Loss 0.27606478333473206\n",
      "[Training Epoch 16] Batch 7500, Loss 0.21114999055862427\n",
      "[Training Epoch 16] Batch 10000, Loss 0.22467157244682312\n",
      "[Training Epoch 16] Batch 12500, Loss 0.24388383328914642\n",
      "[Training Epoch 16] Batch 15000, Loss 0.3057774305343628\n",
      "[Training Epoch 16] Batch 17500, Loss 0.252041757106781\n",
      "[Evluating Epoch 16] HR = 0.6490, NDCG = 0.3753\n",
      "After epoch 16:\n",
      "Hit Ratio: 0.6490\n",
      "NDCG: 0.3753\n",
      "Training Loss: 0.1659\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.2924096882343292\n",
      "[Training Epoch 17] Batch 2500, Loss 0.23862244188785553\n",
      "[Training Epoch 17] Batch 5000, Loss 0.24664950370788574\n",
      "[Training Epoch 17] Batch 7500, Loss 0.24378691613674164\n",
      "[Training Epoch 17] Batch 10000, Loss 0.22320835292339325\n",
      "[Training Epoch 17] Batch 12500, Loss 0.2545982301235199\n",
      "[Training Epoch 17] Batch 15000, Loss 0.2455299198627472\n",
      "[Training Epoch 17] Batch 17500, Loss 0.2668052911758423\n",
      "[Evluating Epoch 17] HR = 0.6517, NDCG = 0.3764\n",
      "After epoch 17:\n",
      "Hit Ratio: 0.6517\n",
      "NDCG: 0.3764\n",
      "Training Loss: 0.1616\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.2535332143306732\n",
      "[Training Epoch 18] Batch 2500, Loss 0.28545522689819336\n",
      "[Training Epoch 18] Batch 5000, Loss 0.2151704728603363\n",
      "[Training Epoch 18] Batch 7500, Loss 0.23210254311561584\n",
      "[Training Epoch 18] Batch 10000, Loss 0.28717049956321716\n",
      "[Training Epoch 18] Batch 12500, Loss 0.29626840353012085\n",
      "[Training Epoch 18] Batch 15000, Loss 0.21399027109146118\n",
      "[Training Epoch 18] Batch 17500, Loss 0.29361647367477417\n",
      "[Evluating Epoch 18] HR = 0.6447, NDCG = 0.3754\n",
      "After epoch 18:\n",
      "Hit Ratio: 0.6447\n",
      "NDCG: 0.3754\n",
      "Training Loss: 0.3169\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.2586515545845032\n",
      "[Training Epoch 19] Batch 2500, Loss 0.2710818648338318\n",
      "[Training Epoch 19] Batch 5000, Loss 0.3069823980331421\n",
      "[Training Epoch 19] Batch 7500, Loss 0.25984564423561096\n",
      "[Training Epoch 19] Batch 10000, Loss 0.2727736234664917\n",
      "[Training Epoch 19] Batch 12500, Loss 0.2770455777645111\n",
      "[Training Epoch 19] Batch 15000, Loss 0.33084017038345337\n",
      "[Training Epoch 19] Batch 17500, Loss 0.2752504348754883\n",
      "[Evluating Epoch 19] HR = 0.6457, NDCG = 0.3784\n",
      "After epoch 19:\n",
      "Hit Ratio: 0.6457\n",
      "NDCG: 0.3784\n",
      "Training Loss: 0.3062\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "config = mlp_config\n",
    "engine = MLPEngine(config)\n",
    "MLP_Model = {'hit_ratio': [], 'ndcg': [], 'training_loss': []}\n",
    "for epoch in range(config['num_epoch']):\n",
    "    print('Epoch {} starts !'.format(epoch))\n",
    "    print('-' * 80)\n",
    "    train_loader = sample_generator.instance_a_train_loader(config['num_negative'], config['batch_size'])\n",
    "    last_batch_loss = engine.train_an_epoch(train_loader, epoch_id=epoch)\n",
    "    \n",
    "    # Evaluate the model after each epoch\n",
    "    hit_ratio, ndcg = engine.evaluate(evaluate_data, epoch_id=epoch)\n",
    "    MLP_Model['hit_ratio'].append(hit_ratio)\n",
    "    MLP_Model['ndcg'].append(ndcg)\n",
    "    MLP_Model['training_loss'].append(last_batch_loss)    \n",
    "    # Print the evaluation metrics\n",
    "    print('After epoch {}:'.format(epoch))\n",
    "    print('Hit Ratio: {:.4f}'.format(hit_ratio))\n",
    "    print('NDCG: {:.4f}'.format(ndcg))\n",
    "    print('Training Loss: {:.4f}'.format(last_batch_loss))\n",
    "    print('-' * 80)                                \n",
    "    # Save the model after each epoch\n",
    "    engine.save(config['alias'], epoch, hit_ratio, ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T12:29:49.646325Z",
     "iopub.status.busy": "2024-05-22T12:29:49.645969Z",
     "iopub.status.idle": "2024-05-22T12:57:40.789323Z",
     "shell.execute_reply": "2024-05-22T12:57:40.788412Z",
     "shell.execute_reply.started": "2024-05-22T12:29:49.646285Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=16, bias=True)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=16, out_features=1, bias=True)\n",
      "NeuMF(\n",
      "  (embedding_user_mlp): Embedding(6040, 8)\n",
      "  (embedding_item_mlp): Embedding(3706, 8)\n",
      "  (embedding_user_mf): Embedding(6040, 8)\n",
      "  (embedding_item_mf): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.6948800086975098\n",
      "[Training Epoch 0] Batch 2500, Loss 0.3127749562263489\n",
      "[Evluating Epoch 0] HR = 0.5414, NDCG = 0.3041\n",
      "After epoch 0:\n",
      "Hit Ratio: 0.5414\n",
      "NDCG: 0.3041\n",
      "Training Loss: 0.3464\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.3184869587421417\n",
      "[Training Epoch 1] Batch 2500, Loss 0.2945004999637604\n",
      "[Evluating Epoch 1] HR = 0.5916, NDCG = 0.3345\n",
      "After epoch 1:\n",
      "Hit Ratio: 0.5916\n",
      "NDCG: 0.3345\n",
      "Training Loss: 0.2657\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.26224642992019653\n",
      "[Training Epoch 2] Batch 2500, Loss 0.2807256579399109\n",
      "[Evluating Epoch 2] HR = 0.6253, NDCG = 0.3594\n",
      "After epoch 2:\n",
      "Hit Ratio: 0.6253\n",
      "NDCG: 0.3594\n",
      "Training Loss: 0.2813\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.2760920524597168\n",
      "[Training Epoch 3] Batch 2500, Loss 0.2777918577194214\n",
      "[Evluating Epoch 3] HR = 0.6379, NDCG = 0.3685\n",
      "After epoch 3:\n",
      "Hit Ratio: 0.6379\n",
      "NDCG: 0.3685\n",
      "Training Loss: 0.2313\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.2869648337364197\n",
      "[Training Epoch 4] Batch 2500, Loss 0.26815682649612427\n",
      "[Evluating Epoch 4] HR = 0.6475, NDCG = 0.3752\n",
      "After epoch 4:\n",
      "Hit Ratio: 0.6475\n",
      "NDCG: 0.3752\n",
      "Training Loss: 0.3033\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.27545905113220215\n",
      "[Training Epoch 5] Batch 2500, Loss 0.2661738991737366\n",
      "[Evluating Epoch 5] HR = 0.6520, NDCG = 0.3771\n",
      "After epoch 5:\n",
      "Hit Ratio: 0.6520\n",
      "NDCG: 0.3771\n",
      "Training Loss: 0.2449\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.27884435653686523\n",
      "[Training Epoch 6] Batch 2500, Loss 0.27654239535331726\n",
      "[Evluating Epoch 6] HR = 0.6543, NDCG = 0.3825\n",
      "After epoch 6:\n",
      "Hit Ratio: 0.6543\n",
      "NDCG: 0.3825\n",
      "Training Loss: 0.2761\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.24947156012058258\n",
      "[Training Epoch 7] Batch 2500, Loss 0.25394171476364136\n",
      "[Evluating Epoch 7] HR = 0.6558, NDCG = 0.3833\n",
      "After epoch 7:\n",
      "Hit Ratio: 0.6558\n",
      "NDCG: 0.3833\n",
      "Training Loss: 0.2884\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.24792203307151794\n",
      "[Training Epoch 8] Batch 2500, Loss 0.2898106276988983\n",
      "[Evluating Epoch 8] HR = 0.6618, NDCG = 0.3872\n",
      "After epoch 8:\n",
      "Hit Ratio: 0.6618\n",
      "NDCG: 0.3872\n",
      "Training Loss: 0.2791\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.2459309995174408\n",
      "[Training Epoch 9] Batch 2500, Loss 0.2970389723777771\n",
      "[Evluating Epoch 9] HR = 0.6652, NDCG = 0.3905\n",
      "After epoch 9:\n",
      "Hit Ratio: 0.6652\n",
      "NDCG: 0.3905\n",
      "Training Loss: 0.2679\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.24274778366088867\n",
      "[Training Epoch 10] Batch 2500, Loss 0.24688075482845306\n",
      "[Evluating Epoch 10] HR = 0.6667, NDCG = 0.3917\n",
      "After epoch 10:\n",
      "Hit Ratio: 0.6667\n",
      "NDCG: 0.3917\n",
      "Training Loss: 0.2643\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.2551252841949463\n",
      "[Training Epoch 11] Batch 2500, Loss 0.2549542188644409\n",
      "[Evluating Epoch 11] HR = 0.6677, NDCG = 0.3924\n",
      "After epoch 11:\n",
      "Hit Ratio: 0.6677\n",
      "NDCG: 0.3924\n",
      "Training Loss: 0.2442\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.22579613327980042\n",
      "[Training Epoch 12] Batch 2500, Loss 0.21840664744377136\n",
      "[Evluating Epoch 12] HR = 0.6692, NDCG = 0.3949\n",
      "After epoch 12:\n",
      "Hit Ratio: 0.6692\n",
      "NDCG: 0.3949\n",
      "Training Loss: 0.2560\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.22740256786346436\n",
      "[Training Epoch 13] Batch 2500, Loss 0.2734038531780243\n",
      "[Evluating Epoch 13] HR = 0.6671, NDCG = 0.3959\n",
      "After epoch 13:\n",
      "Hit Ratio: 0.6671\n",
      "NDCG: 0.3959\n",
      "Training Loss: 0.2368\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.26614147424697876\n",
      "[Training Epoch 14] Batch 2500, Loss 0.25244206190109253\n",
      "[Evluating Epoch 14] HR = 0.6705, NDCG = 0.3944\n",
      "After epoch 14:\n",
      "Hit Ratio: 0.6705\n",
      "NDCG: 0.3944\n",
      "Training Loss: 0.2054\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.2457149624824524\n",
      "[Training Epoch 15] Batch 2500, Loss 0.254575252532959\n",
      "[Evluating Epoch 15] HR = 0.6707, NDCG = 0.3966\n",
      "After epoch 15:\n",
      "Hit Ratio: 0.6707\n",
      "NDCG: 0.3966\n",
      "Training Loss: 0.2858\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.22919747233390808\n",
      "[Training Epoch 16] Batch 2500, Loss 0.25733011960983276\n",
      "[Evluating Epoch 16] HR = 0.6717, NDCG = 0.4003\n",
      "After epoch 16:\n",
      "Hit Ratio: 0.6717\n",
      "NDCG: 0.4003\n",
      "Training Loss: 0.2530\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.22744905948638916\n",
      "[Training Epoch 17] Batch 2500, Loss 0.25856295228004456\n",
      "[Evluating Epoch 17] HR = 0.6738, NDCG = 0.4003\n",
      "After epoch 17:\n",
      "Hit Ratio: 0.6738\n",
      "NDCG: 0.4003\n",
      "Training Loss: 0.2971\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.25074297189712524\n",
      "[Training Epoch 18] Batch 2500, Loss 0.22766157984733582\n",
      "[Evluating Epoch 18] HR = 0.6806, NDCG = 0.4028\n",
      "After epoch 18:\n",
      "Hit Ratio: 0.6806\n",
      "NDCG: 0.4028\n",
      "Training Loss: 0.2240\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.2358873337507248\n",
      "[Training Epoch 19] Batch 2500, Loss 0.22377876937389374\n",
      "[Evluating Epoch 19] HR = 0.6791, NDCG = 0.4023\n",
      "After epoch 19:\n",
      "Hit Ratio: 0.6791\n",
      "NDCG: 0.4023\n",
      "Training Loss: 0.2833\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "config = neumf_config\n",
    "engine = NeuMFEngine(config)\n",
    "NeuMF_Model = {'hit_ratio': [], 'ndcg': [], 'training_loss': []}\n",
    "for epoch in range(config['num_epoch']):\n",
    "    print('Epoch {} starts !'.format(epoch))\n",
    "    print('-' * 80)\n",
    "    train_loader = sample_generator.instance_a_train_loader(config['num_negative'], config['batch_size'])\n",
    "    last_batch_loss = engine.train_an_epoch(train_loader, epoch_id=epoch)\n",
    "    \n",
    "    # Evaluate the model after each epoch\n",
    "    hit_ratio, ndcg = engine.evaluate(evaluate_data, epoch_id=epoch)\n",
    "    NeuMF_Model['hit_ratio'].append(hit_ratio)\n",
    "    NeuMF_Model['ndcg'].append(ndcg)\n",
    "    NeuMF_Model['training_loss'].append(last_batch_loss)\n",
    "    # Print the evaluation metrics\n",
    "    print('After epoch {}:'.format(epoch))\n",
    "    print('Hit Ratio: {:.4f}'.format(hit_ratio))\n",
    "    print('NDCG: {:.4f}'.format(ndcg))\n",
    "    print('Training Loss: {:.4f}'.format(last_batch_loss))\n",
    "    print('-' * 80)                                \n",
    "    \n",
    "    # Save the model after each epoch\n",
    "    engine.save(config['alias'], epoch, hit_ratio, ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T12:57:40.790354Z",
     "iopub.status.busy": "2024-05-22T12:57:40.790189Z",
     "iopub.status.idle": "2024-05-22T12:57:40.818651Z",
     "shell.execute_reply": "2024-05-22T12:57:40.818025Z",
     "shell.execute_reply.started": "2024-05-22T12:57:40.790336Z"
    }
   },
   "outputs": [],
   "source": [
    "df_NeuMF = pd.DataFrame(NeuMF_Model)\n",
    "df_GMF = pd.DataFrame(GMF_Model)\n",
    "df_MLP = pd.DataFrame(MLP_Model)\n",
    "\n",
    "# Loop to save dataframes to CSV files with different names\n",
    "df_NeuMF.to_csv('NeuMF_Model.csv', index=False)\n",
    "df_GMF.to_csv('GMF_Model.csv', index=False)\n",
    "df_MLP.to_csv('MLP_Model.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T12:57:40.819522Z",
     "iopub.status.busy": "2024-05-22T12:57:40.819370Z",
     "iopub.status.idle": "2024-05-22T12:57:41.264442Z",
     "shell.execute_reply": "2024-05-22T12:57:41.263933Z",
     "shell.execute_reply.started": "2024-05-22T12:57:40.819505Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterations = list(range(1, 21))\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Plot Training Loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(iterations, NeuMF_Model['training_loss'], 'r-', label='NeuMF')\n",
    "plt.plot(iterations, GMF_Model['training_loss'], 'b--', label='GMF')\n",
    "plt.plot(iterations, MLP_Model['training_loss'], 'm-.', label='MLP')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('MovieLens - Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot HR@10 over iterations\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(iterations, NeuMF_Model['hit_ratio'], 'r-', label='NeuMF')\n",
    "plt.plot(iterations, GMF_Model['hit_ratio'], 'b--', label='GMF')\n",
    "plt.plot(iterations, MLP_Model['hit_ratio'], 'm-.', label='MLP')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('HR@10')\n",
    "plt.title('MovieLens - HR@10')\n",
    "plt.legend()\n",
    "\n",
    "# Plot NDCG@10 over iterations\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(iterations, NeuMF_Model['ndcg'], 'r-', label='NeuMF')\n",
    "plt.plot(iterations, GMF_Model['ndcg'], 'b--', label='GMF')\n",
    "plt.plot(iterations, MLP_Model['ndcg'], 'm-.', label='MLP')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('NDCG@10')\n",
    "plt.title('MovieLens - NDCG@10')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ablation study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T13:19:19.455948Z",
     "iopub.status.busy": "2024-05-22T13:19:19.455704Z",
     "iopub.status.idle": "2024-05-22T15:09:15.169486Z",
     "shell.execute_reply": "2024-05-22T15:09:15.168898Z",
     "shell.execute_reply.started": "2024-05-22T13:19:19.455929Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": [16, 64, 32, 16, 8]\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=16, bias=True)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=8, out_features=1, bias=True)\n",
      "MLP(\n",
      "  (embedding_user): Embedding(6040, 8)\n",
      "  (embedding_item): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.7926473617553711\n",
      "[Training Epoch 0] Batch 2500, Loss 0.3280324339866638\n",
      "[Training Epoch 0] Batch 5000, Loss 0.38084179162979126\n",
      "[Training Epoch 0] Batch 7500, Loss 0.37946659326553345\n",
      "[Training Epoch 0] Batch 10000, Loss 0.40915024280548096\n",
      "[Training Epoch 0] Batch 12500, Loss 0.34963369369506836\n",
      "[Training Epoch 0] Batch 15000, Loss 0.3511112928390503\n",
      "[Training Epoch 0] Batch 17500, Loss 0.3147240877151489\n",
      "[Evluating Epoch 0] HR = 0.4439, NDCG = 0.2430\n",
      " 0: HR = 0.44387417218543046, NDCG = 0.24301819689909487\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.3261973261833191\n",
      "[Training Epoch 1] Batch 2500, Loss 0.36383694410324097\n",
      "[Training Epoch 1] Batch 5000, Loss 0.33506569266319275\n",
      "[Training Epoch 1] Batch 7500, Loss 0.3358994722366333\n",
      "[Training Epoch 1] Batch 10000, Loss 0.3300422132015228\n",
      "[Training Epoch 1] Batch 12500, Loss 0.3131774067878723\n",
      "[Training Epoch 1] Batch 15000, Loss 0.3306048512458801\n",
      "[Training Epoch 1] Batch 17500, Loss 0.32392093539237976\n",
      "[Evluating Epoch 1] HR = 0.4803, NDCG = 0.2670\n",
      " 1: HR = 0.48029801324503313, NDCG = 0.2670295441949118\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.3206535577774048\n",
      "[Training Epoch 2] Batch 2500, Loss 0.3314511775970459\n",
      "[Training Epoch 2] Batch 5000, Loss 0.3227316737174988\n",
      "[Training Epoch 2] Batch 7500, Loss 0.31326743960380554\n",
      "[Training Epoch 2] Batch 10000, Loss 0.29924747347831726\n",
      "[Training Epoch 2] Batch 12500, Loss 0.2813586890697479\n",
      "[Training Epoch 2] Batch 15000, Loss 0.35249632596969604\n",
      "[Training Epoch 2] Batch 17500, Loss 0.27852243185043335\n",
      "[Evluating Epoch 2] HR = 0.5262, NDCG = 0.2934\n",
      " 2: HR = 0.526158940397351, NDCG = 0.2934057262414017\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.36722803115844727\n",
      "[Training Epoch 3] Batch 2500, Loss 0.30778878927230835\n",
      "[Training Epoch 3] Batch 5000, Loss 0.31290188431739807\n",
      "[Training Epoch 3] Batch 7500, Loss 0.30440208315849304\n",
      "[Training Epoch 3] Batch 10000, Loss 0.26327604055404663\n",
      "[Training Epoch 3] Batch 12500, Loss 0.2646077275276184\n",
      "[Training Epoch 3] Batch 15000, Loss 0.3119865655899048\n",
      "[Training Epoch 3] Batch 17500, Loss 0.39091768860816956\n",
      "[Evluating Epoch 3] HR = 0.5644, NDCG = 0.3152\n",
      " 3: HR = 0.5644039735099338, NDCG = 0.3151896950430555\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.3135245442390442\n",
      "[Training Epoch 4] Batch 2500, Loss 0.282573938369751\n",
      "[Training Epoch 4] Batch 5000, Loss 0.2989546060562134\n",
      "[Training Epoch 4] Batch 7500, Loss 0.27590981125831604\n",
      "[Training Epoch 4] Batch 10000, Loss 0.2921222746372223\n",
      "[Training Epoch 4] Batch 12500, Loss 0.26929011940956116\n",
      "[Training Epoch 4] Batch 15000, Loss 0.2814157009124756\n",
      "[Training Epoch 4] Batch 17500, Loss 0.24057164788246155\n",
      "[Evluating Epoch 4] HR = 0.5801, NDCG = 0.3226\n",
      " 4: HR = 0.5801324503311258, NDCG = 0.3225864629031679\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.3125185966491699\n",
      "[Training Epoch 5] Batch 2500, Loss 0.23277068138122559\n",
      "[Training Epoch 5] Batch 5000, Loss 0.28182655572891235\n",
      "[Training Epoch 5] Batch 7500, Loss 0.2632904052734375\n",
      "[Training Epoch 5] Batch 10000, Loss 0.2816835045814514\n",
      "[Training Epoch 5] Batch 12500, Loss 0.25656434893608093\n",
      "[Training Epoch 5] Batch 15000, Loss 0.31137096881866455\n",
      "[Training Epoch 5] Batch 17500, Loss 0.28667864203453064\n",
      "[Evluating Epoch 5] HR = 0.5907, NDCG = 0.3336\n",
      " 5: HR = 0.590728476821192, NDCG = 0.33359257483712046\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.2758650779724121\n",
      "[Training Epoch 6] Batch 2500, Loss 0.2582635283470154\n",
      "[Training Epoch 6] Batch 5000, Loss 0.25551891326904297\n",
      "[Training Epoch 6] Batch 7500, Loss 0.30458587408065796\n",
      "[Training Epoch 6] Batch 10000, Loss 0.30328720808029175\n",
      "[Training Epoch 6] Batch 12500, Loss 0.26646122336387634\n",
      "[Training Epoch 6] Batch 15000, Loss 0.3355873227119446\n",
      "[Training Epoch 6] Batch 17500, Loss 0.2954891324043274\n",
      "[Evluating Epoch 6] HR = 0.6081, NDCG = 0.3450\n",
      " 6: HR = 0.608112582781457, NDCG = 0.3449651308928695\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.23850110173225403\n",
      "[Training Epoch 7] Batch 2500, Loss 0.30039170384407043\n",
      "[Training Epoch 7] Batch 5000, Loss 0.25379127264022827\n",
      "[Training Epoch 7] Batch 7500, Loss 0.3465687334537506\n",
      "[Training Epoch 7] Batch 10000, Loss 0.26030951738357544\n",
      "[Training Epoch 7] Batch 12500, Loss 0.2592846751213074\n",
      "[Training Epoch 7] Batch 15000, Loss 0.266719788312912\n",
      "[Training Epoch 7] Batch 17500, Loss 0.31651249527931213\n",
      "[Evluating Epoch 7] HR = 0.6187, NDCG = 0.3530\n",
      " 7: HR = 0.6187086092715232, NDCG = 0.3530272764236846\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.2921523451805115\n",
      "[Training Epoch 8] Batch 2500, Loss 0.23906633257865906\n",
      "[Training Epoch 8] Batch 5000, Loss 0.27334028482437134\n",
      "[Training Epoch 8] Batch 7500, Loss 0.2565902769565582\n",
      "[Training Epoch 8] Batch 10000, Loss 0.2705669403076172\n",
      "[Training Epoch 8] Batch 12500, Loss 0.29533714056015015\n",
      "[Training Epoch 8] Batch 15000, Loss 0.30700546503067017\n",
      "[Training Epoch 8] Batch 17500, Loss 0.24740462005138397\n",
      "[Evluating Epoch 8] HR = 0.6209, NDCG = 0.3548\n",
      " 8: HR = 0.6208609271523179, NDCG = 0.35475897269982765\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.26143330335617065\n",
      "[Training Epoch 9] Batch 2500, Loss 0.2810380160808563\n",
      "[Training Epoch 9] Batch 5000, Loss 0.2834252119064331\n",
      "[Training Epoch 9] Batch 7500, Loss 0.28805774450302124\n",
      "[Training Epoch 9] Batch 10000, Loss 0.3104923367500305\n",
      "[Training Epoch 9] Batch 12500, Loss 0.25203120708465576\n",
      "[Training Epoch 9] Batch 15000, Loss 0.21086551249027252\n",
      "[Training Epoch 9] Batch 17500, Loss 0.27935791015625\n",
      "[Evluating Epoch 9] HR = 0.6230, NDCG = 0.3563\n",
      " 9: HR = 0.6230132450331126, NDCG = 0.3563279829644533\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.24806484580039978\n",
      "[Training Epoch 10] Batch 2500, Loss 0.2567766606807709\n",
      "[Training Epoch 10] Batch 5000, Loss 0.28496667742729187\n",
      "[Training Epoch 10] Batch 7500, Loss 0.25743168592453003\n",
      "[Training Epoch 10] Batch 10000, Loss 0.21251067519187927\n",
      "[Training Epoch 10] Batch 12500, Loss 0.27002692222595215\n",
      "[Training Epoch 10] Batch 15000, Loss 0.2727013826370239\n",
      "[Training Epoch 10] Batch 17500, Loss 0.3016811013221741\n",
      "[Evluating Epoch 10] HR = 0.6240, NDCG = 0.3608\n",
      " 10: HR = 0.6240066225165563, NDCG = 0.360750905858502\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.24261507391929626\n",
      "[Training Epoch 11] Batch 2500, Loss 0.3041982054710388\n",
      "[Training Epoch 11] Batch 5000, Loss 0.26866060495376587\n",
      "[Training Epoch 11] Batch 7500, Loss 0.2610461711883545\n",
      "[Training Epoch 11] Batch 10000, Loss 0.2506275475025177\n",
      "[Training Epoch 11] Batch 12500, Loss 0.30708861351013184\n",
      "[Training Epoch 11] Batch 15000, Loss 0.2802543044090271\n",
      "[Training Epoch 11] Batch 17500, Loss 0.30035528540611267\n",
      "[Evluating Epoch 11] HR = 0.6293, NDCG = 0.3624\n",
      " 11: HR = 0.6293046357615895, NDCG = 0.3623724881467675\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.26460087299346924\n",
      "[Training Epoch 12] Batch 2500, Loss 0.2589167654514313\n",
      "[Training Epoch 12] Batch 5000, Loss 0.2816898226737976\n",
      "[Training Epoch 12] Batch 7500, Loss 0.2665454149246216\n",
      "[Training Epoch 12] Batch 10000, Loss 0.22328659892082214\n",
      "[Training Epoch 12] Batch 12500, Loss 0.2650858163833618\n",
      "[Training Epoch 12] Batch 15000, Loss 0.2933686375617981\n",
      "[Training Epoch 12] Batch 17500, Loss 0.25149857997894287\n",
      "[Evluating Epoch 12] HR = 0.6354, NDCG = 0.3669\n",
      " 12: HR = 0.635430463576159, NDCG = 0.36691779509634126\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.27585557103157043\n",
      "[Training Epoch 13] Batch 2500, Loss 0.29930365085601807\n",
      "[Training Epoch 13] Batch 5000, Loss 0.23129627108573914\n",
      "[Training Epoch 13] Batch 7500, Loss 0.2551620602607727\n",
      "[Training Epoch 13] Batch 10000, Loss 0.27471476793289185\n",
      "[Training Epoch 13] Batch 12500, Loss 0.30743712186813354\n",
      "[Training Epoch 13] Batch 15000, Loss 0.28397825360298157\n",
      "[Training Epoch 13] Batch 17500, Loss 0.24969974160194397\n",
      "[Evluating Epoch 13] HR = 0.6349, NDCG = 0.3676\n",
      " 13: HR = 0.6349337748344371, NDCG = 0.3675964332644559\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.21840935945510864\n",
      "[Training Epoch 14] Batch 2500, Loss 0.29469186067581177\n",
      "[Training Epoch 14] Batch 5000, Loss 0.21745431423187256\n",
      "[Training Epoch 14] Batch 7500, Loss 0.24751350283622742\n",
      "[Training Epoch 14] Batch 10000, Loss 0.2635294795036316\n",
      "[Training Epoch 14] Batch 12500, Loss 0.24760663509368896\n",
      "[Training Epoch 14] Batch 15000, Loss 0.24671781063079834\n",
      "[Training Epoch 14] Batch 17500, Loss 0.27386045455932617\n",
      "[Evluating Epoch 14] HR = 0.6363, NDCG = 0.3684\n",
      " 14: HR = 0.6362582781456954, NDCG = 0.36837246786452693\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.21650564670562744\n",
      "[Training Epoch 15] Batch 2500, Loss 0.23082607984542847\n",
      "[Training Epoch 15] Batch 5000, Loss 0.23215626180171967\n",
      "[Training Epoch 15] Batch 7500, Loss 0.26207903027534485\n",
      "[Training Epoch 15] Batch 10000, Loss 0.35126248002052307\n",
      "[Training Epoch 15] Batch 12500, Loss 0.24140726029872894\n",
      "[Training Epoch 15] Batch 15000, Loss 0.2523767650127411\n",
      "[Training Epoch 15] Batch 17500, Loss 0.30142849683761597\n",
      "[Evluating Epoch 15] HR = 0.6379, NDCG = 0.3686\n",
      " 15: HR = 0.6379139072847683, NDCG = 0.36861882509692573\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.2472326159477234\n",
      "[Training Epoch 16] Batch 2500, Loss 0.21650579571723938\n",
      "[Training Epoch 16] Batch 5000, Loss 0.3018632233142853\n",
      "[Training Epoch 16] Batch 7500, Loss 0.2964075207710266\n",
      "[Training Epoch 16] Batch 10000, Loss 0.2905269265174866\n",
      "[Training Epoch 16] Batch 12500, Loss 0.25393974781036377\n",
      "[Training Epoch 16] Batch 15000, Loss 0.2623116672039032\n",
      "[Training Epoch 16] Batch 17500, Loss 0.2613542675971985\n",
      "[Evluating Epoch 16] HR = 0.6387, NDCG = 0.3683\n",
      " 16: HR = 0.6387417218543047, NDCG = 0.3683254410757943\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.29712462425231934\n",
      "[Training Epoch 17] Batch 2500, Loss 0.32574665546417236\n",
      "[Training Epoch 17] Batch 5000, Loss 0.3204357326030731\n",
      "[Training Epoch 17] Batch 7500, Loss 0.24458393454551697\n",
      "[Training Epoch 17] Batch 10000, Loss 0.30324786901474\n",
      "[Training Epoch 17] Batch 12500, Loss 0.2402951717376709\n",
      "[Training Epoch 17] Batch 15000, Loss 0.3105567693710327\n",
      "[Training Epoch 17] Batch 17500, Loss 0.18916097283363342\n",
      "[Evluating Epoch 17] HR = 0.6396, NDCG = 0.3716\n",
      " 17: HR = 0.6395695364238411, NDCG = 0.37161005691670895\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.2585567235946655\n",
      "[Training Epoch 18] Batch 2500, Loss 0.22465282678604126\n",
      "[Training Epoch 18] Batch 5000, Loss 0.25792360305786133\n",
      "[Training Epoch 18] Batch 7500, Loss 0.2547004222869873\n",
      "[Training Epoch 18] Batch 10000, Loss 0.24407266080379486\n",
      "[Training Epoch 18] Batch 12500, Loss 0.2909157872200012\n",
      "[Training Epoch 18] Batch 15000, Loss 0.27301597595214844\n",
      "[Training Epoch 18] Batch 17500, Loss 0.25858116149902344\n",
      "[Evluating Epoch 18] HR = 0.6419, NDCG = 0.3699\n",
      " 18: HR = 0.641887417218543, NDCG = 0.36990712131991627\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.2630773186683655\n",
      "[Training Epoch 19] Batch 2500, Loss 0.20570889115333557\n",
      "[Training Epoch 19] Batch 5000, Loss 0.25706517696380615\n",
      "[Training Epoch 19] Batch 7500, Loss 0.24339988827705383\n",
      "[Training Epoch 19] Batch 10000, Loss 0.24686917662620544\n",
      "[Training Epoch 19] Batch 12500, Loss 0.3364456295967102\n",
      "[Training Epoch 19] Batch 15000, Loss 0.2700454592704773\n",
      "[Training Epoch 19] Batch 17500, Loss 0.24513351917266846\n",
      "[Evluating Epoch 19] HR = 0.6396, NDCG = 0.3705\n",
      " 19: HR = 0.6395695364238411, NDCG = 0.37052773821023843\n",
      ": [16, 32, 16, 8]\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=16, bias=True)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=8, out_features=1, bias=True)\n",
      "MLP(\n",
      "  (embedding_user): Embedding(6040, 8)\n",
      "  (embedding_item): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.6141445636749268\n",
      "[Training Epoch 0] Batch 2500, Loss 0.3047988712787628\n",
      "[Training Epoch 0] Batch 5000, Loss 0.281643271446228\n",
      "[Training Epoch 0] Batch 7500, Loss 0.34411782026290894\n",
      "[Training Epoch 0] Batch 10000, Loss 0.3589538037776947\n",
      "[Training Epoch 0] Batch 12500, Loss 0.3643935024738312\n",
      "[Training Epoch 0] Batch 15000, Loss 0.3661256730556488\n",
      "[Training Epoch 0] Batch 17500, Loss 0.33157530426979065\n",
      "[Evluating Epoch 0] HR = 0.4502, NDCG = 0.2469\n",
      " 0: HR = 0.4501655629139073, NDCG = 0.24694158074212452\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.3847195506095886\n",
      "[Training Epoch 1] Batch 2500, Loss 0.3242582380771637\n",
      "[Training Epoch 1] Batch 5000, Loss 0.37685349583625793\n",
      "[Training Epoch 1] Batch 7500, Loss 0.3763163685798645\n",
      "[Training Epoch 1] Batch 10000, Loss 0.32321101427078247\n",
      "[Training Epoch 1] Batch 12500, Loss 0.3416142165660858\n",
      "[Training Epoch 1] Batch 15000, Loss 0.3534858822822571\n",
      "[Training Epoch 1] Batch 17500, Loss 0.3282286524772644\n",
      "[Evluating Epoch 1] HR = 0.4803, NDCG = 0.2647\n",
      " 1: HR = 0.48029801324503313, NDCG = 0.2647321177205671\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.2944334149360657\n",
      "[Training Epoch 2] Batch 2500, Loss 0.3563147187232971\n",
      "[Training Epoch 2] Batch 5000, Loss 0.39898550510406494\n",
      "[Training Epoch 2] Batch 7500, Loss 0.3707279860973358\n",
      "[Training Epoch 2] Batch 10000, Loss 0.3334624171257019\n",
      "[Training Epoch 2] Batch 12500, Loss 0.3582645356655121\n",
      "[Training Epoch 2] Batch 15000, Loss 0.3118903338909149\n",
      "[Training Epoch 2] Batch 17500, Loss 0.3369029760360718\n",
      "[Evluating Epoch 2] HR = 0.5048, NDCG = 0.2801\n",
      " 2: HR = 0.5048013245033113, NDCG = 0.2801331965570234\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.33582261204719543\n",
      "[Training Epoch 3] Batch 2500, Loss 0.27747994661331177\n",
      "[Training Epoch 3] Batch 5000, Loss 0.3081038296222687\n",
      "[Training Epoch 3] Batch 7500, Loss 0.31722718477249146\n",
      "[Training Epoch 3] Batch 10000, Loss 0.3034166395664215\n",
      "[Training Epoch 3] Batch 12500, Loss 0.29820847511291504\n",
      "[Training Epoch 3] Batch 15000, Loss 0.29304513335227966\n",
      "[Training Epoch 3] Batch 17500, Loss 0.30703482031822205\n",
      "[Evluating Epoch 3] HR = 0.5414, NDCG = 0.3025\n",
      " 3: HR = 0.5413907284768212, NDCG = 0.30247031790632406\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.24420282244682312\n",
      "[Training Epoch 4] Batch 2500, Loss 0.3022051453590393\n",
      "[Training Epoch 4] Batch 5000, Loss 0.318356454372406\n",
      "[Training Epoch 4] Batch 7500, Loss 0.2778887450695038\n",
      "[Training Epoch 4] Batch 10000, Loss 0.30688393115997314\n",
      "[Training Epoch 4] Batch 12500, Loss 0.29485392570495605\n",
      "[Training Epoch 4] Batch 15000, Loss 0.31874412298202515\n",
      "[Training Epoch 4] Batch 17500, Loss 0.2716071605682373\n",
      "[Evluating Epoch 4] HR = 0.5712, NDCG = 0.3191\n",
      " 4: HR = 0.5711920529801324, NDCG = 0.3190501499604146\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.339520663022995\n",
      "[Training Epoch 5] Batch 2500, Loss 0.3342498540878296\n",
      "[Training Epoch 5] Batch 5000, Loss 0.3081032931804657\n",
      "[Training Epoch 5] Batch 7500, Loss 0.31643205881118774\n",
      "[Training Epoch 5] Batch 10000, Loss 0.33549612760543823\n",
      "[Training Epoch 5] Batch 12500, Loss 0.2845122814178467\n",
      "[Training Epoch 5] Batch 15000, Loss 0.32367461919784546\n",
      "[Training Epoch 5] Batch 17500, Loss 0.2829013466835022\n",
      "[Evluating Epoch 5] HR = 0.5778, NDCG = 0.3232\n",
      " 5: HR = 0.5778145695364238, NDCG = 0.32315313935806056\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.3194453716278076\n",
      "[Training Epoch 6] Batch 2500, Loss 0.20075629651546478\n",
      "[Training Epoch 6] Batch 5000, Loss 0.2536109685897827\n",
      "[Training Epoch 6] Batch 7500, Loss 0.2363056242465973\n",
      "[Training Epoch 6] Batch 10000, Loss 0.23526838421821594\n",
      "[Training Epoch 6] Batch 12500, Loss 0.2765532433986664\n",
      "[Training Epoch 6] Batch 15000, Loss 0.28643864393234253\n",
      "[Training Epoch 6] Batch 17500, Loss 0.26764780282974243\n",
      "[Evluating Epoch 6] HR = 0.5934, NDCG = 0.3350\n",
      " 6: HR = 0.5933774834437087, NDCG = 0.33497130974336875\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.28011006116867065\n",
      "[Training Epoch 7] Batch 2500, Loss 0.3593665063381195\n",
      "[Training Epoch 7] Batch 5000, Loss 0.2924315333366394\n",
      "[Training Epoch 7] Batch 7500, Loss 0.2960878610610962\n",
      "[Training Epoch 7] Batch 10000, Loss 0.30513688921928406\n",
      "[Training Epoch 7] Batch 12500, Loss 0.3021947145462036\n",
      "[Training Epoch 7] Batch 15000, Loss 0.24714726209640503\n",
      "[Training Epoch 7] Batch 17500, Loss 0.24424757063388824\n",
      "[Evluating Epoch 7] HR = 0.6022, NDCG = 0.3431\n",
      " 7: HR = 0.6021523178807947, NDCG = 0.34312794517634754\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.30591797828674316\n",
      "[Training Epoch 8] Batch 2500, Loss 0.2851702868938446\n",
      "[Training Epoch 8] Batch 5000, Loss 0.27002233266830444\n",
      "[Training Epoch 8] Batch 7500, Loss 0.27545803785324097\n",
      "[Training Epoch 8] Batch 10000, Loss 0.28577929735183716\n",
      "[Training Epoch 8] Batch 12500, Loss 0.2826196551322937\n",
      "[Training Epoch 8] Batch 15000, Loss 0.3179508447647095\n",
      "[Training Epoch 8] Batch 17500, Loss 0.27174222469329834\n",
      "[Evluating Epoch 8] HR = 0.6007, NDCG = 0.3452\n",
      " 8: HR = 0.6006622516556291, NDCG = 0.3451556796046069\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.3104843497276306\n",
      "[Training Epoch 9] Batch 2500, Loss 0.2876216769218445\n",
      "[Training Epoch 9] Batch 5000, Loss 0.31056341528892517\n",
      "[Training Epoch 9] Batch 7500, Loss 0.2734839618206024\n",
      "[Training Epoch 9] Batch 10000, Loss 0.247590109705925\n",
      "[Training Epoch 9] Batch 12500, Loss 0.2868380546569824\n",
      "[Training Epoch 9] Batch 15000, Loss 0.2002687156200409\n",
      "[Training Epoch 9] Batch 17500, Loss 0.25036489963531494\n",
      "[Evluating Epoch 9] HR = 0.6104, NDCG = 0.3463\n",
      " 9: HR = 0.6104304635761589, NDCG = 0.34630731363331724\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.2779272198677063\n",
      "[Training Epoch 10] Batch 2500, Loss 0.2294587790966034\n",
      "[Training Epoch 10] Batch 5000, Loss 0.2809344232082367\n",
      "[Training Epoch 10] Batch 7500, Loss 0.28159427642822266\n",
      "[Training Epoch 10] Batch 10000, Loss 0.24733734130859375\n",
      "[Training Epoch 10] Batch 12500, Loss 0.29634779691696167\n",
      "[Training Epoch 10] Batch 15000, Loss 0.3090892434120178\n",
      "[Training Epoch 10] Batch 17500, Loss 0.24645845592021942\n",
      "[Evluating Epoch 10] HR = 0.6161, NDCG = 0.3511\n",
      " 10: HR = 0.6160596026490066, NDCG = 0.3510768740555164\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.31962013244628906\n",
      "[Training Epoch 11] Batch 2500, Loss 0.30906254053115845\n",
      "[Training Epoch 11] Batch 5000, Loss 0.2889536917209625\n",
      "[Training Epoch 11] Batch 7500, Loss 0.24776393175125122\n",
      "[Training Epoch 11] Batch 10000, Loss 0.24783456325531006\n",
      "[Training Epoch 11] Batch 12500, Loss 0.23448950052261353\n",
      "[Training Epoch 11] Batch 15000, Loss 0.2701299786567688\n",
      "[Training Epoch 11] Batch 17500, Loss 0.3261711299419403\n",
      "[Evluating Epoch 11] HR = 0.6162, NDCG = 0.3508\n",
      " 11: HR = 0.616225165562914, NDCG = 0.3507946083308406\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.252514123916626\n",
      "[Training Epoch 12] Batch 2500, Loss 0.3323785066604614\n",
      "[Training Epoch 12] Batch 5000, Loss 0.2999175786972046\n",
      "[Training Epoch 12] Batch 7500, Loss 0.25293129682540894\n",
      "[Training Epoch 12] Batch 10000, Loss 0.23932747542858124\n",
      "[Training Epoch 12] Batch 12500, Loss 0.28578048944473267\n",
      "[Training Epoch 12] Batch 15000, Loss 0.26638516783714294\n",
      "[Training Epoch 12] Batch 17500, Loss 0.3084074854850769\n",
      "[Evluating Epoch 12] HR = 0.6199, NDCG = 0.3545\n",
      " 12: HR = 0.6198675496688741, NDCG = 0.3544802555373842\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.3017612397670746\n",
      "[Training Epoch 13] Batch 2500, Loss 0.21534258127212524\n",
      "[Training Epoch 13] Batch 5000, Loss 0.28390932083129883\n",
      "[Training Epoch 13] Batch 7500, Loss 0.30633479356765747\n",
      "[Training Epoch 13] Batch 10000, Loss 0.22605587542057037\n",
      "[Training Epoch 13] Batch 12500, Loss 0.27590227127075195\n",
      "[Training Epoch 13] Batch 15000, Loss 0.27301520109176636\n",
      "[Training Epoch 13] Batch 17500, Loss 0.3073786497116089\n",
      "[Evluating Epoch 13] HR = 0.6219, NDCG = 0.3552\n",
      " 13: HR = 0.6218543046357616, NDCG = 0.35523920937821596\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.2936638593673706\n",
      "[Training Epoch 14] Batch 2500, Loss 0.22177928686141968\n",
      "[Training Epoch 14] Batch 5000, Loss 0.2574271559715271\n",
      "[Training Epoch 14] Batch 7500, Loss 0.2572312355041504\n",
      "[Training Epoch 14] Batch 10000, Loss 0.3130607008934021\n",
      "[Training Epoch 14] Batch 12500, Loss 0.3186837434768677\n",
      "[Training Epoch 14] Batch 15000, Loss 0.24111834168434143\n",
      "[Training Epoch 14] Batch 17500, Loss 0.2203962653875351\n",
      "[Evluating Epoch 14] HR = 0.6224, NDCG = 0.3582\n",
      " 14: HR = 0.6223509933774835, NDCG = 0.3581910256941243\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.270436555147171\n",
      "[Training Epoch 15] Batch 2500, Loss 0.24993230402469635\n",
      "[Training Epoch 15] Batch 5000, Loss 0.2794955372810364\n",
      "[Training Epoch 15] Batch 7500, Loss 0.23675701022148132\n",
      "[Training Epoch 15] Batch 10000, Loss 0.26139140129089355\n",
      "[Training Epoch 15] Batch 12500, Loss 0.2925095856189728\n",
      "[Training Epoch 15] Batch 15000, Loss 0.2845138609409332\n",
      "[Training Epoch 15] Batch 17500, Loss 0.25428593158721924\n",
      "[Evluating Epoch 15] HR = 0.6238, NDCG = 0.3561\n",
      " 15: HR = 0.623841059602649, NDCG = 0.3560549846817433\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.26992738246917725\n",
      "[Training Epoch 16] Batch 2500, Loss 0.3054286241531372\n",
      "[Training Epoch 16] Batch 5000, Loss 0.3011799454689026\n",
      "[Training Epoch 16] Batch 7500, Loss 0.28813982009887695\n",
      "[Training Epoch 16] Batch 10000, Loss 0.2940897047519684\n",
      "[Training Epoch 16] Batch 12500, Loss 0.2526005208492279\n",
      "[Training Epoch 16] Batch 15000, Loss 0.2826412618160248\n",
      "[Training Epoch 16] Batch 17500, Loss 0.25163882970809937\n",
      "[Evluating Epoch 16] HR = 0.6242, NDCG = 0.3595\n",
      " 16: HR = 0.6241721854304636, NDCG = 0.3595293024643947\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.29991939663887024\n",
      "[Training Epoch 17] Batch 2500, Loss 0.30584079027175903\n",
      "[Training Epoch 17] Batch 5000, Loss 0.34031227231025696\n",
      "[Training Epoch 17] Batch 7500, Loss 0.31718799471855164\n",
      "[Training Epoch 17] Batch 10000, Loss 0.28241461515426636\n",
      "[Training Epoch 17] Batch 12500, Loss 0.29464301466941833\n",
      "[Training Epoch 17] Batch 15000, Loss 0.23584289848804474\n",
      "[Training Epoch 17] Batch 17500, Loss 0.33116859197616577\n",
      "[Evluating Epoch 17] HR = 0.6258, NDCG = 0.3599\n",
      " 17: HR = 0.6258278145695364, NDCG = 0.35987802083398557\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.31071797013282776\n",
      "[Training Epoch 18] Batch 2500, Loss 0.26766371726989746\n",
      "[Training Epoch 18] Batch 5000, Loss 0.2757415771484375\n",
      "[Training Epoch 18] Batch 7500, Loss 0.25329864025115967\n",
      "[Training Epoch 18] Batch 10000, Loss 0.2680348753929138\n",
      "[Training Epoch 18] Batch 12500, Loss 0.21836042404174805\n",
      "[Training Epoch 18] Batch 15000, Loss 0.2879287302494049\n",
      "[Training Epoch 18] Batch 17500, Loss 0.2643425166606903\n",
      "[Evluating Epoch 18] HR = 0.6235, NDCG = 0.3599\n",
      " 18: HR = 0.6235099337748344, NDCG = 0.35993032120138185\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.2849976420402527\n",
      "[Training Epoch 19] Batch 2500, Loss 0.2532148063182831\n",
      "[Training Epoch 19] Batch 5000, Loss 0.30116748809814453\n",
      "[Training Epoch 19] Batch 7500, Loss 0.3026232123374939\n",
      "[Training Epoch 19] Batch 10000, Loss 0.25129884481430054\n",
      "[Training Epoch 19] Batch 12500, Loss 0.278468519449234\n",
      "[Training Epoch 19] Batch 15000, Loss 0.22375135123729706\n",
      "[Training Epoch 19] Batch 17500, Loss 0.26826661825180054\n",
      "[Evluating Epoch 19] HR = 0.6257, NDCG = 0.3607\n",
      " 19: HR = 0.6256622516556292, NDCG = 0.36066137256145786\n",
      ": [16, 16, 8]\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=16, bias=True)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=8, out_features=1, bias=True)\n",
      "MLP(\n",
      "  (embedding_user): Embedding(6040, 8)\n",
      "  (embedding_item): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.7774734497070312\n",
      "[Training Epoch 0] Batch 2500, Loss 0.3540472388267517\n",
      "[Training Epoch 0] Batch 5000, Loss 0.2796211242675781\n",
      "[Training Epoch 0] Batch 7500, Loss 0.39023518562316895\n",
      "[Training Epoch 0] Batch 10000, Loss 0.33235520124435425\n",
      "[Training Epoch 0] Batch 12500, Loss 0.34797728061676025\n",
      "[Training Epoch 0] Batch 15000, Loss 0.30829674005508423\n",
      "[Training Epoch 0] Batch 17500, Loss 0.30822986364364624\n",
      "[Evluating Epoch 0] HR = 0.4464, NDCG = 0.2448\n",
      " 0: HR = 0.4463576158940397, NDCG = 0.2447512914825079\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.377671480178833\n",
      "[Training Epoch 1] Batch 2500, Loss 0.3418058753013611\n",
      "[Training Epoch 1] Batch 5000, Loss 0.39052897691726685\n",
      "[Training Epoch 1] Batch 7500, Loss 0.3711806833744049\n",
      "[Training Epoch 1] Batch 10000, Loss 0.32924124598503113\n",
      "[Training Epoch 1] Batch 12500, Loss 0.38289350271224976\n",
      "[Training Epoch 1] Batch 15000, Loss 0.43994399905204773\n",
      "[Training Epoch 1] Batch 17500, Loss 0.29115837812423706\n",
      "[Evluating Epoch 1] HR = 0.4553, NDCG = 0.2518\n",
      " 1: HR = 0.4552980132450331, NDCG = 0.25183455484007744\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.3833189010620117\n",
      "[Training Epoch 2] Batch 2500, Loss 0.3109109103679657\n",
      "[Training Epoch 2] Batch 5000, Loss 0.2866312861442566\n",
      "[Training Epoch 2] Batch 7500, Loss 0.38479185104370117\n",
      "[Training Epoch 2] Batch 10000, Loss 0.37368959188461304\n",
      "[Training Epoch 2] Batch 12500, Loss 0.30517637729644775\n",
      "[Training Epoch 2] Batch 15000, Loss 0.33082035183906555\n",
      "[Training Epoch 2] Batch 17500, Loss 0.37847691774368286\n",
      "[Evluating Epoch 2] HR = 0.4959, NDCG = 0.2727\n",
      " 2: HR = 0.4958609271523179, NDCG = 0.27266758340360514\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.39051949977874756\n",
      "[Training Epoch 3] Batch 2500, Loss 0.3190224766731262\n",
      "[Training Epoch 3] Batch 5000, Loss 0.31637147068977356\n",
      "[Training Epoch 3] Batch 7500, Loss 0.2918989062309265\n",
      "[Training Epoch 3] Batch 10000, Loss 0.3342125415802002\n",
      "[Training Epoch 3] Batch 12500, Loss 0.31198903918266296\n",
      "[Training Epoch 3] Batch 15000, Loss 0.2976999282836914\n",
      "[Training Epoch 3] Batch 17500, Loss 0.3702620267868042\n",
      "[Evluating Epoch 3] HR = 0.5205, NDCG = 0.2907\n",
      " 3: HR = 0.5205298013245033, NDCG = 0.2906880306205035\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.3576396107673645\n",
      "[Training Epoch 4] Batch 2500, Loss 0.3185845613479614\n",
      "[Training Epoch 4] Batch 5000, Loss 0.32997894287109375\n",
      "[Training Epoch 4] Batch 7500, Loss 0.3352067470550537\n",
      "[Training Epoch 4] Batch 10000, Loss 0.29370516538619995\n",
      "[Training Epoch 4] Batch 12500, Loss 0.24370793998241425\n",
      "[Training Epoch 4] Batch 15000, Loss 0.29055461287498474\n",
      "[Training Epoch 4] Batch 17500, Loss 0.3355814218521118\n",
      "[Evluating Epoch 4] HR = 0.5449, NDCG = 0.3072\n",
      " 4: HR = 0.5448675496688742, NDCG = 0.3072273811284977\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.3103490471839905\n",
      "[Training Epoch 5] Batch 2500, Loss 0.31692200899124146\n",
      "[Training Epoch 5] Batch 5000, Loss 0.349489688873291\n",
      "[Training Epoch 5] Batch 7500, Loss 0.24280115962028503\n",
      "[Training Epoch 5] Batch 10000, Loss 0.30302679538726807\n",
      "[Training Epoch 5] Batch 12500, Loss 0.3343656659126282\n",
      "[Training Epoch 5] Batch 15000, Loss 0.2981361150741577\n",
      "[Training Epoch 5] Batch 17500, Loss 0.2899057865142822\n",
      "[Evluating Epoch 5] HR = 0.5621, NDCG = 0.3149\n",
      " 5: HR = 0.5620860927152318, NDCG = 0.31485059896537215\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.26080065965652466\n",
      "[Training Epoch 6] Batch 2500, Loss 0.3096728026866913\n",
      "[Training Epoch 6] Batch 5000, Loss 0.3192393183708191\n",
      "[Training Epoch 6] Batch 7500, Loss 0.2862721085548401\n",
      "[Training Epoch 6] Batch 10000, Loss 0.32227566838264465\n",
      "[Training Epoch 6] Batch 12500, Loss 0.34938424825668335\n",
      "[Training Epoch 6] Batch 15000, Loss 0.3465026021003723\n",
      "[Training Epoch 6] Batch 17500, Loss 0.3100276589393616\n",
      "[Evluating Epoch 6] HR = 0.5732, NDCG = 0.3187\n",
      " 6: HR = 0.5731788079470199, NDCG = 0.31866211170838693\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.28437498211860657\n",
      "[Training Epoch 7] Batch 2500, Loss 0.27682071924209595\n",
      "[Training Epoch 7] Batch 5000, Loss 0.26661771535873413\n",
      "[Training Epoch 7] Batch 7500, Loss 0.33316099643707275\n",
      "[Training Epoch 7] Batch 10000, Loss 0.311610609292984\n",
      "[Training Epoch 7] Batch 12500, Loss 0.3012155294418335\n",
      "[Training Epoch 7] Batch 15000, Loss 0.30353111028671265\n",
      "[Training Epoch 7] Batch 17500, Loss 0.3404577076435089\n",
      "[Evluating Epoch 7] HR = 0.5810, NDCG = 0.3234\n",
      " 7: HR = 0.5809602649006622, NDCG = 0.32339522273910354\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.2545589804649353\n",
      "[Training Epoch 8] Batch 2500, Loss 0.27716532349586487\n",
      "[Training Epoch 8] Batch 5000, Loss 0.33028098940849304\n",
      "[Training Epoch 8] Batch 7500, Loss 0.24801863729953766\n",
      "[Training Epoch 8] Batch 10000, Loss 0.2652110457420349\n",
      "[Training Epoch 8] Batch 12500, Loss 0.2554762065410614\n",
      "[Training Epoch 8] Batch 15000, Loss 0.3177623748779297\n",
      "[Training Epoch 8] Batch 17500, Loss 0.29688769578933716\n",
      "[Evluating Epoch 8] HR = 0.5834, NDCG = 0.3234\n",
      " 8: HR = 0.5834437086092715, NDCG = 0.32343737095133274\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.32648518681526184\n",
      "[Training Epoch 9] Batch 2500, Loss 0.24594862759113312\n",
      "[Training Epoch 9] Batch 5000, Loss 0.26757586002349854\n",
      "[Training Epoch 9] Batch 7500, Loss 0.30435577034950256\n",
      "[Training Epoch 9] Batch 10000, Loss 0.26453012228012085\n",
      "[Training Epoch 9] Batch 12500, Loss 0.29047903418540955\n",
      "[Training Epoch 9] Batch 15000, Loss 0.29818081855773926\n",
      "[Training Epoch 9] Batch 17500, Loss 0.279341459274292\n",
      "[Evluating Epoch 9] HR = 0.5873, NDCG = 0.3275\n",
      " 9: HR = 0.5872516556291391, NDCG = 0.32745417681733835\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.33364400267601013\n",
      "[Training Epoch 10] Batch 2500, Loss 0.29119765758514404\n",
      "[Training Epoch 10] Batch 5000, Loss 0.2701421082019806\n",
      "[Training Epoch 10] Batch 7500, Loss 0.3009710907936096\n",
      "[Training Epoch 10] Batch 10000, Loss 0.2797110676765442\n",
      "[Training Epoch 10] Batch 12500, Loss 0.2772192358970642\n",
      "[Training Epoch 10] Batch 15000, Loss 0.27688729763031006\n",
      "[Training Epoch 10] Batch 17500, Loss 0.25985974073410034\n",
      "[Evluating Epoch 10] HR = 0.5906, NDCG = 0.3323\n",
      " 10: HR = 0.5905629139072848, NDCG = 0.33233314780803674\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.2497623860836029\n",
      "[Training Epoch 11] Batch 2500, Loss 0.3178633451461792\n",
      "[Training Epoch 11] Batch 5000, Loss 0.29412642121315\n",
      "[Training Epoch 11] Batch 7500, Loss 0.2659534513950348\n",
      "[Training Epoch 11] Batch 10000, Loss 0.27642542123794556\n",
      "[Training Epoch 11] Batch 12500, Loss 0.33736786246299744\n",
      "[Training Epoch 11] Batch 15000, Loss 0.2587239742279053\n",
      "[Training Epoch 11] Batch 17500, Loss 0.2617543935775757\n",
      "[Evluating Epoch 11] HR = 0.5868, NDCG = 0.3305\n",
      " 11: HR = 0.5867549668874172, NDCG = 0.33054218659135626\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.2523532211780548\n",
      "[Training Epoch 12] Batch 2500, Loss 0.3189126253128052\n",
      "[Training Epoch 12] Batch 5000, Loss 0.2604115605354309\n",
      "[Training Epoch 12] Batch 7500, Loss 0.26147621870040894\n",
      "[Training Epoch 12] Batch 10000, Loss 0.2955203056335449\n",
      "[Training Epoch 12] Batch 12500, Loss 0.3079918622970581\n",
      "[Training Epoch 12] Batch 15000, Loss 0.28063082695007324\n",
      "[Training Epoch 12] Batch 17500, Loss 0.4053131937980652\n",
      "[Evluating Epoch 12] HR = 0.5955, NDCG = 0.3347\n",
      " 12: HR = 0.5955298013245033, NDCG = 0.3347482560537021\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.22557175159454346\n",
      "[Training Epoch 13] Batch 2500, Loss 0.2569008469581604\n",
      "[Training Epoch 13] Batch 5000, Loss 0.2766401469707489\n",
      "[Training Epoch 13] Batch 7500, Loss 0.3812240660190582\n",
      "[Training Epoch 13] Batch 10000, Loss 0.3078758716583252\n",
      "[Training Epoch 13] Batch 12500, Loss 0.26176372170448303\n",
      "[Training Epoch 13] Batch 15000, Loss 0.2760452330112457\n",
      "[Training Epoch 13] Batch 17500, Loss 0.3255474865436554\n",
      "[Evluating Epoch 13] HR = 0.5965, NDCG = 0.3381\n",
      " 13: HR = 0.596523178807947, NDCG = 0.33809835088691925\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.26035869121551514\n",
      "[Training Epoch 14] Batch 2500, Loss 0.30464619398117065\n",
      "[Training Epoch 14] Batch 5000, Loss 0.2948392629623413\n",
      "[Training Epoch 14] Batch 7500, Loss 0.3184475004673004\n",
      "[Training Epoch 14] Batch 10000, Loss 0.2712094783782959\n",
      "[Training Epoch 14] Batch 12500, Loss 0.30222633481025696\n",
      "[Training Epoch 14] Batch 15000, Loss 0.27360978722572327\n",
      "[Training Epoch 14] Batch 17500, Loss 0.2714312672615051\n",
      "[Evluating Epoch 14] HR = 0.5929, NDCG = 0.3374\n",
      " 14: HR = 0.5928807947019867, NDCG = 0.3374150668487741\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.27909213304519653\n",
      "[Training Epoch 15] Batch 2500, Loss 0.263136088848114\n",
      "[Training Epoch 15] Batch 5000, Loss 0.2901213765144348\n",
      "[Training Epoch 15] Batch 7500, Loss 0.31038665771484375\n",
      "[Training Epoch 15] Batch 10000, Loss 0.28484371304512024\n",
      "[Training Epoch 15] Batch 12500, Loss 0.27549314498901367\n",
      "[Training Epoch 15] Batch 15000, Loss 0.26220953464508057\n",
      "[Training Epoch 15] Batch 17500, Loss 0.32062578201293945\n",
      "[Evluating Epoch 15] HR = 0.6002, NDCG = 0.3413\n",
      " 15: HR = 0.6001655629139073, NDCG = 0.34134351941888347\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.23254269361495972\n",
      "[Training Epoch 16] Batch 2500, Loss 0.2656632661819458\n",
      "[Training Epoch 16] Batch 5000, Loss 0.2552475929260254\n",
      "[Training Epoch 16] Batch 7500, Loss 0.2837941348552704\n",
      "[Training Epoch 16] Batch 10000, Loss 0.3497476577758789\n",
      "[Training Epoch 16] Batch 12500, Loss 0.26763030886650085\n",
      "[Training Epoch 16] Batch 15000, Loss 0.30613410472869873\n",
      "[Training Epoch 16] Batch 17500, Loss 0.26690250635147095\n",
      "[Evluating Epoch 16] HR = 0.5990, NDCG = 0.3409\n",
      " 16: HR = 0.5990066225165563, NDCG = 0.3408651193178972\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.34818822145462036\n",
      "[Training Epoch 17] Batch 2500, Loss 0.3244495689868927\n",
      "[Training Epoch 17] Batch 5000, Loss 0.2683908939361572\n",
      "[Training Epoch 17] Batch 7500, Loss 0.2980310618877411\n",
      "[Training Epoch 17] Batch 10000, Loss 0.23924444615840912\n",
      "[Training Epoch 17] Batch 12500, Loss 0.23744523525238037\n",
      "[Training Epoch 17] Batch 15000, Loss 0.31291449069976807\n",
      "[Training Epoch 17] Batch 17500, Loss 0.2730712294578552\n",
      "[Evluating Epoch 17] HR = 0.6026, NDCG = 0.3405\n",
      " 17: HR = 0.6026490066225165, NDCG = 0.3404713001611559\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.27744656801223755\n",
      "[Training Epoch 18] Batch 2500, Loss 0.2698427140712738\n",
      "[Training Epoch 18] Batch 5000, Loss 0.24873146414756775\n",
      "[Training Epoch 18] Batch 7500, Loss 0.315516859292984\n",
      "[Training Epoch 18] Batch 10000, Loss 0.26775962114334106\n",
      "[Training Epoch 18] Batch 12500, Loss 0.20344769954681396\n",
      "[Training Epoch 18] Batch 15000, Loss 0.2513837218284607\n",
      "[Training Epoch 18] Batch 17500, Loss 0.26142996549606323\n",
      "[Evluating Epoch 18] HR = 0.6066, NDCG = 0.3433\n",
      " 18: HR = 0.6066225165562914, NDCG = 0.3432879914786008\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.2785438299179077\n",
      "[Training Epoch 19] Batch 2500, Loss 0.24002878367900848\n",
      "[Training Epoch 19] Batch 5000, Loss 0.286327064037323\n",
      "[Training Epoch 19] Batch 7500, Loss 0.32808661460876465\n",
      "[Training Epoch 19] Batch 10000, Loss 0.24550342559814453\n",
      "[Training Epoch 19] Batch 12500, Loss 0.2810012698173523\n",
      "[Training Epoch 19] Batch 15000, Loss 0.2876315116882324\n",
      "[Training Epoch 19] Batch 17500, Loss 0.24558596312999725\n",
      "[Evluating Epoch 19] HR = 0.6076, NDCG = 0.3474\n",
      " 19: HR = 0.6076158940397351, NDCG = 0.3473798724670447\n",
      ": [16, 8]\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=8, out_features=1, bias=True)\n",
      "MLP(\n",
      "  (embedding_user): Embedding(6040, 8)\n",
      "  (embedding_item): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.7059371471405029\n",
      "[Training Epoch 0] Batch 2500, Loss 0.3807097375392914\n",
      "[Training Epoch 0] Batch 5000, Loss 0.4022441804409027\n",
      "[Training Epoch 0] Batch 7500, Loss 0.3716268539428711\n",
      "[Training Epoch 0] Batch 10000, Loss 0.3214215934276581\n",
      "[Training Epoch 0] Batch 12500, Loss 0.3635065257549286\n",
      "[Training Epoch 0] Batch 15000, Loss 0.33435893058776855\n",
      "[Training Epoch 0] Batch 17500, Loss 0.3301306962966919\n",
      "[Evluating Epoch 0] HR = 0.4515, NDCG = 0.2503\n",
      " 0: HR = 0.4514900662251656, NDCG = 0.2502786055821283\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.39438363909721375\n",
      "[Training Epoch 1] Batch 2500, Loss 0.3872140645980835\n",
      "[Training Epoch 1] Batch 5000, Loss 0.37513402104377747\n",
      "[Training Epoch 1] Batch 7500, Loss 0.3346763253211975\n",
      "[Training Epoch 1] Batch 10000, Loss 0.3971512019634247\n",
      "[Training Epoch 1] Batch 12500, Loss 0.331936240196228\n",
      "[Training Epoch 1] Batch 15000, Loss 0.3390408754348755\n",
      "[Training Epoch 1] Batch 17500, Loss 0.4165361523628235\n",
      "[Evluating Epoch 1] HR = 0.4478, NDCG = 0.2489\n",
      " 1: HR = 0.4478476821192053, NDCG = 0.24888327904805782\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.3300110101699829\n",
      "[Training Epoch 2] Batch 2500, Loss 0.3571125864982605\n",
      "[Training Epoch 2] Batch 5000, Loss 0.3644958734512329\n",
      "[Training Epoch 2] Batch 7500, Loss 0.34579697251319885\n",
      "[Training Epoch 2] Batch 10000, Loss 0.36063164472579956\n",
      "[Training Epoch 2] Batch 12500, Loss 0.4327850937843323\n",
      "[Training Epoch 2] Batch 15000, Loss 0.3222830891609192\n",
      "[Training Epoch 2] Batch 17500, Loss 0.31141045689582825\n",
      "[Evluating Epoch 2] HR = 0.4758, NDCG = 0.2645\n",
      " 2: HR = 0.47582781456953643, NDCG = 0.2644832976595614\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.33473414182662964\n",
      "[Training Epoch 3] Batch 2500, Loss 0.3329225778579712\n",
      "[Training Epoch 3] Batch 5000, Loss 0.3328525424003601\n",
      "[Training Epoch 3] Batch 7500, Loss 0.30611932277679443\n",
      "[Training Epoch 3] Batch 10000, Loss 0.35848963260650635\n",
      "[Training Epoch 3] Batch 12500, Loss 0.24697345495224\n",
      "[Training Epoch 3] Batch 15000, Loss 0.3259817361831665\n",
      "[Training Epoch 3] Batch 17500, Loss 0.32174569368362427\n",
      "[Evluating Epoch 3] HR = 0.4929, NDCG = 0.2720\n",
      " 3: HR = 0.49288079470198676, NDCG = 0.27198029874221413\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.3900647759437561\n",
      "[Training Epoch 4] Batch 2500, Loss 0.3568911552429199\n",
      "[Training Epoch 4] Batch 5000, Loss 0.3544851839542389\n",
      "[Training Epoch 4] Batch 7500, Loss 0.32234081625938416\n",
      "[Training Epoch 4] Batch 10000, Loss 0.3264397978782654\n",
      "[Training Epoch 4] Batch 12500, Loss 0.3375396728515625\n",
      "[Training Epoch 4] Batch 15000, Loss 0.27890345454216003\n",
      "[Training Epoch 4] Batch 17500, Loss 0.3254975378513336\n",
      "[Evluating Epoch 4] HR = 0.5031, NDCG = 0.2787\n",
      " 4: HR = 0.5031456953642384, NDCG = 0.2787359176849249\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.3413555920124054\n",
      "[Training Epoch 5] Batch 2500, Loss 0.35809358954429626\n",
      "[Training Epoch 5] Batch 5000, Loss 0.3103613555431366\n",
      "[Training Epoch 5] Batch 7500, Loss 0.3592929244041443\n",
      "[Training Epoch 5] Batch 10000, Loss 0.346077561378479\n",
      "[Training Epoch 5] Batch 12500, Loss 0.28831735253334045\n",
      "[Training Epoch 5] Batch 15000, Loss 0.3556073307991028\n",
      "[Training Epoch 5] Batch 17500, Loss 0.3237344026565552\n",
      "[Evluating Epoch 5] HR = 0.5190, NDCG = 0.2868\n",
      " 5: HR = 0.5190397350993378, NDCG = 0.2867887165393091\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.3501284122467041\n",
      "[Training Epoch 6] Batch 2500, Loss 0.308263897895813\n",
      "[Training Epoch 6] Batch 5000, Loss 0.3305162787437439\n",
      "[Training Epoch 6] Batch 7500, Loss 0.372495174407959\n",
      "[Training Epoch 6] Batch 10000, Loss 0.3088330328464508\n",
      "[Training Epoch 6] Batch 12500, Loss 0.2996748387813568\n",
      "[Training Epoch 6] Batch 15000, Loss 0.28482750058174133\n",
      "[Training Epoch 6] Batch 17500, Loss 0.3183344006538391\n",
      "[Evluating Epoch 6] HR = 0.5387, NDCG = 0.2992\n",
      " 6: HR = 0.5387417218543047, NDCG = 0.2992128533107702\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.35022830963134766\n",
      "[Training Epoch 7] Batch 2500, Loss 0.2670970559120178\n",
      "[Training Epoch 7] Batch 5000, Loss 0.2929535210132599\n",
      "[Training Epoch 7] Batch 7500, Loss 0.30220019817352295\n",
      "[Training Epoch 7] Batch 10000, Loss 0.30813339352607727\n",
      "[Training Epoch 7] Batch 12500, Loss 0.35947760939598083\n",
      "[Training Epoch 7] Batch 15000, Loss 0.291626513004303\n",
      "[Training Epoch 7] Batch 17500, Loss 0.264555960893631\n",
      "[Evluating Epoch 7] HR = 0.5488, NDCG = 0.3044\n",
      " 7: HR = 0.5488410596026491, NDCG = 0.304423743560956\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.2769162058830261\n",
      "[Training Epoch 8] Batch 2500, Loss 0.33435899019241333\n",
      "[Training Epoch 8] Batch 5000, Loss 0.28086572885513306\n",
      "[Training Epoch 8] Batch 7500, Loss 0.2964169979095459\n",
      "[Training Epoch 8] Batch 10000, Loss 0.3006454408168793\n",
      "[Training Epoch 8] Batch 12500, Loss 0.30462655425071716\n",
      "[Training Epoch 8] Batch 15000, Loss 0.2972601652145386\n",
      "[Training Epoch 8] Batch 17500, Loss 0.3039628565311432\n",
      "[Evluating Epoch 8] HR = 0.5528, NDCG = 0.3075\n",
      " 8: HR = 0.5528145695364238, NDCG = 0.30749649747701757\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.33348748087882996\n",
      "[Training Epoch 9] Batch 2500, Loss 0.27613914012908936\n",
      "[Training Epoch 9] Batch 5000, Loss 0.30121707916259766\n",
      "[Training Epoch 9] Batch 7500, Loss 0.26511913537979126\n",
      "[Training Epoch 9] Batch 10000, Loss 0.3036685585975647\n",
      "[Training Epoch 9] Batch 12500, Loss 0.29467475414276123\n",
      "[Training Epoch 9] Batch 15000, Loss 0.33975881338119507\n",
      "[Training Epoch 9] Batch 17500, Loss 0.32562676072120667\n",
      "[Evluating Epoch 9] HR = 0.5593, NDCG = 0.3082\n",
      " 9: HR = 0.5592715231788079, NDCG = 0.3082421154296127\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.2693749666213989\n",
      "[Training Epoch 10] Batch 2500, Loss 0.29775941371917725\n",
      "[Training Epoch 10] Batch 5000, Loss 0.3099777400493622\n",
      "[Training Epoch 10] Batch 7500, Loss 0.36057406663894653\n",
      "[Training Epoch 10] Batch 10000, Loss 0.27320563793182373\n",
      "[Training Epoch 10] Batch 12500, Loss 0.30370959639549255\n",
      "[Training Epoch 10] Batch 15000, Loss 0.31471046805381775\n",
      "[Training Epoch 10] Batch 17500, Loss 0.3145585358142853\n",
      "[Evluating Epoch 10] HR = 0.5659, NDCG = 0.3159\n",
      " 10: HR = 0.5658940397350993, NDCG = 0.31593150613452003\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.3246823251247406\n",
      "[Training Epoch 11] Batch 2500, Loss 0.3075807988643646\n",
      "[Training Epoch 11] Batch 5000, Loss 0.24519827961921692\n",
      "[Training Epoch 11] Batch 7500, Loss 0.22158661484718323\n",
      "[Training Epoch 11] Batch 10000, Loss 0.25815892219543457\n",
      "[Training Epoch 11] Batch 12500, Loss 0.2874718904495239\n",
      "[Training Epoch 11] Batch 15000, Loss 0.2905442714691162\n",
      "[Training Epoch 11] Batch 17500, Loss 0.24997557699680328\n",
      "[Evluating Epoch 11] HR = 0.5644, NDCG = 0.3137\n",
      " 11: HR = 0.5644039735099338, NDCG = 0.31367899823307827\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.2966761887073517\n",
      "[Training Epoch 12] Batch 2500, Loss 0.28544214367866516\n",
      "[Training Epoch 12] Batch 5000, Loss 0.28290605545043945\n",
      "[Training Epoch 12] Batch 7500, Loss 0.38139304518699646\n",
      "[Training Epoch 12] Batch 10000, Loss 0.2676970958709717\n",
      "[Training Epoch 12] Batch 12500, Loss 0.31111806631088257\n",
      "[Training Epoch 12] Batch 15000, Loss 0.271440327167511\n",
      "[Training Epoch 12] Batch 17500, Loss 0.34108781814575195\n",
      "[Evluating Epoch 12] HR = 0.5637, NDCG = 0.3129\n",
      " 12: HR = 0.5637417218543046, NDCG = 0.31286549736715197\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.26093024015426636\n",
      "[Training Epoch 13] Batch 2500, Loss 0.3187085688114166\n",
      "[Training Epoch 13] Batch 5000, Loss 0.3040257692337036\n",
      "[Training Epoch 13] Batch 7500, Loss 0.3212301731109619\n",
      "[Training Epoch 13] Batch 10000, Loss 0.31575441360473633\n",
      "[Training Epoch 13] Batch 12500, Loss 0.27401694655418396\n",
      "[Training Epoch 13] Batch 15000, Loss 0.29308921098709106\n",
      "[Training Epoch 13] Batch 17500, Loss 0.29811689257621765\n",
      "[Evluating Epoch 13] HR = 0.5704, NDCG = 0.3171\n",
      " 13: HR = 0.570364238410596, NDCG = 0.31709267168039984\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.2973552942276001\n",
      "[Training Epoch 14] Batch 2500, Loss 0.2778795659542084\n",
      "[Training Epoch 14] Batch 5000, Loss 0.2956567406654358\n",
      "[Training Epoch 14] Batch 7500, Loss 0.35390233993530273\n",
      "[Training Epoch 14] Batch 10000, Loss 0.24357709288597107\n",
      "[Training Epoch 14] Batch 12500, Loss 0.3167839050292969\n",
      "[Training Epoch 14] Batch 15000, Loss 0.26593828201293945\n",
      "[Training Epoch 14] Batch 17500, Loss 0.28578540682792664\n",
      "[Evluating Epoch 14] HR = 0.5709, NDCG = 0.3184\n",
      " 14: HR = 0.5708609271523178, NDCG = 0.31838477757238504\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.35364657640457153\n",
      "[Training Epoch 15] Batch 2500, Loss 0.31288424134254456\n",
      "[Training Epoch 15] Batch 5000, Loss 0.3241838812828064\n",
      "[Training Epoch 15] Batch 7500, Loss 0.24150271713733673\n",
      "[Training Epoch 15] Batch 10000, Loss 0.30367210507392883\n",
      "[Training Epoch 15] Batch 12500, Loss 0.27996566891670227\n",
      "[Training Epoch 15] Batch 15000, Loss 0.27552974224090576\n",
      "[Training Epoch 15] Batch 17500, Loss 0.2798118591308594\n",
      "[Evluating Epoch 15] HR = 0.5699, NDCG = 0.3185\n",
      " 15: HR = 0.5698675496688742, NDCG = 0.3185348580630472\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.35806742310523987\n",
      "[Training Epoch 16] Batch 2500, Loss 0.26460275053977966\n",
      "[Training Epoch 16] Batch 5000, Loss 0.3169799745082855\n",
      "[Training Epoch 16] Batch 7500, Loss 0.33846694231033325\n",
      "[Training Epoch 16] Batch 10000, Loss 0.3034593462944031\n",
      "[Training Epoch 16] Batch 12500, Loss 0.4032420516014099\n",
      "[Training Epoch 16] Batch 15000, Loss 0.27758660912513733\n",
      "[Training Epoch 16] Batch 17500, Loss 0.23664334416389465\n",
      "[Evluating Epoch 16] HR = 0.5694, NDCG = 0.3200\n",
      " 16: HR = 0.5693708609271523, NDCG = 0.3199758323417055\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.3345200717449188\n",
      "[Training Epoch 17] Batch 2500, Loss 0.3316025137901306\n",
      "[Training Epoch 17] Batch 5000, Loss 0.2837252616882324\n",
      "[Training Epoch 17] Batch 7500, Loss 0.2510252594947815\n",
      "[Training Epoch 17] Batch 10000, Loss 0.3550910949707031\n",
      "[Training Epoch 17] Batch 12500, Loss 0.30504000186920166\n",
      "[Training Epoch 17] Batch 15000, Loss 0.325229287147522\n",
      "[Training Epoch 17] Batch 17500, Loss 0.28098559379577637\n",
      "[Evluating Epoch 17] HR = 0.5750, NDCG = 0.3213\n",
      " 17: HR = 0.575, NDCG = 0.3212976388984038\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.2721130847930908\n",
      "[Training Epoch 18] Batch 2500, Loss 0.2900312840938568\n",
      "[Training Epoch 18] Batch 5000, Loss 0.2789917588233948\n",
      "[Training Epoch 18] Batch 7500, Loss 0.23685407638549805\n",
      "[Training Epoch 18] Batch 10000, Loss 0.302460640668869\n",
      "[Training Epoch 18] Batch 12500, Loss 0.2629120349884033\n",
      "[Training Epoch 18] Batch 15000, Loss 0.23405325412750244\n",
      "[Training Epoch 18] Batch 17500, Loss 0.27729690074920654\n",
      "[Evluating Epoch 18] HR = 0.5745, NDCG = 0.3224\n",
      " 18: HR = 0.5745033112582781, NDCG = 0.32241224321647005\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 10000, Loss 0.2453741729259491\n",
      "[Training Epoch 19] Batch 12500, Loss 0.21671859920024872\n",
      "[Training Epoch 19] Batch 15000, Loss 0.31578710675239563\n",
      "[Training Epoch 19] Batch 17500, Loss 0.35088950395584106\n",
      "[Evluating Epoch 19] HR = 0.5803, NDCG = 0.3245\n",
      " 19: HR = 0.5802980132450332, NDCG = 0.32449722176454476\n",
      ": [16, 64, 32, 16, 8], HR: 0.6396, NDCG: 0.3705\n",
      ": [16, 32, 16, 8], HR: 0.6257, NDCG: 0.3607\n",
      ": [16, 16, 8], HR: 0.6076, NDCG: 0.3474\n",
      ": [16, 8], HR: 0.5803, NDCG: 0.3245\n"
     ]
    }
   ],
   "source": [
    "layer_configs = [\n",
    "    [16, 64, 32, 16, 8],     # \n",
    "    [16, 32, 16, 8],         # \n",
    "    [16, 16, 8],        \n",
    "    [16,8]     \n",
    "]\n",
    "\n",
    "results = []\n",
    "config = mlp_config\n",
    "for layers in layer_configs:\n",
    "    print(f\": {layers}\")\n",
    "    config['layers'] = layers\n",
    "    # \n",
    "    engine = MLPEngine(config)\n",
    "\n",
    "    # \n",
    "    for epoch in range(mlp_config['num_epoch']):\n",
    "        print('Epoch {} starts !'.format(epoch))\n",
    "        print('-' * 80)\n",
    "        train_loader = sample_generator.instance_a_train_loader(config['num_negative'], config['batch_size'])\n",
    "        engine.train_an_epoch(train_loader, epoch_id=epoch)\n",
    "        hit_ratio, ndcg = engine.evaluate(evaluate_data, epoch_id=epoch)\n",
    "        print(f\" {epoch}: HR = {hit_ratio}, NDCG = {ndcg}\")\n",
    "        engine.save(config['alias'], epoch, hit_ratio, ndcg)\n",
    "    # \n",
    "    results.append({\n",
    "        'layers': layers,\n",
    "        'HR': hit_ratio,\n",
    "        'NDCG': ndcg\n",
    "    })\n",
    "\n",
    "# \n",
    "for result in results:\n",
    "    print(f\": {result['layers']}, HR: {result['HR']:.4f}, NDCG: {result['NDCG']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### num_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T15:09:15.170711Z",
     "iopub.status.busy": "2024-05-22T15:09:15.170466Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "number of negatives: 1\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=8, out_features=1, bias=True)\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.6935431957244873\n",
      "[Evluating Epoch 0] HR = 0.4596, NDCG = 0.2543\n",
      "Hit Ratio: 0.4596\n",
      "NDCG: 0.2543\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.455078661441803\n",
      "[Evluating Epoch 1] HR = 0.4856, NDCG = 0.2698\n",
      "Hit Ratio: 0.4856\n",
      "NDCG: 0.2698\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.4544658064842224\n",
      "[Evluating Epoch 2] HR = 0.4947, NDCG = 0.2763\n",
      "Hit Ratio: 0.4947\n",
      "NDCG: 0.2763\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.4202196002006531\n",
      "[Evluating Epoch 3] HR = 0.5068, NDCG = 0.2819\n",
      "Hit Ratio: 0.5068\n",
      "NDCG: 0.2819\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.4030550718307495\n",
      "[Evluating Epoch 4] HR = 0.5315, NDCG = 0.2963\n",
      "Hit Ratio: 0.5315\n",
      "NDCG: 0.2963\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.38060462474823\n",
      "[Evluating Epoch 5] HR = 0.5599, NDCG = 0.3109\n",
      "Hit Ratio: 0.5599\n",
      "NDCG: 0.3109\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.3820163607597351\n",
      "[Evluating Epoch 6] HR = 0.5629, NDCG = 0.3146\n",
      "Hit Ratio: 0.5629\n",
      "NDCG: 0.3146\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.3999258875846863\n",
      "[Evluating Epoch 7] HR = 0.5720, NDCG = 0.3188\n",
      "Hit Ratio: 0.5720\n",
      "NDCG: 0.3188\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.37951260805130005\n",
      "[Evluating Epoch 8] HR = 0.5775, NDCG = 0.3220\n",
      "Hit Ratio: 0.5775\n",
      "NDCG: 0.3220\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.36705148220062256\n",
      "[Evluating Epoch 9] HR = 0.5821, NDCG = 0.3264\n",
      "Hit Ratio: 0.5821\n",
      "NDCG: 0.3264\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.352511465549469\n",
      "[Evluating Epoch 10] HR = 0.5871, NDCG = 0.3291\n",
      "Hit Ratio: 0.5871\n",
      "NDCG: 0.3291\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.3681681156158447\n",
      "[Evluating Epoch 11] HR = 0.5945, NDCG = 0.3346\n",
      "Hit Ratio: 0.5945\n",
      "NDCG: 0.3346\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.357130765914917\n",
      "[Evluating Epoch 12] HR = 0.5990, NDCG = 0.3376\n",
      "Hit Ratio: 0.5990\n",
      "NDCG: 0.3376\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.3579002618789673\n",
      "[Evluating Epoch 13] HR = 0.6060, NDCG = 0.3404\n",
      "Hit Ratio: 0.6060\n",
      "NDCG: 0.3404\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.33933448791503906\n",
      "[Evluating Epoch 14] HR = 0.6106, NDCG = 0.3438\n",
      "Hit Ratio: 0.6106\n",
      "NDCG: 0.3438\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.33320391178131104\n",
      "[Evluating Epoch 15] HR = 0.6161, NDCG = 0.3480\n",
      "Hit Ratio: 0.6161\n",
      "NDCG: 0.3480\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.3372224271297455\n",
      "[Evluating Epoch 16] HR = 0.6179, NDCG = 0.3489\n",
      "Hit Ratio: 0.6179\n",
      "NDCG: 0.3489\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.35492143034935\n",
      "[Evluating Epoch 17] HR = 0.6174, NDCG = 0.3494\n",
      "Hit Ratio: 0.6174\n",
      "NDCG: 0.3494\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.323362797498703\n",
      "[Evluating Epoch 18] HR = 0.6171, NDCG = 0.3508\n",
      "Hit Ratio: 0.6171\n",
      "NDCG: 0.3508\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.36905232071876526\n",
      "[Evluating Epoch 19] HR = 0.6220, NDCG = 0.3522\n",
      "Hit Ratio: 0.6220\n",
      "NDCG: 0.3522\n",
      "--------------------------------------------------------------------------------\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=8, out_features=1, bias=True)\n",
      "MLP(\n",
      "  (embedding_user): Embedding(6040, 8)\n",
      "  (embedding_item): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.6880518198013306\n",
      "[Training Epoch 0] Batch 2500, Loss 0.4432917535305023\n",
      "[Training Epoch 0] Batch 5000, Loss 0.436626136302948\n",
      "[Training Epoch 0] Batch 7500, Loss 0.44310420751571655\n",
      "[Evluating Epoch 0] HR = 0.4505, NDCG = 0.2297\n",
      "Hit Ratio: 0.4505\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.47222018241882324\n",
      "[Training Epoch 1] Batch 2500, Loss 0.495657742023468\n",
      "[Training Epoch 1] Batch 5000, Loss 0.43641430139541626\n",
      "[Training Epoch 1] Batch 7500, Loss 0.4700019955635071\n",
      "[Evluating Epoch 1] HR = 0.4493, NDCG = 0.2502\n",
      "Hit Ratio: 0.4493\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.4673643112182617\n",
      "[Training Epoch 2] Batch 2500, Loss 0.5077625513076782\n",
      "[Training Epoch 2] Batch 5000, Loss 0.4600311517715454\n",
      "[Training Epoch 2] Batch 7500, Loss 0.4645553231239319\n",
      "[Evluating Epoch 2] HR = 0.4485, NDCG = 0.2485\n",
      "Hit Ratio: 0.4485\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.47443410754203796\n",
      "[Training Epoch 3] Batch 2500, Loss 0.4280891716480255\n",
      "[Training Epoch 3] Batch 5000, Loss 0.47068560123443604\n",
      "[Training Epoch 3] Batch 7500, Loss 0.45750129222869873\n",
      "[Evluating Epoch 3] HR = 0.4465, NDCG = 0.2493\n",
      "Hit Ratio: 0.4465\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.5152791738510132\n",
      "[Training Epoch 4] Batch 2500, Loss 0.4520760476589203\n",
      "[Training Epoch 4] Batch 5000, Loss 0.4861217141151428\n",
      "[Training Epoch 4] Batch 7500, Loss 0.44443827867507935\n",
      "[Evluating Epoch 4] HR = 0.4677, NDCG = 0.2582\n",
      "Hit Ratio: 0.4677\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.4756573736667633\n",
      "[Training Epoch 5] Batch 2500, Loss 0.4654410481452942\n",
      "[Training Epoch 5] Batch 5000, Loss 0.4213678240776062\n",
      "[Training Epoch 5] Batch 7500, Loss 0.444275438785553\n",
      "[Evluating Epoch 5] HR = 0.4811, NDCG = 0.2667\n",
      "Hit Ratio: 0.4811\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.4454904794692993\n",
      "[Training Epoch 6] Batch 2500, Loss 0.3750287890434265\n",
      "[Training Epoch 6] Batch 5000, Loss 0.42911383509635925\n",
      "[Training Epoch 6] Batch 7500, Loss 0.4509078562259674\n",
      "[Evluating Epoch 6] HR = 0.4906, NDCG = 0.2701\n",
      "Hit Ratio: 0.4906\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.5082067251205444\n",
      "[Training Epoch 7] Batch 2500, Loss 0.46177369356155396\n",
      "[Training Epoch 7] Batch 5000, Loss 0.4572431445121765\n",
      "[Training Epoch 7] Batch 7500, Loss 0.42011046409606934\n",
      "[Evluating Epoch 7] HR = 0.4945, NDCG = 0.2715\n",
      "Hit Ratio: 0.4945\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.4299450218677521\n",
      "[Training Epoch 8] Batch 2500, Loss 0.3749505877494812\n",
      "[Training Epoch 8] Batch 5000, Loss 0.41469869017601013\n",
      "[Training Epoch 8] Batch 7500, Loss 0.43181222677230835\n",
      "[Evluating Epoch 8] HR = 0.4950, NDCG = 0.2746\n",
      "Hit Ratio: 0.4950\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.43560659885406494\n",
      "[Training Epoch 9] Batch 2500, Loss 0.4400460124015808\n",
      "[Training Epoch 9] Batch 5000, Loss 0.4098581075668335\n",
      "[Training Epoch 9] Batch 7500, Loss 0.4656258821487427\n",
      "[Evluating Epoch 9] HR = 0.4980, NDCG = 0.2756\n",
      "Hit Ratio: 0.4980\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.45107603073120117\n",
      "[Training Epoch 10] Batch 2500, Loss 0.4414297938346863\n",
      "[Training Epoch 10] Batch 5000, Loss 0.3820016384124756\n",
      "[Training Epoch 10] Batch 7500, Loss 0.4495021104812622\n",
      "[Evluating Epoch 10] HR = 0.5099, NDCG = 0.2811\n",
      "Hit Ratio: 0.5099\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.39108362793922424\n",
      "[Training Epoch 11] Batch 2500, Loss 0.4736657440662384\n",
      "[Training Epoch 11] Batch 5000, Loss 0.4346398115158081\n",
      "[Training Epoch 11] Batch 7500, Loss 0.4244924783706665\n",
      "[Evluating Epoch 11] HR = 0.5212, NDCG = 0.2868\n",
      "Hit Ratio: 0.5212\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.43863824009895325\n",
      "[Training Epoch 12] Batch 2500, Loss 0.4079134464263916\n",
      "[Training Epoch 12] Batch 5000, Loss 0.3730037212371826\n",
      "[Training Epoch 12] Batch 7500, Loss 0.4240437150001526\n",
      "[Evluating Epoch 12] HR = 0.5280, NDCG = 0.2892\n",
      "Hit Ratio: 0.5280\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.48263028264045715\n",
      "[Training Epoch 13] Batch 2500, Loss 0.4279436767101288\n",
      "[Training Epoch 13] Batch 5000, Loss 0.35509252548217773\n",
      "[Training Epoch 13] Batch 7500, Loss 0.4202974736690521\n",
      "[Evluating Epoch 13] HR = 0.5359, NDCG = 0.2968\n",
      "Hit Ratio: 0.5359\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.42380380630493164\n",
      "[Training Epoch 14] Batch 2500, Loss 0.36531469225883484\n",
      "[Training Epoch 14] Batch 5000, Loss 0.4164300560951233\n",
      "[Training Epoch 14] Batch 7500, Loss 0.384635329246521\n",
      "[Evluating Epoch 14] HR = 0.5376, NDCG = 0.3000\n",
      "Hit Ratio: 0.5376\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.41505658626556396\n",
      "[Training Epoch 15] Batch 2500, Loss 0.39842793345451355\n",
      "[Training Epoch 15] Batch 5000, Loss 0.4509141445159912\n",
      "[Training Epoch 15] Batch 7500, Loss 0.42745983600616455\n",
      "[Evluating Epoch 15] HR = 0.5454, NDCG = 0.3010\n",
      "Hit Ratio: 0.5454\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.3393021821975708\n",
      "[Training Epoch 16] Batch 2500, Loss 0.4233573079109192\n",
      "[Training Epoch 16] Batch 5000, Loss 0.40760156512260437\n",
      "[Training Epoch 16] Batch 7500, Loss 0.4114261865615845\n",
      "[Evluating Epoch 16] HR = 0.5450, NDCG = 0.3010\n",
      "Hit Ratio: 0.5450\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.41159316897392273\n",
      "[Training Epoch 17] Batch 2500, Loss 0.38895535469055176\n",
      "[Training Epoch 17] Batch 5000, Loss 0.446833074092865\n",
      "[Training Epoch 17] Batch 7500, Loss 0.3498220145702362\n",
      "[Evluating Epoch 17] HR = 0.5457, NDCG = 0.3005\n",
      "Hit Ratio: 0.5457\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.3853175640106201\n",
      "[Training Epoch 18] Batch 2500, Loss 0.4081430435180664\n",
      "[Training Epoch 18] Batch 5000, Loss 0.45026034116744995\n",
      "[Training Epoch 18] Batch 7500, Loss 0.34857821464538574\n",
      "[Evluating Epoch 18] HR = 0.5520, NDCG = 0.3063\n",
      "Hit Ratio: 0.5520\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.35460618138313293\n",
      "[Training Epoch 19] Batch 2500, Loss 0.3652192950248718\n",
      "[Training Epoch 19] Batch 5000, Loss 0.4063858389854431\n",
      "[Training Epoch 19] Batch 7500, Loss 0.4139626622200012\n",
      "[Evluating Epoch 19] HR = 0.5570, NDCG = 0.3053\n",
      "Hit Ratio: 0.5570\n",
      "--------------------------------------------------------------------------------\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=16, bias=True)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=16, out_features=1, bias=True)\n",
      "NeuMF(\n",
      "  (embedding_user_mlp): Embedding(6040, 8)\n",
      "  (embedding_item_mlp): Embedding(3706, 8)\n",
      "  (embedding_user_mf): Embedding(6040, 8)\n",
      "  (embedding_item_mf): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.6932835578918457\n",
      "[Evluating Epoch 0] HR = 0.4447, NDCG = 0.2478\n",
      "Hit Ratio: 0.4447\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.46662622690200806\n",
      "[Evluating Epoch 1] HR = 0.4530, NDCG = 0.2510\n",
      "Hit Ratio: 0.4530\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.4482293128967285\n",
      "[Evluating Epoch 2] HR = 0.4732, NDCG = 0.2623\n",
      "Hit Ratio: 0.4732\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.42332127690315247\n",
      "[Evluating Epoch 3] HR = 0.4990, NDCG = 0.2740\n",
      "Hit Ratio: 0.4990\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.4127384424209595\n",
      "[Evluating Epoch 4] HR = 0.5195, NDCG = 0.2891\n",
      "Hit Ratio: 0.5195\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.393208771944046\n",
      "[Evluating Epoch 5] HR = 0.5376, NDCG = 0.2991\n",
      "Hit Ratio: 0.5376\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.3815057575702667\n",
      "[Evluating Epoch 6] HR = 0.5571, NDCG = 0.3091\n",
      "Hit Ratio: 0.5571\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.4307478070259094\n",
      "[Evluating Epoch 7] HR = 0.5743, NDCG = 0.3197\n",
      "Hit Ratio: 0.5743\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.36765867471694946\n",
      "[Evluating Epoch 8] HR = 0.5916, NDCG = 0.3305\n",
      "Hit Ratio: 0.5916\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.3815437853336334\n",
      "[Evluating Epoch 9] HR = 0.6003, NDCG = 0.3376\n",
      "Hit Ratio: 0.6003\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.36307820677757263\n",
      "[Evluating Epoch 10] HR = 0.6050, NDCG = 0.3415\n",
      "Hit Ratio: 0.6050\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.3619380295276642\n",
      "[Evluating Epoch 11] HR = 0.6134, NDCG = 0.3459\n",
      "Hit Ratio: 0.6134\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.34253937005996704\n",
      "[Evluating Epoch 12] HR = 0.6194, NDCG = 0.3504\n",
      "Hit Ratio: 0.6194\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.31019240617752075\n",
      "[Evluating Epoch 13] HR = 0.6217, NDCG = 0.3517\n",
      "Hit Ratio: 0.6217\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.35419997572898865\n",
      "[Evluating Epoch 14] HR = 0.6278, NDCG = 0.3519\n",
      "Hit Ratio: 0.6278\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.3407275080680847\n",
      "[Evluating Epoch 15] HR = 0.6288, NDCG = 0.3543\n",
      "Hit Ratio: 0.6288\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.367702841758728\n",
      "[Evluating Epoch 16] HR = 0.6349, NDCG = 0.3583\n",
      "Hit Ratio: 0.6349\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Evluating Epoch 17] HR = 0.6359, NDCG = 0.3632\n",
      "Hit Ratio: 0.6359\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.3597831130027771\n",
      "[Evluating Epoch 18] HR = 0.6376, NDCG = 0.3639\n",
      "Hit Ratio: 0.6376\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.3181266784667969\n",
      "[Evluating Epoch 19] HR = 0.6447, NDCG = 0.3684\n",
      "Hit Ratio: 0.6447\n",
      "--------------------------------------------------------------------------------\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "number of negatives: 2\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=8, out_features=1, bias=True)\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.6661321520805359\n",
      "[Training Epoch 0] Batch 2500, Loss 0.41099339723587036\n",
      "[Evluating Epoch 0] HR = 0.4692, NDCG = 0.2613\n",
      "Hit Ratio: 0.4692\n",
      "NDCG: 0.2613\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.4536661207675934\n",
      "[Training Epoch 1] Batch 2500, Loss 0.41828635334968567\n",
      "[Evluating Epoch 1] HR = 0.4871, NDCG = 0.2738\n",
      "Hit Ratio: 0.4871\n",
      "NDCG: 0.2738\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.39615899324417114\n",
      "[Training Epoch 2] Batch 2500, Loss 0.3902682065963745\n",
      "[Evluating Epoch 2] HR = 0.5219, NDCG = 0.2905\n",
      "Hit Ratio: 0.5219\n",
      "NDCG: 0.2905\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.37531423568725586\n",
      "[Training Epoch 3] Batch 2500, Loss 0.40536314249038696\n",
      "[Evluating Epoch 3] HR = 0.5349, NDCG = 0.2982\n",
      "Hit Ratio: 0.5349\n",
      "NDCG: 0.2982\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.41110336780548096\n",
      "[Training Epoch 4] Batch 2500, Loss 0.3780088424682617\n",
      "[Evluating Epoch 4] HR = 0.5548, NDCG = 0.3101\n",
      "Hit Ratio: 0.5548\n",
      "NDCG: 0.3101\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.38212352991104126\n",
      "[Training Epoch 5] Batch 2500, Loss 0.3402882218360901\n",
      "[Evluating Epoch 5] HR = 0.5725, NDCG = 0.3202\n",
      "Hit Ratio: 0.5725\n",
      "NDCG: 0.3202\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.3458610773086548\n",
      "[Training Epoch 6] Batch 2500, Loss 0.36863353848457336\n",
      "[Evluating Epoch 6] HR = 0.5877, NDCG = 0.3324\n",
      "Hit Ratio: 0.5877\n",
      "NDCG: 0.3324\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.35208579897880554\n",
      "[Training Epoch 7] Batch 2500, Loss 0.3475393056869507\n",
      "[Evluating Epoch 7] HR = 0.5997, NDCG = 0.3391\n",
      "Hit Ratio: 0.5997\n",
      "NDCG: 0.3391\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.3193918764591217\n",
      "[Training Epoch 8] Batch 2500, Loss 0.37856143712997437\n",
      "[Evluating Epoch 8] HR = 0.6089, NDCG = 0.3435\n",
      "Hit Ratio: 0.6089\n",
      "NDCG: 0.3435\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.3163667321205139\n",
      "[Training Epoch 9] Batch 2500, Loss 0.3407645523548126\n",
      "[Evluating Epoch 9] HR = 0.6116, NDCG = 0.3459\n",
      "Hit Ratio: 0.6116\n",
      "NDCG: 0.3459\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.3525129556655884\n",
      "[Training Epoch 10] Batch 2500, Loss 0.32162725925445557\n",
      "[Evluating Epoch 10] HR = 0.6172, NDCG = 0.3489\n",
      "Hit Ratio: 0.6172\n",
      "NDCG: 0.3489\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.3295169472694397\n",
      "[Training Epoch 11] Batch 2500, Loss 0.3258189558982849\n",
      "[Evluating Epoch 11] HR = 0.6243, NDCG = 0.3526\n",
      "Hit Ratio: 0.6243\n",
      "NDCG: 0.3526\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.3351881802082062\n",
      "[Training Epoch 12] Batch 2500, Loss 0.336948037147522\n",
      "[Evluating Epoch 12] HR = 0.6252, NDCG = 0.3553\n",
      "Hit Ratio: 0.6252\n",
      "NDCG: 0.3553\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.3319137990474701\n",
      "[Training Epoch 13] Batch 2500, Loss 0.34509187936782837\n",
      "[Evluating Epoch 13] HR = 0.6291, NDCG = 0.3586\n",
      "Hit Ratio: 0.6291\n",
      "NDCG: 0.3586\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.3307363986968994\n",
      "[Training Epoch 14] Batch 2500, Loss 0.3406350612640381\n",
      "[Evluating Epoch 14] HR = 0.6300, NDCG = 0.3608\n",
      "Hit Ratio: 0.6300\n",
      "NDCG: 0.3608\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.32692408561706543\n",
      "[Training Epoch 15] Batch 2500, Loss 0.32997849583625793\n",
      "[Evluating Epoch 15] HR = 0.6308, NDCG = 0.3593\n",
      "Hit Ratio: 0.6308\n",
      "NDCG: 0.3593\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.3268287777900696\n",
      "[Training Epoch 16] Batch 2500, Loss 0.3151192367076874\n",
      "[Evluating Epoch 16] HR = 0.6316, NDCG = 0.3617\n",
      "Hit Ratio: 0.6316\n",
      "NDCG: 0.3617\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.3258712887763977\n",
      "[Training Epoch 17] Batch 2500, Loss 0.33378392457962036\n",
      "[Evluating Epoch 17] HR = 0.6331, NDCG = 0.3610\n",
      "Hit Ratio: 0.6331\n",
      "NDCG: 0.3610\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.3126683235168457\n",
      "[Training Epoch 18] Batch 2500, Loss 0.3403523564338684\n",
      "[Evluating Epoch 18] HR = 0.6315, NDCG = 0.3635\n",
      "Hit Ratio: 0.6315\n",
      "NDCG: 0.3635\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.33794698119163513\n",
      "[Training Epoch 19] Batch 2500, Loss 0.3067103922367096\n",
      "[Evluating Epoch 19] HR = 0.6334, NDCG = 0.3627\n",
      "Hit Ratio: 0.6334\n",
      "NDCG: 0.3627\n",
      "--------------------------------------------------------------------------------\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=8, out_features=1, bias=True)\n",
      "MLP(\n",
      "  (embedding_user): Embedding(6040, 8)\n",
      "  (embedding_item): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.6565536856651306\n",
      "[Training Epoch 0] Batch 2500, Loss 0.4648866653442383\n",
      "[Training Epoch 0] Batch 5000, Loss 0.48602041602134705\n",
      "[Training Epoch 0] Batch 7500, Loss 0.4554997682571411\n",
      "[Training Epoch 0] Batch 10000, Loss 0.4013822674751282\n",
      "[Evluating Epoch 0] HR = 0.4483, NDCG = 0.2507\n",
      "Hit Ratio: 0.4483\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.5295959711074829\n",
      "[Training Epoch 1] Batch 2500, Loss 0.45558831095695496\n",
      "[Training Epoch 1] Batch 5000, Loss 0.41746973991394043\n",
      "[Training Epoch 1] Batch 7500, Loss 0.46277034282684326\n",
      "[Training Epoch 1] Batch 10000, Loss 0.46822452545166016\n",
      "[Evluating Epoch 1] HR = 0.4480, NDCG = 0.2493\n",
      "Hit Ratio: 0.4480\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.44641464948654175\n",
      "[Training Epoch 2] Batch 2500, Loss 0.41756004095077515\n",
      "[Training Epoch 2] Batch 5000, Loss 0.4276205599308014\n",
      "[Training Epoch 2] Batch 7500, Loss 0.3975122272968292\n",
      "[Training Epoch 2] Batch 10000, Loss 0.48903167247772217\n",
      "[Evluating Epoch 2] HR = 0.4480, NDCG = 0.2481\n",
      "Hit Ratio: 0.4480\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.4451417326927185\n",
      "[Training Epoch 3] Batch 2500, Loss 0.4256860315799713\n",
      "[Training Epoch 3] Batch 5000, Loss 0.408902645111084\n",
      "[Training Epoch 3] Batch 7500, Loss 0.41551417112350464\n",
      "[Training Epoch 3] Batch 10000, Loss 0.42942115664482117\n",
      "[Evluating Epoch 3] HR = 0.4709, NDCG = 0.2648\n",
      "Hit Ratio: 0.4709\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.40295350551605225\n",
      "[Training Epoch 4] Batch 2500, Loss 0.40999919176101685\n",
      "[Training Epoch 4] Batch 5000, Loss 0.4013271927833557\n",
      "[Training Epoch 4] Batch 7500, Loss 0.4350718557834625\n",
      "[Training Epoch 4] Batch 10000, Loss 0.41887420415878296\n",
      "[Evluating Epoch 4] HR = 0.4897, NDCG = 0.2730\n",
      "Hit Ratio: 0.4897\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.46611589193344116\n",
      "[Training Epoch 5] Batch 2500, Loss 0.3657519817352295\n",
      "[Training Epoch 5] Batch 5000, Loss 0.39567428827285767\n",
      "[Training Epoch 5] Batch 7500, Loss 0.3647609055042267\n",
      "[Training Epoch 5] Batch 10000, Loss 0.33936166763305664\n",
      "[Evluating Epoch 5] HR = 0.4980, NDCG = 0.2776\n",
      "Hit Ratio: 0.4980\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.3834681808948517\n",
      "[Training Epoch 6] Batch 2500, Loss 0.4128144085407257\n",
      "[Training Epoch 6] Batch 5000, Loss 0.38649410009384155\n",
      "[Training Epoch 6] Batch 7500, Loss 0.3655702769756317\n",
      "[Training Epoch 6] Batch 10000, Loss 0.4298478364944458\n",
      "[Evluating Epoch 6] HR = 0.5026, NDCG = 0.2791\n",
      "Hit Ratio: 0.5026\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.43593549728393555\n",
      "[Training Epoch 7] Batch 2500, Loss 0.43454769253730774\n",
      "[Training Epoch 7] Batch 5000, Loss 0.35591065883636475\n",
      "[Training Epoch 7] Batch 7500, Loss 0.4277029037475586\n",
      "[Training Epoch 7] Batch 10000, Loss 0.3436935842037201\n",
      "[Evluating Epoch 7] HR = 0.5108, NDCG = 0.2854\n",
      "Hit Ratio: 0.5108\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.35712748765945435\n",
      "[Training Epoch 8] Batch 2500, Loss 0.38015300035476685\n",
      "[Training Epoch 8] Batch 5000, Loss 0.3432903289794922\n",
      "[Training Epoch 8] Batch 7500, Loss 0.4153918921947479\n",
      "[Training Epoch 8] Batch 10000, Loss 0.35031214356422424\n",
      "[Evluating Epoch 8] HR = 0.5154, NDCG = 0.2893\n",
      "Hit Ratio: 0.5154\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.39504414796829224\n",
      "[Training Epoch 9] Batch 2500, Loss 0.42358696460723877\n",
      "[Training Epoch 9] Batch 5000, Loss 0.3545846939086914\n",
      "[Training Epoch 9] Batch 7500, Loss 0.3214692175388336\n",
      "[Training Epoch 9] Batch 10000, Loss 0.38947203755378723\n",
      "[Evluating Epoch 9] HR = 0.5235, NDCG = 0.2930\n",
      "Hit Ratio: 0.5235\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.3766394257545471\n",
      "[Training Epoch 10] Batch 2500, Loss 0.45438140630722046\n",
      "[Training Epoch 10] Batch 5000, Loss 0.4226706027984619\n",
      "[Training Epoch 10] Batch 7500, Loss 0.40690332651138306\n",
      "[Training Epoch 10] Batch 10000, Loss 0.3604625165462494\n",
      "[Evluating Epoch 10] HR = 0.5296, NDCG = 0.2977\n",
      "Hit Ratio: 0.5296\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.3723617494106293\n",
      "[Training Epoch 11] Batch 2500, Loss 0.43286052346229553\n",
      "[Training Epoch 11] Batch 5000, Loss 0.3584037125110626\n",
      "[Training Epoch 11] Batch 7500, Loss 0.4231114387512207\n",
      "[Training Epoch 11] Batch 10000, Loss 0.3980255126953125\n",
      "[Evluating Epoch 11] HR = 0.5291, NDCG = 0.2979\n",
      "Hit Ratio: 0.5291\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.37742817401885986\n",
      "[Training Epoch 12] Batch 2500, Loss 0.38428995013237\n",
      "[Training Epoch 12] Batch 5000, Loss 0.3842221796512604\n",
      "[Training Epoch 12] Batch 7500, Loss 0.3820331394672394\n",
      "[Training Epoch 12] Batch 10000, Loss 0.3593183755874634\n",
      "[Evluating Epoch 12] HR = 0.5366, NDCG = 0.3004\n",
      "Hit Ratio: 0.5366\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.44715753197669983\n",
      "[Training Epoch 13] Batch 2500, Loss 0.35632312297821045\n",
      "[Training Epoch 13] Batch 5000, Loss 0.4347103238105774\n",
      "[Training Epoch 13] Batch 7500, Loss 0.4249797463417053\n",
      "[Training Epoch 13] Batch 10000, Loss 0.40352970361709595\n",
      "[Evluating Epoch 13] HR = 0.5416, NDCG = 0.3032\n",
      "Hit Ratio: 0.5416\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.38045549392700195\n",
      "[Training Epoch 14] Batch 2500, Loss 0.37650007009506226\n",
      "[Training Epoch 14] Batch 5000, Loss 0.4082390367984772\n",
      "[Training Epoch 14] Batch 7500, Loss 0.35093292593955994\n",
      "[Training Epoch 14] Batch 10000, Loss 0.34392836689949036\n",
      "[Evluating Epoch 14] HR = 0.5531, NDCG = 0.3084\n",
      "Hit Ratio: 0.5531\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.36721131205558777\n",
      "[Training Epoch 15] Batch 2500, Loss 0.37963712215423584\n",
      "[Training Epoch 15] Batch 5000, Loss 0.3786710500717163\n",
      "[Training Epoch 15] Batch 7500, Loss 0.32332706451416016\n",
      "[Training Epoch 15] Batch 10000, Loss 0.374961256980896\n",
      "[Evluating Epoch 15] HR = 0.5589, NDCG = 0.3123\n",
      "Hit Ratio: 0.5589\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.3563570976257324\n",
      "[Training Epoch 16] Batch 2500, Loss 0.35084545612335205\n",
      "[Training Epoch 16] Batch 5000, Loss 0.31254953145980835\n",
      "[Training Epoch 16] Batch 7500, Loss 0.33067241311073303\n",
      "[Training Epoch 16] Batch 10000, Loss 0.3269802927970886\n",
      "[Evluating Epoch 16] HR = 0.5608, NDCG = 0.3119\n",
      "Hit Ratio: 0.5608\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.4098874628543854\n",
      "[Training Epoch 17] Batch 2500, Loss 0.4205496907234192\n",
      "[Training Epoch 17] Batch 5000, Loss 0.41331374645233154\n",
      "[Training Epoch 17] Batch 7500, Loss 0.38246235251426697\n",
      "[Training Epoch 17] Batch 10000, Loss 0.4351566433906555\n",
      "[Evluating Epoch 17] HR = 0.5631, NDCG = 0.3143\n",
      "Hit Ratio: 0.5631\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.34470224380493164\n",
      "[Training Epoch 18] Batch 2500, Loss 0.35148996114730835\n",
      "[Training Epoch 18] Batch 5000, Loss 0.3331899046897888\n",
      "[Training Epoch 18] Batch 7500, Loss 0.34590208530426025\n",
      "[Training Epoch 18] Batch 10000, Loss 0.37536323070526123\n",
      "[Evluating Epoch 18] HR = 0.5666, NDCG = 0.3175\n",
      "Hit Ratio: 0.5666\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.3909814655780792\n",
      "[Training Epoch 19] Batch 2500, Loss 0.376765638589859\n",
      "[Training Epoch 19] Batch 5000, Loss 0.3905380368232727\n",
      "[Training Epoch 19] Batch 7500, Loss 0.35955893993377686\n",
      "[Training Epoch 19] Batch 10000, Loss 0.3311361074447632\n",
      "[Evluating Epoch 19] HR = 0.5714, NDCG = 0.3175\n",
      "Hit Ratio: 0.5714\n",
      "--------------------------------------------------------------------------------\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=16, bias=True)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=16, out_features=1, bias=True)\n",
      "NeuMF(\n",
      "  (embedding_user_mlp): Embedding(6040, 8)\n",
      "  (embedding_item_mlp): Embedding(3706, 8)\n",
      "  (embedding_user_mf): Embedding(6040, 8)\n",
      "  (embedding_item_mf): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.6820954084396362\n",
      "[Training Epoch 0] Batch 2500, Loss 0.4079437255859375\n",
      "[Evluating Epoch 0] HR = 0.4825, NDCG = 0.2693\n",
      "Hit Ratio: 0.4825\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.3872901201248169\n",
      "[Training Epoch 1] Batch 2500, Loss 0.3977150619029999\n",
      "[Evluating Epoch 1] HR = 0.5409, NDCG = 0.3035\n",
      "Hit Ratio: 0.5409\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.372575044631958\n",
      "[Training Epoch 2] Batch 2500, Loss 0.3673757314682007\n",
      "[Evluating Epoch 2] HR = 0.5821, NDCG = 0.3265\n",
      "Hit Ratio: 0.5821\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.348877876996994\n",
      "[Training Epoch 3] Batch 2500, Loss 0.33406388759613037\n",
      "[Evluating Epoch 3] HR = 0.6017, NDCG = 0.3391\n",
      "Hit Ratio: 0.6017\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.3160806894302368\n",
      "[Training Epoch 4] Batch 2500, Loss 0.33503642678260803\n",
      "[Evluating Epoch 4] HR = 0.6224, NDCG = 0.3547\n",
      "Hit Ratio: 0.6224\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.35911402106285095\n",
      "[Training Epoch 5] Batch 2500, Loss 0.3454998731613159\n",
      "[Evluating Epoch 5] HR = 0.6306, NDCG = 0.3597\n",
      "Hit Ratio: 0.6306\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.3180478513240814\n",
      "[Training Epoch 6] Batch 2500, Loss 0.31667613983154297\n",
      "[Evluating Epoch 6] HR = 0.6401, NDCG = 0.3680\n",
      "Hit Ratio: 0.6401\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.32787054777145386\n",
      "[Training Epoch 7] Batch 2500, Loss 0.31485694646835327\n",
      "[Evluating Epoch 7] HR = 0.6449, NDCG = 0.3716\n",
      "Hit Ratio: 0.6449\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.30840176343917847\n",
      "[Training Epoch 8] Batch 2500, Loss 0.33869343996047974\n",
      "[Evluating Epoch 8] HR = 0.6444, NDCG = 0.3722\n",
      "Hit Ratio: 0.6444\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.32894378900527954\n",
      "[Training Epoch 9] Batch 2500, Loss 0.31000572443008423\n",
      "[Evluating Epoch 9] HR = 0.6444, NDCG = 0.3734\n",
      "Hit Ratio: 0.6444\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.3132549524307251\n",
      "[Training Epoch 10] Batch 2500, Loss 0.31267786026000977\n",
      "[Evluating Epoch 10] HR = 0.6541, NDCG = 0.3798\n",
      "Hit Ratio: 0.6541\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.2997053265571594\n",
      "[Training Epoch 11] Batch 2500, Loss 0.30706310272216797\n",
      "[Evluating Epoch 11] HR = 0.6502, NDCG = 0.3773\n",
      "Hit Ratio: 0.6502\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.3065565526485443\n",
      "[Training Epoch 12] Batch 2500, Loss 0.3241182565689087\n",
      "[Evluating Epoch 12] HR = 0.6528, NDCG = 0.3810\n",
      "Hit Ratio: 0.6528\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.3021252155303955\n",
      "[Training Epoch 13] Batch 2500, Loss 0.32353565096855164\n",
      "[Evluating Epoch 13] HR = 0.6555, NDCG = 0.3828\n",
      "Hit Ratio: 0.6555\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.3101910650730133\n",
      "[Training Epoch 14] Batch 2500, Loss 0.32465893030166626\n",
      "[Evluating Epoch 14] HR = 0.6591, NDCG = 0.3842\n",
      "Hit Ratio: 0.6591\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.29325342178344727\n",
      "[Training Epoch 15] Batch 2500, Loss 0.3069344162940979\n",
      "[Evluating Epoch 15] HR = 0.6555, NDCG = 0.3820\n",
      "Hit Ratio: 0.6555\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.29932689666748047\n",
      "[Training Epoch 16] Batch 2500, Loss 0.297102689743042\n",
      "[Evluating Epoch 16] HR = 0.6586, NDCG = 0.3877\n",
      "Hit Ratio: 0.6586\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.32143330574035645\n",
      "[Training Epoch 17] Batch 2500, Loss 0.3329756557941437\n",
      "[Evluating Epoch 17] HR = 0.6631, NDCG = 0.3889\n",
      "Hit Ratio: 0.6631\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.2874917984008789\n",
      "[Training Epoch 18] Batch 2500, Loss 0.2933886647224426\n",
      "[Evluating Epoch 18] HR = 0.6588, NDCG = 0.3909\n",
      "Hit Ratio: 0.6588\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.26939624547958374\n",
      "[Training Epoch 19] Batch 2500, Loss 0.3087133467197418\n",
      "[Evluating Epoch 19] HR = 0.6611, NDCG = 0.3892\n",
      "Hit Ratio: 0.6611\n",
      "--------------------------------------------------------------------------------\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "number of negatives: 3\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=8, out_features=1, bias=True)\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.7088791728019714\n",
      "[Training Epoch 0] Batch 2500, Loss 0.3951125741004944\n",
      "[Evluating Epoch 0] HR = 0.4483, NDCG = 0.2479\n",
      "Hit Ratio: 0.4483\n",
      "NDCG: 0.2479\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.3485074043273926\n",
      "[Training Epoch 1] Batch 2500, Loss 0.397381454706192\n",
      "[Evluating Epoch 1] HR = 0.4884, NDCG = 0.2708\n",
      "Hit Ratio: 0.4884\n",
      "NDCG: 0.2708\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.39035189151763916\n",
      "[Training Epoch 2] Batch 2500, Loss 0.3747212886810303\n",
      "[Evluating Epoch 2] HR = 0.5359, NDCG = 0.2975\n",
      "Hit Ratio: 0.5359\n",
      "NDCG: 0.2975\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.3334536552429199\n",
      "[Training Epoch 3] Batch 2500, Loss 0.3409324288368225\n",
      "[Evluating Epoch 3] HR = 0.5647, NDCG = 0.3161\n",
      "Hit Ratio: 0.5647\n",
      "NDCG: 0.3161\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.33333587646484375\n",
      "[Training Epoch 4] Batch 2500, Loss 0.3299878239631653\n",
      "[Evluating Epoch 4] HR = 0.5846, NDCG = 0.3280\n",
      "Hit Ratio: 0.5846\n",
      "NDCG: 0.3280\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.3080987334251404\n",
      "[Training Epoch 5] Batch 2500, Loss 0.33247002959251404\n",
      "[Evluating Epoch 5] HR = 0.6051, NDCG = 0.3420\n",
      "Hit Ratio: 0.6051\n",
      "NDCG: 0.3420\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.310901403427124\n",
      "[Training Epoch 6] Batch 2500, Loss 0.28972703218460083\n",
      "[Evluating Epoch 6] HR = 0.6088, NDCG = 0.3455\n",
      "Hit Ratio: 0.6088\n",
      "NDCG: 0.3455\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.2650254964828491\n",
      "[Training Epoch 7] Batch 2500, Loss 0.30427125096321106\n",
      "[Evluating Epoch 7] HR = 0.6152, NDCG = 0.3514\n",
      "Hit Ratio: 0.6152\n",
      "NDCG: 0.3514\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.2869934141635895\n",
      "[Training Epoch 8] Batch 2500, Loss 0.2982458770275116\n",
      "[Evluating Epoch 8] HR = 0.6210, NDCG = 0.3558\n",
      "Hit Ratio: 0.6210\n",
      "NDCG: 0.3558\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.3014433979988098\n",
      "[Training Epoch 9] Batch 2500, Loss 0.3016160726547241\n",
      "[Evluating Epoch 9] HR = 0.6293, NDCG = 0.3591\n",
      "Hit Ratio: 0.6293\n",
      "NDCG: 0.3591\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.2998120188713074\n",
      "[Training Epoch 10] Batch 2500, Loss 0.3046260476112366\n",
      "[Evluating Epoch 10] HR = 0.6320, NDCG = 0.3594\n",
      "Hit Ratio: 0.6320\n",
      "NDCG: 0.3594\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.263639897108078\n",
      "[Training Epoch 11] Batch 2500, Loss 0.307582825422287\n",
      "[Evluating Epoch 11] HR = 0.6320, NDCG = 0.3599\n",
      "Hit Ratio: 0.6320\n",
      "NDCG: 0.3599\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.3235304355621338\n",
      "[Training Epoch 12] Batch 2500, Loss 0.2831141948699951\n",
      "[Evluating Epoch 12] HR = 0.6283, NDCG = 0.3595\n",
      "Hit Ratio: 0.6283\n",
      "NDCG: 0.3595\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.30434611439704895\n",
      "[Training Epoch 13] Batch 2500, Loss 0.28308168053627014\n",
      "[Evluating Epoch 13] HR = 0.6346, NDCG = 0.3618\n",
      "Hit Ratio: 0.6346\n",
      "NDCG: 0.3618\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.3098289370536804\n",
      "[Training Epoch 14] Batch 2500, Loss 0.29547256231307983\n",
      "[Evluating Epoch 14] HR = 0.6320, NDCG = 0.3617\n",
      "Hit Ratio: 0.6320\n",
      "NDCG: 0.3617\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.27560698986053467\n",
      "[Training Epoch 15] Batch 2500, Loss 0.29764217138290405\n",
      "[Evluating Epoch 15] HR = 0.6301, NDCG = 0.3619\n",
      "Hit Ratio: 0.6301\n",
      "NDCG: 0.3619\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.2920961380004883\n",
      "[Training Epoch 16] Batch 2500, Loss 0.2831971049308777\n",
      "[Evluating Epoch 16] HR = 0.6331, NDCG = 0.3621\n",
      "Hit Ratio: 0.6331\n",
      "NDCG: 0.3621\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.3060074746608734\n",
      "[Training Epoch 17] Batch 2500, Loss 0.3166101574897766\n",
      "[Evluating Epoch 17] HR = 0.6310, NDCG = 0.3604\n",
      "Hit Ratio: 0.6310\n",
      "NDCG: 0.3604\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.2794097065925598\n",
      "[Training Epoch 18] Batch 2500, Loss 0.28950172662734985\n",
      "[Evluating Epoch 18] HR = 0.6364, NDCG = 0.3635\n",
      "Hit Ratio: 0.6364\n",
      "NDCG: 0.3635\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.2772344946861267\n",
      "[Training Epoch 19] Batch 2500, Loss 0.27666354179382324\n",
      "[Evluating Epoch 19] HR = 0.6381, NDCG = 0.3662\n",
      "Hit Ratio: 0.6381\n",
      "NDCG: 0.3662\n",
      "--------------------------------------------------------------------------------\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=8, out_features=1, bias=True)\n",
      "MLP(\n",
      "  (embedding_user): Embedding(6040, 8)\n",
      "  (embedding_item): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.7649184465408325\n",
      "[Training Epoch 0] Batch 2500, Loss 0.4407133162021637\n",
      "[Training Epoch 0] Batch 5000, Loss 0.40639057755470276\n",
      "[Training Epoch 0] Batch 7500, Loss 0.3875882923603058\n",
      "[Training Epoch 0] Batch 10000, Loss 0.3859943449497223\n",
      "[Training Epoch 0] Batch 12500, Loss 0.3801257908344269\n",
      "[Training Epoch 0] Batch 15000, Loss 0.3518778085708618\n",
      "[Evluating Epoch 0] HR = 0.4490, NDCG = 0.2453\n",
      "Hit Ratio: 0.4490\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.39454591274261475\n",
      "[Training Epoch 1] Batch 2500, Loss 0.37860602140426636\n",
      "[Training Epoch 1] Batch 5000, Loss 0.37792354822158813\n",
      "[Training Epoch 1] Batch 7500, Loss 0.37809211015701294\n",
      "[Training Epoch 1] Batch 10000, Loss 0.3477190136909485\n",
      "[Training Epoch 1] Batch 12500, Loss 0.38305020332336426\n",
      "[Training Epoch 1] Batch 15000, Loss 0.35397499799728394\n",
      "[Evluating Epoch 1] HR = 0.4490, NDCG = 0.2495\n",
      "Hit Ratio: 0.4490\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.3679330348968506\n",
      "[Training Epoch 2] Batch 2500, Loss 0.4019894003868103\n",
      "[Training Epoch 2] Batch 5000, Loss 0.36860519647598267\n",
      "[Training Epoch 2] Batch 7500, Loss 0.3920651078224182\n",
      "[Training Epoch 2] Batch 10000, Loss 0.45133939385414124\n",
      "[Training Epoch 2] Batch 12500, Loss 0.377854585647583\n",
      "[Training Epoch 2] Batch 15000, Loss 0.36609986424446106\n",
      "[Evluating Epoch 2] HR = 0.4719, NDCG = 0.2629\n",
      "Hit Ratio: 0.4719\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.3976820707321167\n",
      "[Training Epoch 3] Batch 2500, Loss 0.3541069030761719\n",
      "[Training Epoch 3] Batch 5000, Loss 0.2929500937461853\n",
      "[Training Epoch 3] Batch 7500, Loss 0.3491959869861603\n",
      "[Training Epoch 3] Batch 10000, Loss 0.3960167169570923\n",
      "[Training Epoch 3] Batch 12500, Loss 0.3808959722518921\n",
      "[Training Epoch 3] Batch 15000, Loss 0.36067238450050354\n",
      "[Evluating Epoch 3] HR = 0.4884, NDCG = 0.2708\n",
      "Hit Ratio: 0.4884\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.3339858651161194\n",
      "[Training Epoch 4] Batch 2500, Loss 0.35436761379241943\n",
      "[Training Epoch 4] Batch 5000, Loss 0.33860403299331665\n",
      "[Training Epoch 4] Batch 7500, Loss 0.3683375418186188\n",
      "[Training Epoch 4] Batch 10000, Loss 0.2782860994338989\n",
      "[Training Epoch 4] Batch 12500, Loss 0.34955894947052\n",
      "[Training Epoch 4] Batch 15000, Loss 0.36005526781082153\n",
      "[Evluating Epoch 4] HR = 0.4970, NDCG = 0.2775\n",
      "Hit Ratio: 0.4970\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.4041997194290161\n",
      "[Training Epoch 5] Batch 2500, Loss 0.39747780561447144\n",
      "[Training Epoch 5] Batch 5000, Loss 0.43178194761276245\n",
      "[Training Epoch 5] Batch 7500, Loss 0.38040053844451904\n",
      "[Training Epoch 5] Batch 10000, Loss 0.39529281854629517\n",
      "[Training Epoch 5] Batch 12500, Loss 0.3331449627876282\n",
      "[Training Epoch 5] Batch 15000, Loss 0.3746291995048523\n",
      "[Evluating Epoch 5] HR = 0.5119, NDCG = 0.2851\n",
      "Hit Ratio: 0.5119\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.3942614197731018\n",
      "[Training Epoch 6] Batch 2500, Loss 0.29022276401519775\n",
      "[Training Epoch 6] Batch 5000, Loss 0.35068655014038086\n",
      "[Training Epoch 6] Batch 7500, Loss 0.32178348302841187\n",
      "[Training Epoch 6] Batch 10000, Loss 0.33040544390678406\n",
      "[Training Epoch 6] Batch 12500, Loss 0.34107404947280884\n",
      "[Training Epoch 6] Batch 15000, Loss 0.34825754165649414\n",
      "[Evluating Epoch 6] HR = 0.5329, NDCG = 0.2969\n",
      "Hit Ratio: 0.5329\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.33230268955230713\n",
      "[Training Epoch 7] Batch 2500, Loss 0.3752681016921997\n",
      "[Training Epoch 7] Batch 5000, Loss 0.36099573969841003\n",
      "[Training Epoch 7] Batch 7500, Loss 0.3150666356086731\n",
      "[Training Epoch 7] Batch 10000, Loss 0.3838825225830078\n",
      "[Training Epoch 7] Batch 12500, Loss 0.34553107619285583\n",
      "[Training Epoch 7] Batch 15000, Loss 0.3146604001522064\n",
      "[Evluating Epoch 7] HR = 0.5452, NDCG = 0.2996\n",
      "Hit Ratio: 0.5452\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.36966508626937866\n",
      "[Training Epoch 8] Batch 2500, Loss 0.355307400226593\n",
      "[Training Epoch 8] Batch 5000, Loss 0.33127063512802124\n",
      "[Training Epoch 8] Batch 7500, Loss 0.3798113465309143\n",
      "[Training Epoch 8] Batch 10000, Loss 0.3899063467979431\n",
      "[Training Epoch 8] Batch 12500, Loss 0.35679885745048523\n",
      "[Training Epoch 8] Batch 15000, Loss 0.34732311964035034\n",
      "[Evluating Epoch 8] HR = 0.5493, NDCG = 0.3040\n",
      "Hit Ratio: 0.5493\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.3250148296356201\n",
      "[Training Epoch 9] Batch 2500, Loss 0.38299477100372314\n",
      "[Training Epoch 9] Batch 5000, Loss 0.32190704345703125\n",
      "[Training Epoch 9] Batch 7500, Loss 0.37070122361183167\n",
      "[Training Epoch 9] Batch 10000, Loss 0.3227352499961853\n",
      "[Training Epoch 9] Batch 12500, Loss 0.3776032328605652\n",
      "[Training Epoch 9] Batch 15000, Loss 0.3885221481323242\n",
      "[Evluating Epoch 9] HR = 0.5495, NDCG = 0.3047\n",
      "Hit Ratio: 0.5495\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.33055728673934937\n",
      "[Training Epoch 10] Batch 2500, Loss 0.3125169277191162\n",
      "[Training Epoch 10] Batch 5000, Loss 0.34189459681510925\n",
      "[Training Epoch 10] Batch 7500, Loss 0.34414300322532654\n",
      "[Training Epoch 10] Batch 10000, Loss 0.37072673439979553\n",
      "[Training Epoch 10] Batch 12500, Loss 0.3147065043449402\n",
      "[Training Epoch 10] Batch 15000, Loss 0.3410350978374481\n",
      "[Evluating Epoch 10] HR = 0.5548, NDCG = 0.3091\n",
      "Hit Ratio: 0.5548\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.3048499822616577\n",
      "[Training Epoch 11] Batch 2500, Loss 0.3611816465854645\n",
      "[Training Epoch 11] Batch 5000, Loss 0.30309706926345825\n",
      "[Training Epoch 11] Batch 7500, Loss 0.27331870794296265\n",
      "[Training Epoch 11] Batch 10000, Loss 0.25166165828704834\n",
      "[Training Epoch 11] Batch 12500, Loss 0.3979436755180359\n",
      "[Training Epoch 11] Batch 15000, Loss 0.3897571861743927\n",
      "[Evluating Epoch 11] HR = 0.5568, NDCG = 0.3100\n",
      "Hit Ratio: 0.5568\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.3005187511444092\n",
      "[Training Epoch 12] Batch 2500, Loss 0.342120498418808\n",
      "[Training Epoch 12] Batch 5000, Loss 0.3534142076969147\n",
      "[Training Epoch 12] Batch 7500, Loss 0.3248671293258667\n",
      "[Training Epoch 12] Batch 10000, Loss 0.36011260747909546\n",
      "[Training Epoch 12] Batch 12500, Loss 0.3278646469116211\n",
      "[Training Epoch 12] Batch 15000, Loss 0.336039274930954\n",
      "[Evluating Epoch 12] HR = 0.5651, NDCG = 0.3140\n",
      "Hit Ratio: 0.5651\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.2917581796646118\n",
      "[Training Epoch 13] Batch 2500, Loss 0.3198079466819763\n",
      "[Training Epoch 13] Batch 5000, Loss 0.3795602023601532\n",
      "[Training Epoch 13] Batch 7500, Loss 0.345083624124527\n",
      "[Training Epoch 13] Batch 10000, Loss 0.37918901443481445\n",
      "[Training Epoch 13] Batch 12500, Loss 0.3833090364933014\n",
      "[Training Epoch 13] Batch 15000, Loss 0.3624632656574249\n",
      "[Evluating Epoch 13] HR = 0.5646, NDCG = 0.3150\n",
      "Hit Ratio: 0.5646\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.34384942054748535\n",
      "[Training Epoch 14] Batch 2500, Loss 0.340279757976532\n",
      "[Training Epoch 14] Batch 5000, Loss 0.2880229949951172\n",
      "[Training Epoch 14] Batch 7500, Loss 0.28309085965156555\n",
      "[Training Epoch 14] Batch 10000, Loss 0.2937908470630646\n",
      "[Training Epoch 14] Batch 12500, Loss 0.34974056482315063\n",
      "[Training Epoch 14] Batch 15000, Loss 0.36439406871795654\n",
      "[Evluating Epoch 14] HR = 0.5641, NDCG = 0.3145\n",
      "Hit Ratio: 0.5641\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.32152533531188965\n",
      "[Training Epoch 15] Batch 2500, Loss 0.3218954801559448\n",
      "[Training Epoch 15] Batch 5000, Loss 0.32699841260910034\n",
      "[Training Epoch 15] Batch 7500, Loss 0.2467249035835266\n",
      "[Training Epoch 15] Batch 10000, Loss 0.39233964681625366\n",
      "[Training Epoch 15] Batch 12500, Loss 0.321977436542511\n",
      "[Training Epoch 15] Batch 15000, Loss 0.3175613582134247\n",
      "[Evluating Epoch 15] HR = 0.5679, NDCG = 0.3170\n",
      "Hit Ratio: 0.5679\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.28822416067123413\n",
      "[Training Epoch 17] Batch 2500, Loss 0.2769889533519745\n",
      "[Training Epoch 17] Batch 5000, Loss 0.35400184988975525\n",
      "[Training Epoch 17] Batch 7500, Loss 0.31107792258262634\n",
      "[Training Epoch 17] Batch 10000, Loss 0.34105733036994934\n",
      "[Training Epoch 17] Batch 12500, Loss 0.3201166093349457\n",
      "[Training Epoch 17] Batch 15000, Loss 0.34894895553588867\n",
      "[Evluating Epoch 17] HR = 0.5722, NDCG = 0.3180\n",
      "Hit Ratio: 0.5722\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.3572048544883728\n",
      "[Training Epoch 18] Batch 2500, Loss 0.34105101227760315\n",
      "[Training Epoch 18] Batch 5000, Loss 0.37305060029029846\n",
      "[Training Epoch 18] Batch 7500, Loss 0.3247462511062622\n",
      "[Training Epoch 18] Batch 10000, Loss 0.27939024567604065\n",
      "[Training Epoch 18] Batch 12500, Loss 0.31103402376174927\n",
      "[Training Epoch 18] Batch 15000, Loss 0.3311436176300049\n",
      "[Evluating Epoch 18] HR = 0.5768, NDCG = 0.3210\n",
      "Hit Ratio: 0.5768\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.33274686336517334\n",
      "[Training Epoch 19] Batch 2500, Loss 0.3096991181373596\n",
      "[Training Epoch 19] Batch 5000, Loss 0.2939367890357971\n",
      "[Training Epoch 19] Batch 7500, Loss 0.32504552602767944\n",
      "[Training Epoch 19] Batch 10000, Loss 0.4006575047969818\n",
      "[Training Epoch 19] Batch 12500, Loss 0.32249096035957336\n",
      "[Training Epoch 19] Batch 15000, Loss 0.3000180125236511\n",
      "[Evluating Epoch 19] HR = 0.5760, NDCG = 0.3223\n",
      "Hit Ratio: 0.5760\n",
      "--------------------------------------------------------------------------------\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=16, bias=True)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=16, out_features=1, bias=True)\n",
      "NeuMF(\n",
      "  (embedding_user_mlp): Embedding(6040, 8)\n",
      "  (embedding_item_mlp): Embedding(3706, 8)\n",
      "  (embedding_user_mf): Embedding(6040, 8)\n",
      "  (embedding_item_mf): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.7408967614173889\n",
      "[Training Epoch 0] Batch 2500, Loss 0.36566483974456787\n",
      "[Evluating Epoch 0] HR = 0.5346, NDCG = 0.2990\n",
      "Hit Ratio: 0.5346\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.32586371898651123\n",
      "[Training Epoch 1] Batch 2500, Loss 0.3497524857521057\n",
      "[Evluating Epoch 1] HR = 0.5808, NDCG = 0.3287\n",
      "Hit Ratio: 0.5808\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.2846490442752838\n",
      "[Training Epoch 2] Batch 2500, Loss 0.28873372077941895\n",
      "[Evluating Epoch 2] HR = 0.6116, NDCG = 0.3473\n",
      "Hit Ratio: 0.6116\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.3023300766944885\n",
      "[Training Epoch 3] Batch 2500, Loss 0.3157079219818115\n",
      "[Evluating Epoch 3] HR = 0.6293, NDCG = 0.3595\n",
      "Hit Ratio: 0.6293\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.3073747754096985\n",
      "[Training Epoch 4] Batch 2500, Loss 0.2778603732585907\n",
      "[Evluating Epoch 4] HR = 0.6338, NDCG = 0.3659\n",
      "Hit Ratio: 0.6338\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.2930358648300171\n",
      "[Training Epoch 5] Batch 2500, Loss 0.27851495146751404\n",
      "[Evluating Epoch 5] HR = 0.6434, NDCG = 0.3689\n",
      "Hit Ratio: 0.6434\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.29120633006095886\n",
      "[Training Epoch 6] Batch 2500, Loss 0.3076900243759155\n",
      "[Evluating Epoch 6] HR = 0.6455, NDCG = 0.3729\n",
      "Hit Ratio: 0.6455\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.3094909191131592\n",
      "[Training Epoch 7] Batch 2500, Loss 0.2690499722957611\n",
      "[Evluating Epoch 7] HR = 0.6483, NDCG = 0.3753\n",
      "Hit Ratio: 0.6483\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.2678537666797638\n",
      "[Training Epoch 8] Batch 2500, Loss 0.2991434931755066\n",
      "[Evluating Epoch 8] HR = 0.6478, NDCG = 0.3754\n",
      "Hit Ratio: 0.6478\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.28161272406578064\n",
      "[Training Epoch 9] Batch 2500, Loss 0.27545779943466187\n",
      "[Evluating Epoch 9] HR = 0.6518, NDCG = 0.3764\n",
      "Hit Ratio: 0.6518\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.3106725811958313\n",
      "[Training Epoch 10] Batch 2500, Loss 0.2734300494194031\n",
      "[Evluating Epoch 10] HR = 0.6588, NDCG = 0.3821\n",
      "Hit Ratio: 0.6588\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.2597140371799469\n",
      "[Training Epoch 11] Batch 2500, Loss 0.2716921269893646\n",
      "[Evluating Epoch 11] HR = 0.6599, NDCG = 0.3835\n",
      "Hit Ratio: 0.6599\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.2646397352218628\n",
      "[Training Epoch 12] Batch 2500, Loss 0.28972750902175903\n",
      "[Evluating Epoch 12] HR = 0.6649, NDCG = 0.3854\n",
      "Hit Ratio: 0.6649\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.2742762565612793\n",
      "[Training Epoch 13] Batch 2500, Loss 0.2824316918849945\n",
      "[Evluating Epoch 13] HR = 0.6584, NDCG = 0.3831\n",
      "Hit Ratio: 0.6584\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.27905839681625366\n",
      "[Training Epoch 14] Batch 2500, Loss 0.2731776237487793\n",
      "[Evluating Epoch 14] HR = 0.6644, NDCG = 0.3884\n",
      "Hit Ratio: 0.6644\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.27411890029907227\n",
      "[Training Epoch 15] Batch 2500, Loss 0.28755199909210205\n",
      "[Evluating Epoch 15] HR = 0.6659, NDCG = 0.3898\n",
      "Hit Ratio: 0.6659\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.26334869861602783\n",
      "[Training Epoch 16] Batch 2500, Loss 0.2693402171134949\n",
      "[Evluating Epoch 16] HR = 0.6705, NDCG = 0.3927\n",
      "Hit Ratio: 0.6705\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.27005481719970703\n",
      "[Training Epoch 17] Batch 2500, Loss 0.2820085287094116\n",
      "[Evluating Epoch 17] HR = 0.6646, NDCG = 0.3911\n",
      "Hit Ratio: 0.6646\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.2817956507205963\n",
      "[Training Epoch 18] Batch 2500, Loss 0.269570529460907\n",
      "[Evluating Epoch 18] HR = 0.6715, NDCG = 0.3947\n",
      "Hit Ratio: 0.6715\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.2631147503852844\n",
      "[Training Epoch 19] Batch 2500, Loss 0.28774493932724\n",
      "[Evluating Epoch 19] HR = 0.6719, NDCG = 0.3957\n",
      "Hit Ratio: 0.6719\n",
      "--------------------------------------------------------------------------------\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "number of negatives: 4\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "number of negatives: 5\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=8, out_features=1, bias=True)\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.740950882434845\n",
      "[Training Epoch 0] Batch 2500, Loss 0.3322336673736572\n",
      "[Training Epoch 0] Batch 5000, Loss 0.3079538345336914\n",
      "[Evluating Epoch 0] HR = 0.4472, NDCG = 0.2494\n",
      "Hit Ratio: 0.4472\n",
      "NDCG: 0.2494\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.29863038659095764\n",
      "[Training Epoch 1] Batch 2500, Loss 0.3146081566810608\n",
      "[Training Epoch 1] Batch 5000, Loss 0.3150981664657593\n",
      "[Evluating Epoch 1] HR = 0.4525, NDCG = 0.2512\n",
      "Hit Ratio: 0.4525\n",
      "NDCG: 0.2512\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.31887781620025635\n",
      "[Training Epoch 2] Batch 2500, Loss 0.32266557216644287\n",
      "[Training Epoch 2] Batch 5000, Loss 0.26764026284217834\n",
      "[Evluating Epoch 2] HR = 0.5040, NDCG = 0.2823\n",
      "Hit Ratio: 0.5040\n",
      "NDCG: 0.2823\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.29848331212997437\n",
      "[Training Epoch 3] Batch 2500, Loss 0.2757661044597626\n",
      "[Training Epoch 3] Batch 5000, Loss 0.26323989033699036\n",
      "[Evluating Epoch 3] HR = 0.5601, NDCG = 0.3140\n",
      "Hit Ratio: 0.5601\n",
      "NDCG: 0.3140\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.25542500615119934\n",
      "[Training Epoch 4] Batch 2500, Loss 0.25115567445755005\n",
      "[Training Epoch 4] Batch 5000, Loss 0.28336334228515625\n",
      "[Evluating Epoch 4] HR = 0.5988, NDCG = 0.3399\n",
      "Hit Ratio: 0.5988\n",
      "NDCG: 0.3399\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.27163684368133545\n",
      "[Training Epoch 5] Batch 2500, Loss 0.23559534549713135\n",
      "[Training Epoch 5] Batch 5000, Loss 0.24483296275138855\n",
      "[Evluating Epoch 5] HR = 0.6093, NDCG = 0.3473\n",
      "Hit Ratio: 0.6093\n",
      "NDCG: 0.3473\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.2519739866256714\n",
      "[Training Epoch 6] Batch 2500, Loss 0.28583136200904846\n",
      "[Training Epoch 6] Batch 5000, Loss 0.23993121087551117\n",
      "[Evluating Epoch 6] HR = 0.6175, NDCG = 0.3525\n",
      "Hit Ratio: 0.6175\n",
      "NDCG: 0.3525\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.2359875738620758\n",
      "[Training Epoch 7] Batch 2500, Loss 0.2695442736148834\n",
      "[Training Epoch 7] Batch 5000, Loss 0.2712652385234833\n",
      "[Evluating Epoch 7] HR = 0.6268, NDCG = 0.3582\n",
      "Hit Ratio: 0.6268\n",
      "NDCG: 0.3582\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.2657366394996643\n",
      "[Training Epoch 8] Batch 2500, Loss 0.23079878091812134\n",
      "[Training Epoch 8] Batch 5000, Loss 0.24416859447956085\n",
      "[Evluating Epoch 8] HR = 0.6238, NDCG = 0.3603\n",
      "Hit Ratio: 0.6238\n",
      "NDCG: 0.3603\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.2234249860048294\n",
      "[Training Epoch 9] Batch 2500, Loss 0.24354979395866394\n",
      "[Training Epoch 9] Batch 5000, Loss 0.2358265221118927\n",
      "[Evluating Epoch 9] HR = 0.6291, NDCG = 0.3605\n",
      "Hit Ratio: 0.6291\n",
      "NDCG: 0.3605\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.2528552711009979\n",
      "[Training Epoch 10] Batch 2500, Loss 0.2802776098251343\n",
      "[Training Epoch 10] Batch 5000, Loss 0.2651370167732239\n",
      "[Evluating Epoch 10] HR = 0.6323, NDCG = 0.3638\n",
      "Hit Ratio: 0.6323\n",
      "NDCG: 0.3638\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.2656932771205902\n",
      "[Training Epoch 11] Batch 2500, Loss 0.23829743266105652\n",
      "[Training Epoch 11] Batch 5000, Loss 0.26245051622390747\n",
      "[Evluating Epoch 11] HR = 0.6336, NDCG = 0.3624\n",
      "Hit Ratio: 0.6336\n",
      "NDCG: 0.3624\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.2556222081184387\n",
      "[Training Epoch 12] Batch 2500, Loss 0.24163223803043365\n",
      "[Training Epoch 12] Batch 5000, Loss 0.21397832036018372\n",
      "[Evluating Epoch 12] HR = 0.6346, NDCG = 0.3641\n",
      "Hit Ratio: 0.6346\n",
      "NDCG: 0.3641\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.2256307601928711\n",
      "[Training Epoch 13] Batch 2500, Loss 0.2530355155467987\n",
      "[Training Epoch 13] Batch 5000, Loss 0.24354735016822815\n",
      "[Evluating Epoch 13] HR = 0.6344, NDCG = 0.3640\n",
      "Hit Ratio: 0.6344\n",
      "NDCG: 0.3640\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.2479638159275055\n",
      "[Training Epoch 14] Batch 2500, Loss 0.2230653464794159\n",
      "[Training Epoch 14] Batch 5000, Loss 0.25504830479621887\n",
      "[Evluating Epoch 14] HR = 0.6382, NDCG = 0.3649\n",
      "Hit Ratio: 0.6382\n",
      "NDCG: 0.3649\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.2564312815666199\n",
      "[Training Epoch 15] Batch 2500, Loss 0.2283119112253189\n",
      "[Training Epoch 15] Batch 5000, Loss 0.23305189609527588\n",
      "[Evluating Epoch 15] HR = 0.6341, NDCG = 0.3671\n",
      "Hit Ratio: 0.6341\n",
      "NDCG: 0.3671\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.24383355677127838\n",
      "[Training Epoch 16] Batch 2500, Loss 0.2388465404510498\n",
      "[Training Epoch 16] Batch 5000, Loss 0.2473207712173462\n",
      "[Evluating Epoch 16] HR = 0.6382, NDCG = 0.3693\n",
      "Hit Ratio: 0.6382\n",
      "NDCG: 0.3693\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.26303631067276\n",
      "[Training Epoch 17] Batch 2500, Loss 0.24599012732505798\n",
      "[Training Epoch 17] Batch 5000, Loss 0.2563101351261139\n",
      "[Evluating Epoch 17] HR = 0.6369, NDCG = 0.3672\n",
      "Hit Ratio: 0.6369\n",
      "NDCG: 0.3672\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.2338084876537323\n",
      "[Training Epoch 18] Batch 2500, Loss 0.251676082611084\n",
      "[Training Epoch 18] Batch 5000, Loss 0.24320140480995178\n",
      "[Evluating Epoch 18] HR = 0.6368, NDCG = 0.3679\n",
      "Hit Ratio: 0.6368\n",
      "NDCG: 0.3679\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.22039127349853516\n",
      "[Training Epoch 19] Batch 2500, Loss 0.23918861150741577\n",
      "[Training Epoch 19] Batch 5000, Loss 0.244145929813385\n",
      "[Evluating Epoch 19] HR = 0.6396, NDCG = 0.3709\n",
      "Hit Ratio: 0.6396\n",
      "NDCG: 0.3709\n",
      "--------------------------------------------------------------------------------\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=8, out_features=1, bias=True)\n",
      "MLP(\n",
      "  (embedding_user): Embedding(6040, 8)\n",
      "  (embedding_item): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.6000545620918274\n",
      "[Training Epoch 0] Batch 2500, Loss 0.3106675148010254\n",
      "[Training Epoch 0] Batch 5000, Loss 0.28651392459869385\n",
      "[Training Epoch 0] Batch 7500, Loss 0.32405197620391846\n",
      "[Training Epoch 0] Batch 10000, Loss 0.40572476387023926\n",
      "[Training Epoch 0] Batch 12500, Loss 0.26314443349838257\n",
      "[Training Epoch 0] Batch 15000, Loss 0.4180919826030731\n",
      "[Training Epoch 0] Batch 17500, Loss 0.33741194009780884\n",
      "[Training Epoch 0] Batch 20000, Loss 0.314550518989563\n",
      "[Training Epoch 0] Batch 22500, Loss 0.3045836091041565\n",
      "[Evluating Epoch 0] HR = 0.4477, NDCG = 0.2482\n",
      "Hit Ratio: 0.4477\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.3696003258228302\n",
      "[Training Epoch 1] Batch 2500, Loss 0.34465736150741577\n",
      "[Training Epoch 1] Batch 5000, Loss 0.29767176508903503\n",
      "[Training Epoch 1] Batch 7500, Loss 0.3301153779029846\n",
      "[Training Epoch 1] Batch 10000, Loss 0.3185625672340393\n",
      "[Training Epoch 1] Batch 12500, Loss 0.3067721724510193\n",
      "[Training Epoch 1] Batch 15000, Loss 0.28156739473342896\n",
      "[Training Epoch 1] Batch 17500, Loss 0.3232749104499817\n",
      "[Training Epoch 1] Batch 20000, Loss 0.3250029683113098\n",
      "[Training Epoch 1] Batch 22500, Loss 0.2691381573677063\n",
      "[Evluating Epoch 1] HR = 0.4447, NDCG = 0.2480\n",
      "Hit Ratio: 0.4447\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.3380340337753296\n",
      "[Training Epoch 2] Batch 2500, Loss 0.2812036871910095\n",
      "[Training Epoch 2] Batch 5000, Loss 0.3556751608848572\n",
      "[Training Epoch 2] Batch 7500, Loss 0.31904441118240356\n",
      "[Training Epoch 2] Batch 10000, Loss 0.3308611512184143\n",
      "[Training Epoch 2] Batch 12500, Loss 0.3348224461078644\n",
      "[Training Epoch 2] Batch 15000, Loss 0.3027934432029724\n",
      "[Training Epoch 2] Batch 17500, Loss 0.3298690617084503\n",
      "[Training Epoch 2] Batch 20000, Loss 0.319151371717453\n",
      "[Training Epoch 2] Batch 22500, Loss 0.3313709497451782\n",
      "[Evluating Epoch 2] HR = 0.4810, NDCG = 0.2658\n",
      "Hit Ratio: 0.4810\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.3032721281051636\n",
      "[Training Epoch 3] Batch 2500, Loss 0.29985153675079346\n",
      "[Training Epoch 3] Batch 5000, Loss 0.30255287885665894\n",
      "[Training Epoch 3] Batch 7500, Loss 0.31560003757476807\n",
      "[Training Epoch 3] Batch 10000, Loss 0.3190544843673706\n",
      "[Training Epoch 3] Batch 12500, Loss 0.32307934761047363\n",
      "[Training Epoch 3] Batch 15000, Loss 0.31877249479293823\n",
      "[Training Epoch 3] Batch 17500, Loss 0.28290197253227234\n",
      "[Training Epoch 3] Batch 20000, Loss 0.26285138726234436\n",
      "[Training Epoch 3] Batch 22500, Loss 0.27766185998916626\n",
      "[Evluating Epoch 3] HR = 0.5152, NDCG = 0.2867\n",
      "Hit Ratio: 0.5152\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.306243360042572\n",
      "[Training Epoch 4] Batch 2500, Loss 0.2940807342529297\n",
      "[Training Epoch 4] Batch 5000, Loss 0.2869921326637268\n",
      "[Training Epoch 4] Batch 7500, Loss 0.2915852963924408\n",
      "[Training Epoch 4] Batch 10000, Loss 0.3225751519203186\n",
      "[Training Epoch 4] Batch 12500, Loss 0.2833191156387329\n",
      "[Training Epoch 4] Batch 15000, Loss 0.3234926462173462\n",
      "[Training Epoch 4] Batch 17500, Loss 0.35813742876052856\n",
      "[Training Epoch 4] Batch 20000, Loss 0.2969214916229248\n",
      "[Training Epoch 4] Batch 22500, Loss 0.2592243254184723\n",
      "[Evluating Epoch 4] HR = 0.5343, NDCG = 0.2992\n",
      "Hit Ratio: 0.5343\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.2996940314769745\n",
      "[Training Epoch 5] Batch 2500, Loss 0.2711107134819031\n",
      "[Training Epoch 5] Batch 5000, Loss 0.27516621351242065\n",
      "[Training Epoch 5] Batch 7500, Loss 0.31053391098976135\n",
      "[Training Epoch 5] Batch 10000, Loss 0.3251808285713196\n",
      "[Training Epoch 5] Batch 12500, Loss 0.3411005139350891\n",
      "[Training Epoch 5] Batch 15000, Loss 0.2524302005767822\n",
      "[Training Epoch 5] Batch 17500, Loss 0.37507110834121704\n",
      "[Training Epoch 5] Batch 20000, Loss 0.2848583161830902\n",
      "[Training Epoch 5] Batch 22500, Loss 0.26685959100723267\n",
      "[Evluating Epoch 5] HR = 0.5397, NDCG = 0.3017\n",
      "Hit Ratio: 0.5397\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.3302954435348511\n",
      "[Training Epoch 6] Batch 2500, Loss 0.3415118157863617\n",
      "[Training Epoch 6] Batch 5000, Loss 0.23181641101837158\n",
      "[Training Epoch 6] Batch 7500, Loss 0.28182148933410645\n",
      "[Training Epoch 6] Batch 10000, Loss 0.34784650802612305\n",
      "[Training Epoch 6] Batch 12500, Loss 0.2792501449584961\n",
      "[Training Epoch 6] Batch 15000, Loss 0.2621886134147644\n",
      "[Training Epoch 6] Batch 17500, Loss 0.3175050914287567\n",
      "[Training Epoch 6] Batch 20000, Loss 0.2713571786880493\n",
      "[Training Epoch 6] Batch 22500, Loss 0.26643499732017517\n",
      "[Evluating Epoch 6] HR = 0.5488, NDCG = 0.3077\n",
      "Hit Ratio: 0.5488\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.284748375415802\n",
      "[Training Epoch 7] Batch 2500, Loss 0.2322167456150055\n",
      "[Training Epoch 7] Batch 5000, Loss 0.24477382004261017\n",
      "[Training Epoch 7] Batch 7500, Loss 0.31587129831314087\n",
      "[Training Epoch 7] Batch 10000, Loss 0.22428765892982483\n",
      "[Training Epoch 7] Batch 12500, Loss 0.24629171192646027\n",
      "[Training Epoch 7] Batch 15000, Loss 0.29363200068473816\n",
      "[Training Epoch 7] Batch 17500, Loss 0.28852787613868713\n",
      "[Training Epoch 7] Batch 20000, Loss 0.28481268882751465\n",
      "[Training Epoch 7] Batch 22500, Loss 0.33338671922683716\n",
      "[Evluating Epoch 7] HR = 0.5551, NDCG = 0.3104\n",
      "Hit Ratio: 0.5551\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.24998953938484192\n",
      "[Training Epoch 8] Batch 2500, Loss 0.2560102045536041\n",
      "[Training Epoch 8] Batch 5000, Loss 0.22304558753967285\n",
      "[Training Epoch 8] Batch 7500, Loss 0.2706405520439148\n",
      "[Training Epoch 8] Batch 10000, Loss 0.32275456190109253\n",
      "[Training Epoch 8] Batch 12500, Loss 0.28268635272979736\n",
      "[Training Epoch 8] Batch 15000, Loss 0.2257707118988037\n",
      "[Training Epoch 8] Batch 17500, Loss 0.23199555277824402\n",
      "[Training Epoch 8] Batch 20000, Loss 0.23048382997512817\n",
      "[Training Epoch 8] Batch 22500, Loss 0.2318282425403595\n",
      "[Evluating Epoch 8] HR = 0.5613, NDCG = 0.3146\n",
      "Hit Ratio: 0.5613\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.25833532214164734\n",
      "[Training Epoch 9] Batch 2500, Loss 0.2429538518190384\n",
      "[Training Epoch 9] Batch 5000, Loss 0.22595950961112976\n",
      "[Training Epoch 9] Batch 7500, Loss 0.2631503939628601\n",
      "[Training Epoch 9] Batch 10000, Loss 0.2479860484600067\n",
      "[Training Epoch 9] Batch 12500, Loss 0.2375241219997406\n",
      "[Training Epoch 9] Batch 15000, Loss 0.3155880272388458\n",
      "[Training Epoch 9] Batch 17500, Loss 0.22290906310081482\n",
      "[Training Epoch 9] Batch 20000, Loss 0.31584733724594116\n",
      "[Training Epoch 9] Batch 22500, Loss 0.28463053703308105\n",
      "[Evluating Epoch 9] HR = 0.5674, NDCG = 0.3197\n",
      "Hit Ratio: 0.5674\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.29676735401153564\n",
      "[Training Epoch 10] Batch 2500, Loss 0.26957935094833374\n",
      "[Training Epoch 10] Batch 5000, Loss 0.2279694676399231\n",
      "[Training Epoch 10] Batch 7500, Loss 0.32283443212509155\n",
      "[Training Epoch 10] Batch 10000, Loss 0.2419288456439972\n",
      "[Training Epoch 10] Batch 12500, Loss 0.24964946508407593\n",
      "[Training Epoch 10] Batch 15000, Loss 0.2636150121688843\n",
      "[Training Epoch 10] Batch 17500, Loss 0.2985345125198364\n",
      "[Training Epoch 10] Batch 20000, Loss 0.23598922789096832\n",
      "[Training Epoch 10] Batch 22500, Loss 0.359247088432312\n",
      "[Evluating Epoch 10] HR = 0.5710, NDCG = 0.3217\n",
      "Hit Ratio: 0.5710\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.2732662260532379\n",
      "[Training Epoch 11] Batch 2500, Loss 0.21501871943473816\n",
      "[Training Epoch 11] Batch 5000, Loss 0.29588645696640015\n",
      "[Training Epoch 11] Batch 7500, Loss 0.2842499017715454\n",
      "[Training Epoch 11] Batch 10000, Loss 0.3013877868652344\n",
      "[Training Epoch 11] Batch 12500, Loss 0.2627132833003998\n",
      "[Training Epoch 11] Batch 15000, Loss 0.2976313829421997\n",
      "[Training Epoch 11] Batch 17500, Loss 0.20333322882652283\n",
      "[Training Epoch 11] Batch 20000, Loss 0.2950732707977295\n",
      "[Training Epoch 11] Batch 22500, Loss 0.24541988968849182\n",
      "[Evluating Epoch 11] HR = 0.5755, NDCG = 0.3256\n",
      "Hit Ratio: 0.5755\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.32840412855148315\n",
      "[Training Epoch 12] Batch 2500, Loss 0.3032258152961731\n",
      "[Training Epoch 12] Batch 5000, Loss 0.2700086832046509\n",
      "[Training Epoch 12] Batch 7500, Loss 0.21248337626457214\n",
      "[Training Epoch 12] Batch 10000, Loss 0.27007514238357544\n",
      "[Training Epoch 12] Batch 12500, Loss 0.2730652093887329\n",
      "[Training Epoch 12] Batch 15000, Loss 0.2601456642150879\n",
      "[Training Epoch 12] Batch 17500, Loss 0.27293214201927185\n",
      "[Training Epoch 12] Batch 20000, Loss 0.2808360457420349\n",
      "[Training Epoch 12] Batch 22500, Loss 0.26664572954177856\n",
      "[Evluating Epoch 12] HR = 0.5737, NDCG = 0.3254\n",
      "Hit Ratio: 0.5737\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.2174929678440094\n",
      "[Training Epoch 13] Batch 2500, Loss 0.2972642779350281\n",
      "[Training Epoch 13] Batch 5000, Loss 0.2332446575164795\n",
      "[Training Epoch 13] Batch 7500, Loss 0.30693089962005615\n",
      "[Training Epoch 13] Batch 10000, Loss 0.2419736087322235\n",
      "[Training Epoch 13] Batch 12500, Loss 0.28283998370170593\n",
      "[Training Epoch 13] Batch 15000, Loss 0.30697113275527954\n",
      "[Training Epoch 13] Batch 17500, Loss 0.23726041615009308\n",
      "[Training Epoch 13] Batch 20000, Loss 0.28302812576293945\n",
      "[Training Epoch 13] Batch 22500, Loss 0.270408034324646\n",
      "[Evluating Epoch 13] HR = 0.5803, NDCG = 0.3285\n",
      "Hit Ratio: 0.5803\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.26504576206207275\n",
      "[Training Epoch 14] Batch 2500, Loss 0.2795100212097168\n",
      "[Training Epoch 14] Batch 5000, Loss 0.281637579202652\n",
      "[Training Epoch 14] Batch 7500, Loss 0.304581880569458\n",
      "[Training Epoch 14] Batch 10000, Loss 0.24298053979873657\n",
      "[Training Epoch 14] Batch 12500, Loss 0.3207886219024658\n",
      "[Training Epoch 14] Batch 15000, Loss 0.1875925064086914\n",
      "[Training Epoch 14] Batch 17500, Loss 0.3144185543060303\n",
      "[Training Epoch 14] Batch 20000, Loss 0.24202634394168854\n",
      "[Training Epoch 14] Batch 22500, Loss 0.23048058152198792\n",
      "[Evluating Epoch 14] HR = 0.5790, NDCG = 0.3284\n",
      "Hit Ratio: 0.5790\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.2360381782054901\n",
      "[Training Epoch 15] Batch 2500, Loss 0.2646935284137726\n",
      "[Training Epoch 15] Batch 5000, Loss 0.21487446129322052\n",
      "[Training Epoch 15] Batch 7500, Loss 0.25306642055511475\n",
      "[Training Epoch 15] Batch 10000, Loss 0.2608645558357239\n",
      "[Training Epoch 15] Batch 12500, Loss 0.22381827235221863\n",
      "[Training Epoch 15] Batch 15000, Loss 0.2530563473701477\n",
      "[Training Epoch 15] Batch 17500, Loss 0.22158154845237732\n",
      "[Training Epoch 15] Batch 20000, Loss 0.32555362582206726\n",
      "[Training Epoch 15] Batch 22500, Loss 0.22777876257896423\n",
      "[Evluating Epoch 15] HR = 0.5758, NDCG = 0.3286\n",
      "Hit Ratio: 0.5758\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.32811182737350464\n",
      "[Training Epoch 16] Batch 2500, Loss 0.2929733097553253\n",
      "[Training Epoch 16] Batch 5000, Loss 0.31722885370254517\n",
      "[Training Epoch 16] Batch 7500, Loss 0.2600933015346527\n",
      "[Training Epoch 16] Batch 10000, Loss 0.2682740092277527\n",
      "[Training Epoch 16] Batch 12500, Loss 0.2625322639942169\n",
      "[Training Epoch 16] Batch 15000, Loss 0.26192453503608704\n",
      "[Training Epoch 16] Batch 17500, Loss 0.26388150453567505\n",
      "[Training Epoch 16] Batch 20000, Loss 0.31466108560562134\n",
      "[Training Epoch 16] Batch 22500, Loss 0.26345741748809814\n",
      "[Evluating Epoch 16] HR = 0.5843, NDCG = 0.3328\n",
      "Hit Ratio: 0.5843\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.2718583941459656\n",
      "[Training Epoch 17] Batch 2500, Loss 0.23176291584968567\n",
      "[Training Epoch 17] Batch 5000, Loss 0.23824012279510498\n",
      "[Training Epoch 17] Batch 7500, Loss 0.3557775616645813\n",
      "[Training Epoch 17] Batch 10000, Loss 0.2993081510066986\n",
      "[Training Epoch 17] Batch 12500, Loss 0.2965582013130188\n",
      "[Training Epoch 17] Batch 15000, Loss 0.25952768325805664\n",
      "[Training Epoch 17] Batch 17500, Loss 0.2739924490451813\n",
      "[Training Epoch 17] Batch 20000, Loss 0.2433045208454132\n",
      "[Training Epoch 17] Batch 22500, Loss 0.26542776823043823\n",
      "[Evluating Epoch 17] HR = 0.5839, NDCG = 0.3336\n",
      "Hit Ratio: 0.5839\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.22010013461112976\n",
      "[Training Epoch 18] Batch 2500, Loss 0.2831455171108246\n",
      "[Training Epoch 18] Batch 5000, Loss 0.2386421263217926\n",
      "[Training Epoch 18] Batch 7500, Loss 0.32063788175582886\n",
      "[Training Epoch 18] Batch 10000, Loss 0.27063703536987305\n",
      "[Training Epoch 18] Batch 12500, Loss 0.25153225660324097\n",
      "[Training Epoch 18] Batch 15000, Loss 0.27605438232421875\n",
      "[Training Epoch 18] Batch 17500, Loss 0.24764519929885864\n",
      "[Training Epoch 18] Batch 20000, Loss 0.31753039360046387\n",
      "[Training Epoch 18] Batch 22500, Loss 0.2833626866340637\n",
      "[Evluating Epoch 18] HR = 0.5901, NDCG = 0.3375\n",
      "Hit Ratio: 0.5901\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.22797136008739471\n",
      "[Training Epoch 19] Batch 2500, Loss 0.2430298775434494\n",
      "[Training Epoch 19] Batch 5000, Loss 0.2120848000049591\n",
      "[Training Epoch 19] Batch 7500, Loss 0.3044608235359192\n",
      "[Training Epoch 19] Batch 10000, Loss 0.2681935727596283\n",
      "[Training Epoch 19] Batch 12500, Loss 0.2830565571784973\n",
      "[Training Epoch 19] Batch 15000, Loss 0.22798091173171997\n",
      "[Training Epoch 19] Batch 17500, Loss 0.2739383280277252\n",
      "[Training Epoch 19] Batch 20000, Loss 0.2799532413482666\n",
      "[Training Epoch 19] Batch 22500, Loss 0.2927061915397644\n",
      "[Evluating Epoch 19] HR = 0.5889, NDCG = 0.3392\n",
      "Hit Ratio: 0.5889\n",
      "--------------------------------------------------------------------------------\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=16, bias=True)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=16, out_features=1, bias=True)\n",
      "NeuMF(\n",
      "  (embedding_user_mlp): Embedding(6040, 8)\n",
      "  (embedding_item_mlp): Embedding(3706, 8)\n",
      "  (embedding_user_mf): Embedding(6040, 8)\n",
      "  (embedding_item_mf): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.7029411792755127\n",
      "[Training Epoch 0] Batch 2500, Loss 0.31349700689315796\n",
      "[Training Epoch 0] Batch 5000, Loss 0.25582846999168396\n",
      "[Evluating Epoch 0] HR = 0.5497, NDCG = 0.3096\n",
      "Hit Ratio: 0.5497\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.2800906002521515\n",
      "[Training Epoch 1] Batch 2500, Loss 0.2346670776605606\n",
      "[Training Epoch 1] Batch 5000, Loss 0.2705264389514923\n",
      "[Evluating Epoch 1] HR = 0.6108, NDCG = 0.3473\n",
      "Hit Ratio: 0.6108\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.25152587890625\n",
      "[Training Epoch 2] Batch 2500, Loss 0.23698687553405762\n",
      "[Training Epoch 2] Batch 5000, Loss 0.25941503047943115\n",
      "[Evluating Epoch 2] HR = 0.6258, NDCG = 0.3615\n",
      "Hit Ratio: 0.6258\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.2148466259241104\n",
      "[Training Epoch 3] Batch 2500, Loss 0.2512472867965698\n",
      "[Training Epoch 3] Batch 5000, Loss 0.2421029806137085\n",
      "[Evluating Epoch 3] HR = 0.6391, NDCG = 0.3720\n",
      "Hit Ratio: 0.6391\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.24207308888435364\n",
      "[Training Epoch 4] Batch 2500, Loss 0.23387792706489563\n",
      "[Training Epoch 4] Batch 5000, Loss 0.21793097257614136\n",
      "[Evluating Epoch 4] HR = 0.6480, NDCG = 0.3756\n",
      "Hit Ratio: 0.6480\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.24227657914161682\n",
      "[Training Epoch 5] Batch 2500, Loss 0.2242438793182373\n",
      "[Training Epoch 5] Batch 5000, Loss 0.2532337009906769\n",
      "[Evluating Epoch 5] HR = 0.6550, NDCG = 0.3793\n",
      "Hit Ratio: 0.6550\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.21997368335723877\n",
      "[Training Epoch 6] Batch 2500, Loss 0.24676065146923065\n",
      "[Training Epoch 6] Batch 5000, Loss 0.24271121621131897\n",
      "[Evluating Epoch 6] HR = 0.6550, NDCG = 0.3822\n",
      "Hit Ratio: 0.6550\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.2171502411365509\n",
      "[Training Epoch 7] Batch 2500, Loss 0.23995929956436157\n",
      "[Training Epoch 7] Batch 5000, Loss 0.27719438076019287\n",
      "[Evluating Epoch 7] HR = 0.6631, NDCG = 0.3878\n",
      "Hit Ratio: 0.6631\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.22021807730197906\n",
      "[Training Epoch 8] Batch 2500, Loss 0.2468899041414261\n",
      "[Training Epoch 8] Batch 5000, Loss 0.21281200647354126\n",
      "[Evluating Epoch 8] HR = 0.6656, NDCG = 0.3892\n",
      "Hit Ratio: 0.6656\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.23922717571258545\n",
      "[Training Epoch 9] Batch 2500, Loss 0.23459303379058838\n",
      "[Training Epoch 9] Batch 5000, Loss 0.26539501547813416\n",
      "[Evluating Epoch 9] HR = 0.6662, NDCG = 0.3914\n",
      "Hit Ratio: 0.6662\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.249930277466774\n",
      "[Training Epoch 10] Batch 2500, Loss 0.24259671568870544\n",
      "[Training Epoch 10] Batch 5000, Loss 0.2524811029434204\n",
      "[Evluating Epoch 10] HR = 0.6728, NDCG = 0.3955\n",
      "Hit Ratio: 0.6728\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.2270703911781311\n",
      "[Training Epoch 11] Batch 2500, Loss 0.22543908655643463\n",
      "[Training Epoch 11] Batch 5000, Loss 0.24669857323169708\n",
      "[Evluating Epoch 11] HR = 0.6699, NDCG = 0.3955\n",
      "Hit Ratio: 0.6699\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.23807916045188904\n",
      "[Training Epoch 12] Batch 2500, Loss 0.21676647663116455\n",
      "[Training Epoch 12] Batch 5000, Loss 0.24928641319274902\n",
      "[Evluating Epoch 12] HR = 0.6738, NDCG = 0.4005\n",
      "Hit Ratio: 0.6738\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.2375820130109787\n",
      "[Training Epoch 13] Batch 2500, Loss 0.2448132187128067\n",
      "[Training Epoch 13] Batch 5000, Loss 0.22188903391361237\n",
      "[Evluating Epoch 13] HR = 0.6758, NDCG = 0.4021\n",
      "Hit Ratio: 0.6758\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.2243039608001709\n",
      "[Training Epoch 14] Batch 2500, Loss 0.2128629833459854\n",
      "[Training Epoch 14] Batch 5000, Loss 0.20437932014465332\n",
      "[Evluating Epoch 14] HR = 0.6733, NDCG = 0.3997\n",
      "Hit Ratio: 0.6733\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.21573160588741302\n",
      "[Training Epoch 15] Batch 2500, Loss 0.224917471408844\n",
      "[Training Epoch 15] Batch 5000, Loss 0.22731676697731018\n",
      "[Evluating Epoch 15] HR = 0.6795, NDCG = 0.4052\n",
      "Hit Ratio: 0.6795\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.2320098876953125\n",
      "[Training Epoch 16] Batch 2500, Loss 0.23274856805801392\n",
      "[Training Epoch 16] Batch 5000, Loss 0.20355823636054993\n",
      "[Evluating Epoch 16] HR = 0.6762, NDCG = 0.4039\n",
      "Hit Ratio: 0.6762\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.24621102213859558\n",
      "[Training Epoch 17] Batch 2500, Loss 0.21291637420654297\n",
      "[Training Epoch 17] Batch 5000, Loss 0.2316979467868805\n",
      "[Evluating Epoch 17] HR = 0.6815, NDCG = 0.4074\n",
      "Hit Ratio: 0.6815\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.21890924870967865\n",
      "[Training Epoch 18] Batch 2500, Loss 0.23993031680583954\n",
      "[Training Epoch 18] Batch 5000, Loss 0.23298071324825287\n",
      "[Evluating Epoch 18] HR = 0.6783, NDCG = 0.4031\n",
      "Hit Ratio: 0.6783\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.2056736797094345\n",
      "[Training Epoch 19] Batch 2500, Loss 0.23774899542331696\n",
      "[Training Epoch 19] Batch 5000, Loss 0.23369599878787994\n",
      "[Evluating Epoch 19] HR = 0.6801, NDCG = 0.4049\n",
      "Hit Ratio: 0.6801\n",
      "--------------------------------------------------------------------------------\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "number of negatives: 6\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=8, out_features=1, bias=True)\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.5972111225128174\n",
      "[Training Epoch 0] Batch 2500, Loss 0.294474333524704\n",
      "[Training Epoch 0] Batch 5000, Loss 0.2857266664505005\n",
      "[Evluating Epoch 0] HR = 0.4685, NDCG = 0.2606\n",
      "Hit Ratio: 0.4685\n",
      "NDCG: 0.2606\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.30787068605422974\n",
      "[Training Epoch 1] Batch 2500, Loss 0.2979883849620819\n",
      "[Training Epoch 1] Batch 5000, Loss 0.28317949175834656\n",
      "[Evluating Epoch 1] HR = 0.5010, NDCG = 0.2788\n",
      "Hit Ratio: 0.5010\n",
      "NDCG: 0.2788\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.3086724579334259\n",
      "[Training Epoch 2] Batch 2500, Loss 0.28283530473709106\n",
      "[Training Epoch 2] Batch 5000, Loss 0.2666287124156952\n",
      "[Evluating Epoch 2] HR = 0.5513, NDCG = 0.3095\n",
      "Hit Ratio: 0.5513\n",
      "NDCG: 0.3095\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.24408718943595886\n",
      "[Training Epoch 3] Batch 2500, Loss 0.2059444934129715\n",
      "[Training Epoch 3] Batch 5000, Loss 0.22942547500133514\n",
      "[Evluating Epoch 3] HR = 0.5874, NDCG = 0.3347\n",
      "Hit Ratio: 0.5874\n",
      "NDCG: 0.3347\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.22673824429512024\n",
      "[Training Epoch 4] Batch 2500, Loss 0.2534690499305725\n",
      "[Training Epoch 4] Batch 5000, Loss 0.23907910287380219\n",
      "[Evluating Epoch 4] HR = 0.6061, NDCG = 0.3453\n",
      "Hit Ratio: 0.6061\n",
      "NDCG: 0.3453\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.2380959838628769\n",
      "[Training Epoch 5] Batch 2500, Loss 0.2256300449371338\n",
      "[Training Epoch 5] Batch 5000, Loss 0.22732417285442352\n",
      "[Evluating Epoch 5] HR = 0.6177, NDCG = 0.3540\n",
      "Hit Ratio: 0.6177\n",
      "NDCG: 0.3540\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.2314436137676239\n",
      "[Training Epoch 6] Batch 2500, Loss 0.2390139102935791\n",
      "[Training Epoch 6] Batch 5000, Loss 0.24374331533908844\n",
      "[Evluating Epoch 6] HR = 0.6252, NDCG = 0.3591\n",
      "Hit Ratio: 0.6252\n",
      "NDCG: 0.3591\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.2509589195251465\n",
      "[Training Epoch 7] Batch 2500, Loss 0.26640281081199646\n",
      "[Training Epoch 7] Batch 5000, Loss 0.23961451649665833\n",
      "[Evluating Epoch 7] HR = 0.6353, NDCG = 0.3639\n",
      "Hit Ratio: 0.6353\n",
      "NDCG: 0.3639\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.19766217470169067\n",
      "[Training Epoch 8] Batch 2500, Loss 0.25764894485473633\n",
      "[Training Epoch 8] Batch 5000, Loss 0.23151299357414246\n",
      "[Evluating Epoch 8] HR = 0.6331, NDCG = 0.3629\n",
      "Hit Ratio: 0.6331\n",
      "NDCG: 0.3629\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.21336159110069275\n",
      "[Training Epoch 9] Batch 2500, Loss 0.21830511093139648\n",
      "[Training Epoch 9] Batch 5000, Loss 0.2185775637626648\n",
      "[Evluating Epoch 9] HR = 0.6323, NDCG = 0.3631\n",
      "Hit Ratio: 0.6323\n",
      "NDCG: 0.3631\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.22533872723579407\n",
      "[Training Epoch 10] Batch 2500, Loss 0.2285662442445755\n",
      "[Training Epoch 10] Batch 5000, Loss 0.2373543679714203\n",
      "[Evluating Epoch 10] HR = 0.6391, NDCG = 0.3657\n",
      "Hit Ratio: 0.6391\n",
      "NDCG: 0.3657\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.22696048021316528\n",
      "[Training Epoch 11] Batch 2500, Loss 0.2044917345046997\n",
      "[Training Epoch 11] Batch 5000, Loss 0.24017980694770813\n",
      "[Evluating Epoch 11] HR = 0.6369, NDCG = 0.3653\n",
      "Hit Ratio: 0.6369\n",
      "NDCG: 0.3653\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.22756792604923248\n",
      "[Training Epoch 12] Batch 2500, Loss 0.20990824699401855\n",
      "[Training Epoch 12] Batch 5000, Loss 0.21551746129989624\n",
      "[Evluating Epoch 12] HR = 0.6406, NDCG = 0.3696\n",
      "Hit Ratio: 0.6406\n",
      "NDCG: 0.3696\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.19449061155319214\n",
      "[Training Epoch 13] Batch 2500, Loss 0.2502185106277466\n",
      "[Training Epoch 13] Batch 5000, Loss 0.21011608839035034\n",
      "[Evluating Epoch 13] HR = 0.6411, NDCG = 0.3693\n",
      "Hit Ratio: 0.6411\n",
      "NDCG: 0.3693\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.2106415033340454\n",
      "[Training Epoch 14] Batch 2500, Loss 0.2290831357240677\n",
      "[Training Epoch 14] Batch 5000, Loss 0.26905936002731323\n",
      "[Evluating Epoch 14] HR = 0.6391, NDCG = 0.3688\n",
      "Hit Ratio: 0.6391\n",
      "NDCG: 0.3688\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.2399580329656601\n",
      "[Training Epoch 15] Batch 2500, Loss 0.22605448961257935\n",
      "[Training Epoch 15] Batch 5000, Loss 0.24064034223556519\n",
      "[Evluating Epoch 15] HR = 0.6396, NDCG = 0.3699\n",
      "Hit Ratio: 0.6396\n",
      "NDCG: 0.3699\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.2445397973060608\n",
      "[Training Epoch 16] Batch 2500, Loss 0.2598850727081299\n",
      "[Training Epoch 16] Batch 5000, Loss 0.21309691667556763\n",
      "[Evluating Epoch 16] HR = 0.6412, NDCG = 0.3713\n",
      "Hit Ratio: 0.6412\n",
      "NDCG: 0.3713\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.2194192111492157\n",
      "[Training Epoch 17] Batch 2500, Loss 0.2052299678325653\n",
      "[Training Epoch 17] Batch 5000, Loss 0.23214411735534668\n",
      "[Evluating Epoch 17] HR = 0.6407, NDCG = 0.3693\n",
      "Hit Ratio: 0.6407\n",
      "NDCG: 0.3693\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.21727201342582703\n",
      "[Training Epoch 18] Batch 2500, Loss 0.21114712953567505\n",
      "[Training Epoch 18] Batch 5000, Loss 0.24282056093215942\n",
      "[Evluating Epoch 18] HR = 0.6425, NDCG = 0.3707\n",
      "Hit Ratio: 0.6425\n",
      "NDCG: 0.3707\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.21309977769851685\n",
      "[Training Epoch 19] Batch 2500, Loss 0.2311970591545105\n",
      "[Training Epoch 19] Batch 5000, Loss 0.20407438278198242\n",
      "[Evluating Epoch 19] HR = 0.6429, NDCG = 0.3726\n",
      "Hit Ratio: 0.6429\n",
      "NDCG: 0.3726\n",
      "--------------------------------------------------------------------------------\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=8, out_features=1, bias=True)\n",
      "MLP(\n",
      "  (embedding_user): Embedding(6040, 8)\n",
      "  (embedding_item): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.8190425038337708\n",
      "[Training Epoch 0] Batch 2500, Loss 0.28647875785827637\n",
      "[Training Epoch 0] Batch 5000, Loss 0.29365304112434387\n",
      "[Training Epoch 0] Batch 7500, Loss 0.3173304796218872\n",
      "[Training Epoch 0] Batch 10000, Loss 0.3295426666736603\n",
      "[Training Epoch 0] Batch 12500, Loss 0.36609119176864624\n",
      "[Training Epoch 0] Batch 15000, Loss 0.3184610903263092\n",
      "[Training Epoch 0] Batch 17500, Loss 0.3100404441356659\n",
      "[Training Epoch 0] Batch 20000, Loss 0.3101387023925781\n",
      "[Training Epoch 0] Batch 22500, Loss 0.31400346755981445\n",
      "[Training Epoch 0] Batch 25000, Loss 0.35128554701805115\n",
      "[Evluating Epoch 0] HR = 0.4497, NDCG = 0.2511\n",
      "Hit Ratio: 0.4497\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.25971341133117676\n",
      "[Training Epoch 1] Batch 2500, Loss 0.29910367727279663\n",
      "[Training Epoch 1] Batch 5000, Loss 0.2808886766433716\n",
      "[Training Epoch 1] Batch 7500, Loss 0.2965908646583557\n",
      "[Training Epoch 1] Batch 10000, Loss 0.271529883146286\n",
      "[Training Epoch 1] Batch 12500, Loss 0.3119901418685913\n",
      "[Training Epoch 1] Batch 15000, Loss 0.2933313846588135\n",
      "[Training Epoch 1] Batch 17500, Loss 0.2809302806854248\n",
      "[Training Epoch 1] Batch 20000, Loss 0.31868189573287964\n",
      "[Training Epoch 1] Batch 22500, Loss 0.2863071858882904\n",
      "[Training Epoch 1] Batch 25000, Loss 0.2605434060096741\n",
      "[Evluating Epoch 1] HR = 0.4478, NDCG = 0.2501\n",
      "Hit Ratio: 0.4478\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.3533680737018585\n",
      "[Training Epoch 2] Batch 2500, Loss 0.3073755204677582\n",
      "[Training Epoch 2] Batch 5000, Loss 0.29944610595703125\n",
      "[Training Epoch 2] Batch 7500, Loss 0.2887900471687317\n",
      "[Training Epoch 2] Batch 10000, Loss 0.30085277557373047\n",
      "[Training Epoch 2] Batch 12500, Loss 0.31697791814804077\n",
      "[Training Epoch 2] Batch 15000, Loss 0.24015673995018005\n",
      "[Training Epoch 2] Batch 17500, Loss 0.31004059314727783\n",
      "[Training Epoch 2] Batch 20000, Loss 0.2221572995185852\n",
      "[Training Epoch 2] Batch 22500, Loss 0.2824442982673645\n",
      "[Training Epoch 2] Batch 25000, Loss 0.23591305315494537\n",
      "[Evluating Epoch 2] HR = 0.4858, NDCG = 0.2706\n",
      "Hit Ratio: 0.4858\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.28475818037986755\n",
      "[Training Epoch 3] Batch 2500, Loss 0.2185731828212738\n",
      "[Training Epoch 3] Batch 5000, Loss 0.2269388735294342\n",
      "[Training Epoch 3] Batch 7500, Loss 0.22898069024085999\n",
      "[Training Epoch 3] Batch 10000, Loss 0.29280024766921997\n",
      "[Training Epoch 3] Batch 12500, Loss 0.2700684070587158\n",
      "[Training Epoch 3] Batch 15000, Loss 0.315307080745697\n",
      "[Training Epoch 3] Batch 17500, Loss 0.3171246349811554\n",
      "[Training Epoch 3] Batch 20000, Loss 0.2801135182380676\n",
      "[Training Epoch 3] Batch 22500, Loss 0.32712221145629883\n",
      "[Training Epoch 3] Batch 25000, Loss 0.2942642271518707\n",
      "[Evluating Epoch 3] HR = 0.4990, NDCG = 0.2777\n",
      "Hit Ratio: 0.4990\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.2652554512023926\n",
      "[Training Epoch 4] Batch 2500, Loss 0.32540756464004517\n",
      "[Training Epoch 4] Batch 5000, Loss 0.270866721868515\n",
      "[Training Epoch 4] Batch 7500, Loss 0.2432107925415039\n",
      "[Training Epoch 4] Batch 10000, Loss 0.27837416529655457\n",
      "[Training Epoch 4] Batch 12500, Loss 0.3211250305175781\n",
      "[Training Epoch 4] Batch 15000, Loss 0.2837231159210205\n",
      "[Training Epoch 4] Batch 17500, Loss 0.296289324760437\n",
      "[Training Epoch 4] Batch 20000, Loss 0.26092588901519775\n",
      "[Training Epoch 4] Batch 22500, Loss 0.3414570689201355\n",
      "[Training Epoch 4] Batch 25000, Loss 0.27677011489868164\n",
      "[Evluating Epoch 4] HR = 0.5114, NDCG = 0.2852\n",
      "Hit Ratio: 0.5114\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.27799075841903687\n",
      "[Training Epoch 5] Batch 2500, Loss 0.2862687408924103\n",
      "[Training Epoch 5] Batch 5000, Loss 0.23108768463134766\n",
      "[Training Epoch 5] Batch 7500, Loss 0.2430194467306137\n",
      "[Training Epoch 5] Batch 10000, Loss 0.24944144487380981\n",
      "[Training Epoch 5] Batch 12500, Loss 0.21108108758926392\n",
      "[Training Epoch 5] Batch 15000, Loss 0.2607235908508301\n",
      "[Training Epoch 5] Batch 17500, Loss 0.2402253895998001\n",
      "[Training Epoch 5] Batch 20000, Loss 0.21459349989891052\n",
      "[Training Epoch 5] Batch 22500, Loss 0.2721762955188751\n",
      "[Training Epoch 5] Batch 25000, Loss 0.3138464689254761\n",
      "[Evluating Epoch 5] HR = 0.5286, NDCG = 0.2957\n",
      "Hit Ratio: 0.5286\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.2779138684272766\n",
      "[Training Epoch 6] Batch 2500, Loss 0.2784040570259094\n",
      "[Training Epoch 6] Batch 5000, Loss 0.24174371361732483\n",
      "[Training Epoch 6] Batch 7500, Loss 0.26905590295791626\n",
      "[Training Epoch 6] Batch 10000, Loss 0.2922508716583252\n",
      "[Training Epoch 6] Batch 12500, Loss 0.27558302879333496\n",
      "[Training Epoch 6] Batch 15000, Loss 0.226356640458107\n",
      "[Training Epoch 6] Batch 17500, Loss 0.3075599670410156\n",
      "[Training Epoch 6] Batch 20000, Loss 0.27012455463409424\n",
      "[Training Epoch 6] Batch 22500, Loss 0.27279525995254517\n",
      "[Training Epoch 6] Batch 25000, Loss 0.2032308578491211\n",
      "[Evluating Epoch 6] HR = 0.5563, NDCG = 0.3100\n",
      "Hit Ratio: 0.5563\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.23766446113586426\n",
      "[Training Epoch 7] Batch 2500, Loss 0.29458171129226685\n",
      "[Training Epoch 7] Batch 5000, Loss 0.32852745056152344\n",
      "[Training Epoch 7] Batch 7500, Loss 0.25140413641929626\n",
      "[Training Epoch 7] Batch 10000, Loss 0.2298489212989807\n",
      "[Training Epoch 7] Batch 12500, Loss 0.26755720376968384\n",
      "[Training Epoch 7] Batch 15000, Loss 0.21644797921180725\n",
      "[Training Epoch 7] Batch 17500, Loss 0.25627601146698\n",
      "[Training Epoch 7] Batch 20000, Loss 0.25350338220596313\n",
      "[Training Epoch 7] Batch 22500, Loss 0.3161202371120453\n",
      "[Training Epoch 7] Batch 25000, Loss 0.27618858218193054\n",
      "[Evluating Epoch 7] HR = 0.5619, NDCG = 0.3143\n",
      "Hit Ratio: 0.5619\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.278170108795166\n",
      "[Training Epoch 8] Batch 2500, Loss 0.24024474620819092\n",
      "[Training Epoch 8] Batch 5000, Loss 0.24011118710041046\n",
      "[Training Epoch 8] Batch 7500, Loss 0.23385848104953766\n",
      "[Training Epoch 8] Batch 10000, Loss 0.1594129353761673\n",
      "[Training Epoch 8] Batch 12500, Loss 0.24022193253040314\n",
      "[Training Epoch 8] Batch 15000, Loss 0.26264965534210205\n",
      "[Training Epoch 8] Batch 17500, Loss 0.28455251455307007\n",
      "[Training Epoch 8] Batch 20000, Loss 0.19158801436424255\n",
      "[Training Epoch 8] Batch 22500, Loss 0.274861216545105\n",
      "[Training Epoch 8] Batch 25000, Loss 0.2473556101322174\n",
      "[Evluating Epoch 8] HR = 0.5725, NDCG = 0.3202\n",
      "Hit Ratio: 0.5725\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.28142762184143066\n",
      "[Training Epoch 9] Batch 2500, Loss 0.27756640315055847\n",
      "[Training Epoch 9] Batch 5000, Loss 0.19504453241825104\n",
      "[Training Epoch 9] Batch 7500, Loss 0.23296451568603516\n",
      "[Training Epoch 9] Batch 10000, Loss 0.2450927048921585\n",
      "[Training Epoch 9] Batch 12500, Loss 0.21771648526191711\n",
      "[Training Epoch 9] Batch 15000, Loss 0.3124566674232483\n",
      "[Training Epoch 9] Batch 17500, Loss 0.2365410029888153\n",
      "[Training Epoch 9] Batch 20000, Loss 0.2527766823768616\n",
      "[Training Epoch 9] Batch 22500, Loss 0.22412541508674622\n",
      "[Training Epoch 9] Batch 25000, Loss 0.21499894559383392\n",
      "[Evluating Epoch 9] HR = 0.5750, NDCG = 0.3249\n",
      "Hit Ratio: 0.5750\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.22565197944641113\n",
      "[Training Epoch 10] Batch 2500, Loss 0.24868285655975342\n",
      "[Training Epoch 10] Batch 5000, Loss 0.2189255952835083\n",
      "[Training Epoch 10] Batch 7500, Loss 0.27507394552230835\n",
      "[Training Epoch 10] Batch 10000, Loss 0.3524933457374573\n",
      "[Training Epoch 10] Batch 12500, Loss 0.2387068271636963\n",
      "[Training Epoch 10] Batch 15000, Loss 0.23386651277542114\n",
      "[Training Epoch 10] Batch 17500, Loss 0.253482460975647\n",
      "[Training Epoch 10] Batch 20000, Loss 0.25978633761405945\n",
      "[Training Epoch 10] Batch 22500, Loss 0.25369030237197876\n",
      "[Training Epoch 10] Batch 25000, Loss 0.25739988684654236\n",
      "[Evluating Epoch 10] HR = 0.5719, NDCG = 0.3233\n",
      "Hit Ratio: 0.5719\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.26333940029144287\n",
      "[Training Epoch 11] Batch 2500, Loss 0.24734905362129211\n",
      "[Training Epoch 11] Batch 5000, Loss 0.255977988243103\n",
      "[Training Epoch 11] Batch 7500, Loss 0.25927555561065674\n",
      "[Training Epoch 11] Batch 10000, Loss 0.18940724432468414\n",
      "[Training Epoch 11] Batch 12500, Loss 0.2462429702281952\n",
      "[Training Epoch 11] Batch 15000, Loss 0.2706936001777649\n",
      "[Training Epoch 11] Batch 17500, Loss 0.3436641991138458\n",
      "[Training Epoch 11] Batch 20000, Loss 0.30385488271713257\n",
      "[Training Epoch 11] Batch 22500, Loss 0.2050212025642395\n",
      "[Training Epoch 11] Batch 25000, Loss 0.2694723904132843\n",
      "[Evluating Epoch 11] HR = 0.5775, NDCG = 0.3266\n",
      "Hit Ratio: 0.5775\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.20603831112384796\n",
      "[Training Epoch 12] Batch 2500, Loss 0.3112386465072632\n",
      "[Training Epoch 12] Batch 5000, Loss 0.24632960557937622\n",
      "[Training Epoch 12] Batch 7500, Loss 0.26731327176094055\n",
      "[Training Epoch 12] Batch 10000, Loss 0.2883358895778656\n",
      "[Training Epoch 12] Batch 12500, Loss 0.24997849762439728\n",
      "[Training Epoch 12] Batch 15000, Loss 0.24005655944347382\n",
      "[Training Epoch 12] Batch 17500, Loss 0.2684745788574219\n",
      "[Training Epoch 12] Batch 20000, Loss 0.2348020374774933\n",
      "[Training Epoch 12] Batch 22500, Loss 0.24313116073608398\n",
      "[Training Epoch 12] Batch 25000, Loss 0.22455433011054993\n",
      "[Evluating Epoch 12] HR = 0.5785, NDCG = 0.3279\n",
      "Hit Ratio: 0.5785\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.27990540862083435\n",
      "[Training Epoch 13] Batch 2500, Loss 0.1749124825000763\n",
      "[Training Epoch 13] Batch 5000, Loss 0.2551823854446411\n",
      "[Training Epoch 13] Batch 7500, Loss 0.22326995432376862\n",
      "[Training Epoch 13] Batch 10000, Loss 0.21727623045444489\n",
      "[Training Epoch 13] Batch 12500, Loss 0.29872652888298035\n",
      "[Training Epoch 13] Batch 15000, Loss 0.24911180138587952\n",
      "[Training Epoch 13] Batch 17500, Loss 0.255212664604187\n",
      "[Training Epoch 13] Batch 20000, Loss 0.22511886060237885\n",
      "[Training Epoch 13] Batch 22500, Loss 0.2608400881290436\n",
      "[Training Epoch 13] Batch 25000, Loss 0.24558629095554352\n",
      "[Evluating Epoch 13] HR = 0.5768, NDCG = 0.3265\n",
      "Hit Ratio: 0.5768\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.25797826051712036\n",
      "[Training Epoch 14] Batch 2500, Loss 0.19685149192810059\n",
      "[Training Epoch 14] Batch 5000, Loss 0.25961196422576904\n",
      "[Training Epoch 14] Batch 7500, Loss 0.2470099776983261\n",
      "[Training Epoch 14] Batch 10000, Loss 0.32511645555496216\n",
      "[Training Epoch 14] Batch 12500, Loss 0.2548511326313019\n",
      "[Training Epoch 14] Batch 15000, Loss 0.24409428238868713\n",
      "[Training Epoch 14] Batch 17500, Loss 0.2797355651855469\n",
      "[Training Epoch 14] Batch 20000, Loss 0.2753625214099884\n",
      "[Training Epoch 14] Batch 22500, Loss 0.2448401302099228\n",
      "[Training Epoch 14] Batch 25000, Loss 0.19080936908721924\n",
      "[Evluating Epoch 14] HR = 0.5828, NDCG = 0.3277\n",
      "Hit Ratio: 0.5828\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.2399674355983734\n",
      "[Training Epoch 15] Batch 2500, Loss 0.2868138551712036\n",
      "[Training Epoch 15] Batch 5000, Loss 0.22741326689720154\n",
      "[Training Epoch 15] Batch 7500, Loss 0.25946685671806335\n",
      "[Training Epoch 15] Batch 10000, Loss 0.2623181641101837\n",
      "[Training Epoch 15] Batch 12500, Loss 0.2806406617164612\n",
      "[Training Epoch 15] Batch 15000, Loss 0.26950591802597046\n",
      "[Training Epoch 15] Batch 17500, Loss 0.2850790321826935\n",
      "[Training Epoch 15] Batch 20000, Loss 0.24309217929840088\n",
      "[Training Epoch 15] Batch 22500, Loss 0.25682538747787476\n",
      "[Training Epoch 15] Batch 25000, Loss 0.20276349782943726\n",
      "[Evluating Epoch 15] HR = 0.5816, NDCG = 0.3296\n",
      "Hit Ratio: 0.5816\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.26660585403442383\n",
      "[Training Epoch 16] Batch 2500, Loss 0.23790469765663147\n",
      "[Training Epoch 16] Batch 5000, Loss 0.23610559105873108\n",
      "[Training Epoch 16] Batch 7500, Loss 0.20509664714336395\n",
      "[Training Epoch 16] Batch 10000, Loss 0.2306816577911377\n",
      "[Training Epoch 16] Batch 12500, Loss 0.2955038845539093\n",
      "[Training Epoch 16] Batch 15000, Loss 0.23912692070007324\n",
      "[Training Epoch 16] Batch 17500, Loss 0.33074021339416504\n",
      "[Training Epoch 16] Batch 20000, Loss 0.21362048387527466\n",
      "[Training Epoch 16] Batch 22500, Loss 0.32620111107826233\n",
      "[Training Epoch 16] Batch 25000, Loss 0.18216966092586517\n",
      "[Evluating Epoch 16] HR = 0.5863, NDCG = 0.3307\n",
      "Hit Ratio: 0.5863\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.2556433379650116\n",
      "[Training Epoch 17] Batch 2500, Loss 0.22373685240745544\n",
      "[Training Epoch 17] Batch 5000, Loss 0.25979310274124146\n",
      "[Training Epoch 17] Batch 7500, Loss 0.23468908667564392\n",
      "[Training Epoch 17] Batch 10000, Loss 0.26818734407424927\n",
      "[Training Epoch 17] Batch 12500, Loss 0.3102002739906311\n",
      "[Training Epoch 17] Batch 15000, Loss 0.20998968183994293\n",
      "[Training Epoch 17] Batch 17500, Loss 0.26903051137924194\n",
      "[Training Epoch 17] Batch 20000, Loss 0.2691071331501007\n",
      "[Training Epoch 17] Batch 22500, Loss 0.20367372035980225\n",
      "[Training Epoch 17] Batch 25000, Loss 0.2015351951122284\n",
      "[Evluating Epoch 17] HR = 0.5863, NDCG = 0.3294\n",
      "Hit Ratio: 0.5863\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.2091406285762787\n",
      "[Training Epoch 18] Batch 2500, Loss 0.20183253288269043\n",
      "[Training Epoch 18] Batch 5000, Loss 0.2183523327112198\n",
      "[Training Epoch 18] Batch 7500, Loss 0.33341389894485474\n",
      "[Training Epoch 18] Batch 10000, Loss 0.23987098038196564\n",
      "[Training Epoch 18] Batch 12500, Loss 0.27068203687667847\n",
      "[Training Epoch 18] Batch 15000, Loss 0.2815306782722473\n",
      "[Training Epoch 18] Batch 17500, Loss 0.2541944980621338\n",
      "[Training Epoch 18] Batch 20000, Loss 0.22406604886054993\n",
      "[Training Epoch 18] Batch 22500, Loss 0.2226552665233612\n",
      "[Training Epoch 18] Batch 25000, Loss 0.2681720554828644\n",
      "[Evluating Epoch 18] HR = 0.5816, NDCG = 0.3290\n",
      "Hit Ratio: 0.5816\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.29103052616119385\n",
      "[Training Epoch 19] Batch 2500, Loss 0.24357233941555023\n",
      "[Training Epoch 19] Batch 5000, Loss 0.23803433775901794\n",
      "[Training Epoch 19] Batch 7500, Loss 0.2269754856824875\n",
      "[Training Epoch 19] Batch 10000, Loss 0.2557191252708435\n",
      "[Training Epoch 19] Batch 12500, Loss 0.24688202142715454\n",
      "[Training Epoch 19] Batch 15000, Loss 0.27771899104118347\n",
      "[Training Epoch 19] Batch 17500, Loss 0.226210817694664\n",
      "[Training Epoch 19] Batch 20000, Loss 0.26094990968704224\n",
      "[Training Epoch 19] Batch 22500, Loss 0.19310885667800903\n",
      "[Training Epoch 19] Batch 25000, Loss 0.24246922135353088\n",
      "[Evluating Epoch 19] HR = 0.5864, NDCG = 0.3314\n",
      "Hit Ratio: 0.5864\n",
      "--------------------------------------------------------------------------------\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=16, bias=True)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=16, out_features=1, bias=True)\n",
      "NeuMF(\n",
      "  (embedding_user_mlp): Embedding(6040, 8)\n",
      "  (embedding_item_mlp): Embedding(3706, 8)\n",
      "  (embedding_user_mf): Embedding(6040, 8)\n",
      "  (embedding_item_mf): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.6942477226257324\n",
      "[Training Epoch 0] Batch 2500, Loss 0.2720581293106079\n",
      "[Training Epoch 0] Batch 5000, Loss 0.2682565450668335\n",
      "[Evluating Epoch 0] HR = 0.5664, NDCG = 0.3199\n",
      "Hit Ratio: 0.5664\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.2623194456100464\n",
      "[Training Epoch 1] Batch 2500, Loss 0.26427292823791504\n",
      "[Training Epoch 1] Batch 5000, Loss 0.24504169821739197\n",
      "[Evluating Epoch 1] HR = 0.6141, NDCG = 0.3529\n",
      "Hit Ratio: 0.6141\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.22288410365581512\n",
      "[Training Epoch 2] Batch 2500, Loss 0.25600939989089966\n",
      "[Training Epoch 2] Batch 5000, Loss 0.23571622371673584\n",
      "[Evluating Epoch 2] HR = 0.6414, NDCG = 0.3673\n",
      "Hit Ratio: 0.6414\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.21631154417991638\n",
      "[Training Epoch 3] Batch 2500, Loss 0.23753315210342407\n",
      "[Training Epoch 3] Batch 5000, Loss 0.20930087566375732\n",
      "[Evluating Epoch 3] HR = 0.6437, NDCG = 0.3759\n",
      "Hit Ratio: 0.6437\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.21697863936424255\n",
      "[Training Epoch 4] Batch 2500, Loss 0.21909090876579285\n",
      "[Training Epoch 4] Batch 5000, Loss 0.22494763135910034\n",
      "[Evluating Epoch 4] HR = 0.6464, NDCG = 0.3770\n",
      "Hit Ratio: 0.6464\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.21358110010623932\n",
      "[Training Epoch 5] Batch 2500, Loss 0.2196383774280548\n",
      "[Training Epoch 5] Batch 5000, Loss 0.22440291941165924\n",
      "[Evluating Epoch 5] HR = 0.6555, NDCG = 0.3826\n",
      "Hit Ratio: 0.6555\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.18103671073913574\n",
      "[Training Epoch 6] Batch 2500, Loss 0.2222771942615509\n",
      "[Training Epoch 6] Batch 5000, Loss 0.21158748865127563\n",
      "[Evluating Epoch 6] HR = 0.6565, NDCG = 0.3849\n",
      "Hit Ratio: 0.6565\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.22371645271778107\n",
      "[Training Epoch 7] Batch 2500, Loss 0.19302549958229065\n",
      "[Training Epoch 7] Batch 5000, Loss 0.22537225484848022\n",
      "[Evluating Epoch 7] HR = 0.6616, NDCG = 0.3884\n",
      "Hit Ratio: 0.6616\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.22878354787826538\n",
      "[Training Epoch 8] Batch 2500, Loss 0.21411806344985962\n",
      "[Training Epoch 8] Batch 5000, Loss 0.20161652565002441\n",
      "[Evluating Epoch 8] HR = 0.6621, NDCG = 0.3888\n",
      "Hit Ratio: 0.6621\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.21710234880447388\n",
      "[Training Epoch 9] Batch 2500, Loss 0.20332129299640656\n",
      "[Training Epoch 9] Batch 5000, Loss 0.22489777207374573\n",
      "[Evluating Epoch 9] HR = 0.6632, NDCG = 0.3918\n",
      "Hit Ratio: 0.6632\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.22069957852363586\n",
      "[Training Epoch 10] Batch 2500, Loss 0.20112599432468414\n",
      "[Training Epoch 10] Batch 5000, Loss 0.2050512433052063\n",
      "[Evluating Epoch 10] HR = 0.6712, NDCG = 0.3944\n",
      "Hit Ratio: 0.6712\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.21410052478313446\n",
      "[Training Epoch 11] Batch 2500, Loss 0.22540676593780518\n",
      "[Training Epoch 11] Batch 5000, Loss 0.23050007224082947\n",
      "[Evluating Epoch 11] HR = 0.6752, NDCG = 0.3995\n",
      "Hit Ratio: 0.6752\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.20117783546447754\n",
      "[Training Epoch 12] Batch 2500, Loss 0.21001897752285004\n",
      "[Training Epoch 12] Batch 5000, Loss 0.202540785074234\n",
      "[Evluating Epoch 12] HR = 0.6758, NDCG = 0.3994\n",
      "Hit Ratio: 0.6758\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.2088170051574707\n",
      "[Training Epoch 13] Batch 2500, Loss 0.22446994483470917\n",
      "[Training Epoch 13] Batch 5000, Loss 0.22229868173599243\n",
      "[Evluating Epoch 13] HR = 0.6780, NDCG = 0.4017\n",
      "Hit Ratio: 0.6780\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.19165433943271637\n",
      "[Training Epoch 14] Batch 2500, Loss 0.22760526835918427\n",
      "[Training Epoch 14] Batch 5000, Loss 0.21885688602924347\n",
      "[Evluating Epoch 14] HR = 0.6770, NDCG = 0.4014\n",
      "Hit Ratio: 0.6770\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.2044958621263504\n",
      "[Training Epoch 15] Batch 2500, Loss 0.206935852766037\n",
      "[Training Epoch 15] Batch 5000, Loss 0.2253153920173645\n",
      "[Evluating Epoch 15] HR = 0.6800, NDCG = 0.4030\n",
      "Hit Ratio: 0.6800\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.19932442903518677\n",
      "[Training Epoch 16] Batch 2500, Loss 0.19143638014793396\n",
      "[Training Epoch 16] Batch 5000, Loss 0.21358157694339752\n",
      "[Evluating Epoch 16] HR = 0.6783, NDCG = 0.4026\n",
      "Hit Ratio: 0.6783\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.19486461579799652\n",
      "[Training Epoch 17] Batch 2500, Loss 0.1956418752670288\n",
      "[Training Epoch 17] Batch 5000, Loss 0.16531187295913696\n",
      "[Evluating Epoch 17] HR = 0.6781, NDCG = 0.4045\n",
      "Hit Ratio: 0.6781\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.22183944284915924\n",
      "[Training Epoch 18] Batch 2500, Loss 0.18732482194900513\n",
      "[Training Epoch 18] Batch 5000, Loss 0.194513738155365\n",
      "[Evluating Epoch 18] HR = 0.6796, NDCG = 0.4041\n",
      "Hit Ratio: 0.6796\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.23883283138275146\n",
      "[Training Epoch 19] Batch 2500, Loss 0.22739364206790924\n",
      "[Training Epoch 19] Batch 5000, Loss 0.20454561710357666\n",
      "[Evluating Epoch 19] HR = 0.6829, NDCG = 0.4072\n",
      "Hit Ratio: 0.6829\n",
      "--------------------------------------------------------------------------------\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "number of negatives: 7\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=8, out_features=1, bias=True)\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.7989685535430908\n",
      "[Training Epoch 0] Batch 2500, Loss 0.2762555181980133\n",
      "[Training Epoch 0] Batch 5000, Loss 0.259682297706604\n",
      "[Training Epoch 0] Batch 7500, Loss 0.2567424178123474\n",
      "[Evluating Epoch 0] HR = 0.4500, NDCG = 0.2497\n",
      "Hit Ratio: 0.4500\n",
      "NDCG: 0.2497\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.26673978567123413\n",
      "[Training Epoch 1] Batch 2500, Loss 0.2689596712589264\n",
      "[Training Epoch 1] Batch 5000, Loss 0.25335830450057983\n",
      "[Training Epoch 1] Batch 7500, Loss 0.23548592627048492\n",
      "[Evluating Epoch 1] HR = 0.4646, NDCG = 0.2595\n",
      "Hit Ratio: 0.4646\n",
      "NDCG: 0.2595\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.2550576329231262\n",
      "[Training Epoch 2] Batch 2500, Loss 0.25363674759864807\n",
      "[Training Epoch 2] Batch 5000, Loss 0.2661691904067993\n",
      "[Training Epoch 2] Batch 7500, Loss 0.24087350070476532\n",
      "[Evluating Epoch 2] HR = 0.5358, NDCG = 0.2986\n",
      "Hit Ratio: 0.5358\n",
      "NDCG: 0.2986\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.255362868309021\n",
      "[Training Epoch 3] Batch 2500, Loss 0.24176821112632751\n",
      "[Training Epoch 3] Batch 5000, Loss 0.24807190895080566\n",
      "[Training Epoch 3] Batch 7500, Loss 0.24394765496253967\n",
      "[Evluating Epoch 3] HR = 0.5611, NDCG = 0.3173\n",
      "Hit Ratio: 0.5611\n",
      "NDCG: 0.3173\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.21163420379161835\n",
      "[Training Epoch 4] Batch 2500, Loss 0.2253285050392151\n",
      "[Training Epoch 4] Batch 5000, Loss 0.21352148056030273\n",
      "[Training Epoch 4] Batch 7500, Loss 0.22546468675136566\n",
      "[Evluating Epoch 4] HR = 0.5843, NDCG = 0.3325\n",
      "Hit Ratio: 0.5843\n",
      "NDCG: 0.3325\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.21258282661437988\n",
      "[Training Epoch 5] Batch 2500, Loss 0.1979244351387024\n",
      "[Training Epoch 5] Batch 5000, Loss 0.22494345903396606\n",
      "[Training Epoch 5] Batch 7500, Loss 0.21466660499572754\n",
      "[Evluating Epoch 5] HR = 0.6046, NDCG = 0.3448\n",
      "Hit Ratio: 0.6046\n",
      "NDCG: 0.3448\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.20281749963760376\n",
      "[Training Epoch 6] Batch 2500, Loss 0.2368590533733368\n",
      "[Training Epoch 6] Batch 5000, Loss 0.2114868462085724\n",
      "[Training Epoch 6] Batch 7500, Loss 0.20387908816337585\n",
      "[Evluating Epoch 6] HR = 0.6141, NDCG = 0.3511\n",
      "Hit Ratio: 0.6141\n",
      "NDCG: 0.3511\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.2305358648300171\n",
      "[Training Epoch 7] Batch 2500, Loss 0.2460627555847168\n",
      "[Training Epoch 7] Batch 5000, Loss 0.2314727008342743\n",
      "[Training Epoch 7] Batch 7500, Loss 0.22362899780273438\n",
      "[Evluating Epoch 7] HR = 0.6253, NDCG = 0.3596\n",
      "Hit Ratio: 0.6253\n",
      "NDCG: 0.3596\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.22368177771568298\n",
      "[Training Epoch 8] Batch 2500, Loss 0.23164141178131104\n",
      "[Training Epoch 8] Batch 5000, Loss 0.20710135996341705\n",
      "[Training Epoch 8] Batch 7500, Loss 0.2006075233221054\n",
      "[Evluating Epoch 8] HR = 0.6306, NDCG = 0.3618\n",
      "Hit Ratio: 0.6306\n",
      "NDCG: 0.3618\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.23701389133930206\n",
      "[Training Epoch 9] Batch 2500, Loss 0.22342748939990997\n",
      "[Training Epoch 9] Batch 5000, Loss 0.19238676130771637\n",
      "[Training Epoch 9] Batch 7500, Loss 0.22906343638896942\n",
      "[Evluating Epoch 9] HR = 0.6351, NDCG = 0.3664\n",
      "Hit Ratio: 0.6351\n",
      "NDCG: 0.3664\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.24037842452526093\n",
      "[Training Epoch 10] Batch 2500, Loss 0.20552979409694672\n",
      "[Training Epoch 10] Batch 5000, Loss 0.20414258539676666\n",
      "[Training Epoch 10] Batch 7500, Loss 0.19228820502758026\n",
      "[Evluating Epoch 10] HR = 0.6407, NDCG = 0.3703\n",
      "Hit Ratio: 0.6407\n",
      "NDCG: 0.3703\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.21802550554275513\n",
      "[Training Epoch 11] Batch 2500, Loss 0.21679729223251343\n",
      "[Training Epoch 11] Batch 5000, Loss 0.23102518916130066\n",
      "[Training Epoch 11] Batch 7500, Loss 0.23046889901161194\n",
      "[Evluating Epoch 11] HR = 0.6374, NDCG = 0.3693\n",
      "Hit Ratio: 0.6374\n",
      "NDCG: 0.3693\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.20326094329357147\n",
      "[Training Epoch 12] Batch 2500, Loss 0.19919970631599426\n",
      "[Training Epoch 12] Batch 5000, Loss 0.24103105068206787\n",
      "[Training Epoch 12] Batch 7500, Loss 0.19281861186027527\n",
      "[Evluating Epoch 12] HR = 0.6449, NDCG = 0.3700\n",
      "Hit Ratio: 0.6449\n",
      "NDCG: 0.3700\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.20900478959083557\n",
      "[Training Epoch 13] Batch 2500, Loss 0.18239188194274902\n",
      "[Training Epoch 13] Batch 5000, Loss 0.21111980080604553\n",
      "[Training Epoch 13] Batch 7500, Loss 0.24289202690124512\n",
      "[Evluating Epoch 13] HR = 0.6373, NDCG = 0.3701\n",
      "Hit Ratio: 0.6373\n",
      "NDCG: 0.3701\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.23568935692310333\n",
      "[Training Epoch 14] Batch 2500, Loss 0.2215893268585205\n",
      "[Training Epoch 14] Batch 5000, Loss 0.22231212258338928\n",
      "[Training Epoch 14] Batch 7500, Loss 0.2033865749835968\n",
      "[Evluating Epoch 14] HR = 0.6454, NDCG = 0.3697\n",
      "Hit Ratio: 0.6454\n",
      "NDCG: 0.3697\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.20632898807525635\n",
      "[Training Epoch 15] Batch 2500, Loss 0.20009112358093262\n",
      "[Training Epoch 15] Batch 5000, Loss 0.2110276222229004\n",
      "[Training Epoch 15] Batch 7500, Loss 0.23819845914840698\n",
      "[Evluating Epoch 15] HR = 0.6457, NDCG = 0.3731\n",
      "Hit Ratio: 0.6457\n",
      "NDCG: 0.3731\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.20411428809165955\n",
      "[Training Epoch 16] Batch 2500, Loss 0.19309091567993164\n",
      "[Training Epoch 16] Batch 5000, Loss 0.23464719951152802\n",
      "[Training Epoch 16] Batch 7500, Loss 0.22838333249092102\n",
      "[Evluating Epoch 16] HR = 0.6470, NDCG = 0.3728\n",
      "Hit Ratio: 0.6470\n",
      "NDCG: 0.3728\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.2184486985206604\n",
      "[Training Epoch 17] Batch 2500, Loss 0.21351957321166992\n",
      "[Training Epoch 17] Batch 5000, Loss 0.24528475105762482\n",
      "[Training Epoch 17] Batch 7500, Loss 0.20683711767196655\n",
      "[Evluating Epoch 17] HR = 0.6437, NDCG = 0.3736\n",
      "Hit Ratio: 0.6437\n",
      "NDCG: 0.3736\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.20941004157066345\n",
      "[Training Epoch 18] Batch 2500, Loss 0.23228150606155396\n",
      "[Training Epoch 18] Batch 5000, Loss 0.2155378758907318\n",
      "[Training Epoch 18] Batch 7500, Loss 0.2020207792520523\n",
      "[Evluating Epoch 18] HR = 0.6450, NDCG = 0.3736\n",
      "Hit Ratio: 0.6450\n",
      "NDCG: 0.3736\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.24143189191818237\n",
      "[Training Epoch 19] Batch 2500, Loss 0.1976008415222168\n",
      "[Training Epoch 19] Batch 5000, Loss 0.22993092238903046\n",
      "[Training Epoch 19] Batch 7500, Loss 0.20769575238227844\n",
      "[Evluating Epoch 19] HR = 0.6465, NDCG = 0.3728\n",
      "Hit Ratio: 0.6465\n",
      "NDCG: 0.3728\n",
      "--------------------------------------------------------------------------------\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=8, out_features=1, bias=True)\n",
      "MLP(\n",
      "  (embedding_user): Embedding(6040, 8)\n",
      "  (embedding_item): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.6890077590942383\n",
      "[Training Epoch 0] Batch 2500, Loss 0.29669952392578125\n",
      "[Training Epoch 0] Batch 5000, Loss 0.2757025361061096\n",
      "[Training Epoch 0] Batch 7500, Loss 0.28210461139678955\n",
      "[Training Epoch 0] Batch 10000, Loss 0.29053163528442383\n",
      "[Training Epoch 0] Batch 12500, Loss 0.23722290992736816\n",
      "[Training Epoch 0] Batch 15000, Loss 0.27893567085266113\n",
      "[Training Epoch 0] Batch 17500, Loss 0.26719164848327637\n",
      "[Training Epoch 0] Batch 20000, Loss 0.31012436747550964\n",
      "[Training Epoch 0] Batch 22500, Loss 0.273146390914917\n",
      "[Training Epoch 0] Batch 25000, Loss 0.31714436411857605\n",
      "[Training Epoch 0] Batch 27500, Loss 0.36294472217559814\n",
      "[Training Epoch 0] Batch 30000, Loss 0.27399390935897827\n",
      "[Evluating Epoch 0] HR = 0.4445, NDCG = 0.2472\n",
      "Hit Ratio: 0.4445\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.29286885261535645\n",
      "[Training Epoch 1] Batch 2500, Loss 0.24469979107379913\n",
      "[Training Epoch 1] Batch 5000, Loss 0.2559473216533661\n",
      "[Training Epoch 1] Batch 7500, Loss 0.2450001984834671\n",
      "[Training Epoch 1] Batch 10000, Loss 0.27544283866882324\n",
      "[Training Epoch 1] Batch 12500, Loss 0.23814482986927032\n",
      "[Training Epoch 1] Batch 15000, Loss 0.3279207944869995\n",
      "[Training Epoch 1] Batch 17500, Loss 0.24083352088928223\n",
      "[Training Epoch 1] Batch 20000, Loss 0.25548648834228516\n",
      "[Training Epoch 1] Batch 22500, Loss 0.233595073223114\n",
      "[Training Epoch 1] Batch 25000, Loss 0.22295688092708588\n",
      "[Training Epoch 1] Batch 27500, Loss 0.2726282477378845\n",
      "[Training Epoch 1] Batch 30000, Loss 0.24422982335090637\n",
      "[Evluating Epoch 1] HR = 0.4637, NDCG = 0.2588\n",
      "Hit Ratio: 0.4637\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.22247488796710968\n",
      "[Training Epoch 2] Batch 2500, Loss 0.2354031354188919\n",
      "[Training Epoch 2] Batch 5000, Loss 0.23370890319347382\n",
      "[Training Epoch 2] Batch 7500, Loss 0.21898038685321808\n",
      "[Training Epoch 2] Batch 10000, Loss 0.3034251928329468\n",
      "[Training Epoch 2] Batch 12500, Loss 0.3089551329612732\n",
      "[Training Epoch 2] Batch 15000, Loss 0.24581101536750793\n",
      "[Training Epoch 2] Batch 17500, Loss 0.2574518620967865\n",
      "[Training Epoch 2] Batch 20000, Loss 0.2765517234802246\n",
      "[Training Epoch 2] Batch 22500, Loss 0.26374512910842896\n",
      "[Training Epoch 2] Batch 25000, Loss 0.28598976135253906\n",
      "[Training Epoch 2] Batch 27500, Loss 0.2487058788537979\n",
      "[Training Epoch 2] Batch 30000, Loss 0.27273157238960266\n",
      "[Evluating Epoch 2] HR = 0.4800, NDCG = 0.2662\n",
      "Hit Ratio: 0.4800\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.2561858296394348\n",
      "[Training Epoch 3] Batch 2500, Loss 0.2482704222202301\n",
      "[Training Epoch 3] Batch 5000, Loss 0.2611292600631714\n",
      "[Training Epoch 3] Batch 7500, Loss 0.2169773429632187\n",
      "[Training Epoch 3] Batch 10000, Loss 0.24802453815937042\n",
      "[Training Epoch 3] Batch 12500, Loss 0.254110187292099\n",
      "[Training Epoch 3] Batch 15000, Loss 0.26135411858558655\n",
      "[Training Epoch 3] Batch 17500, Loss 0.2689509391784668\n",
      "[Training Epoch 3] Batch 20000, Loss 0.2430533915758133\n",
      "[Training Epoch 3] Batch 22500, Loss 0.24934017658233643\n",
      "[Training Epoch 3] Batch 25000, Loss 0.28580430150032043\n",
      "[Training Epoch 3] Batch 27500, Loss 0.22963733971118927\n",
      "[Training Epoch 3] Batch 30000, Loss 0.2994931936264038\n",
      "[Evluating Epoch 3] HR = 0.4978, NDCG = 0.2762\n",
      "Hit Ratio: 0.4978\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.23169386386871338\n",
      "[Training Epoch 4] Batch 2500, Loss 0.22046725451946259\n",
      "[Training Epoch 4] Batch 5000, Loss 0.22882848978042603\n",
      "[Training Epoch 4] Batch 7500, Loss 0.22360457479953766\n",
      "[Training Epoch 4] Batch 10000, Loss 0.2513604760169983\n",
      "[Training Epoch 4] Batch 12500, Loss 0.24209201335906982\n",
      "[Training Epoch 4] Batch 15000, Loss 0.18774595856666565\n",
      "[Training Epoch 4] Batch 17500, Loss 0.2037113606929779\n",
      "[Training Epoch 4] Batch 20000, Loss 0.3252484202384949\n",
      "[Training Epoch 4] Batch 22500, Loss 0.2749931514263153\n",
      "[Training Epoch 4] Batch 25000, Loss 0.23862183094024658\n",
      "[Training Epoch 4] Batch 27500, Loss 0.31972992420196533\n",
      "[Training Epoch 4] Batch 30000, Loss 0.3066663444042206\n",
      "[Evluating Epoch 4] HR = 0.5180, NDCG = 0.2864\n",
      "Hit Ratio: 0.5180\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.26287925243377686\n",
      "[Training Epoch 5] Batch 2500, Loss 0.24449005722999573\n",
      "[Training Epoch 5] Batch 5000, Loss 0.2726650536060333\n",
      "[Training Epoch 5] Batch 7500, Loss 0.20944222807884216\n",
      "[Training Epoch 5] Batch 10000, Loss 0.21486853063106537\n",
      "[Training Epoch 5] Batch 12500, Loss 0.19715122878551483\n",
      "[Training Epoch 5] Batch 15000, Loss 0.21880704164505005\n",
      "[Training Epoch 5] Batch 17500, Loss 0.21190640330314636\n",
      "[Training Epoch 5] Batch 20000, Loss 0.24820876121520996\n",
      "[Training Epoch 5] Batch 22500, Loss 0.3027438819408417\n",
      "[Training Epoch 5] Batch 25000, Loss 0.27958208322525024\n",
      "[Training Epoch 5] Batch 27500, Loss 0.27720701694488525\n",
      "[Training Epoch 5] Batch 30000, Loss 0.19070294499397278\n",
      "[Evluating Epoch 5] HR = 0.5343, NDCG = 0.2974\n",
      "Hit Ratio: 0.5343\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.2786279320716858\n",
      "[Training Epoch 6] Batch 2500, Loss 0.23437395691871643\n",
      "[Training Epoch 6] Batch 5000, Loss 0.20873382687568665\n",
      "[Training Epoch 6] Batch 7500, Loss 0.24608251452445984\n",
      "[Training Epoch 6] Batch 10000, Loss 0.24245648086071014\n",
      "[Training Epoch 6] Batch 12500, Loss 0.2531450092792511\n",
      "[Training Epoch 6] Batch 15000, Loss 0.20576366782188416\n",
      "[Training Epoch 6] Batch 17500, Loss 0.26883193850517273\n",
      "[Training Epoch 6] Batch 20000, Loss 0.1639096736907959\n",
      "[Training Epoch 6] Batch 22500, Loss 0.25382497906684875\n",
      "[Training Epoch 6] Batch 25000, Loss 0.23876091837882996\n",
      "[Training Epoch 6] Batch 27500, Loss 0.18844062089920044\n",
      "[Training Epoch 6] Batch 30000, Loss 0.2822757959365845\n",
      "[Evluating Epoch 6] HR = 0.5381, NDCG = 0.3004\n",
      "Hit Ratio: 0.5381\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.23889562487602234\n",
      "[Training Epoch 7] Batch 2500, Loss 0.23949499428272247\n",
      "[Training Epoch 7] Batch 5000, Loss 0.19009585678577423\n",
      "[Training Epoch 7] Batch 7500, Loss 0.2505701780319214\n",
      "[Training Epoch 7] Batch 10000, Loss 0.25610649585723877\n",
      "[Training Epoch 7] Batch 12500, Loss 0.1900751292705536\n",
      "[Training Epoch 7] Batch 15000, Loss 0.23448577523231506\n",
      "[Training Epoch 7] Batch 17500, Loss 0.24492132663726807\n",
      "[Training Epoch 7] Batch 20000, Loss 0.28516364097595215\n",
      "[Training Epoch 7] Batch 22500, Loss 0.2932918667793274\n",
      "[Training Epoch 7] Batch 25000, Loss 0.1768663376569748\n",
      "[Training Epoch 7] Batch 27500, Loss 0.22286629676818848\n",
      "[Training Epoch 7] Batch 30000, Loss 0.26334232091903687\n",
      "[Evluating Epoch 7] HR = 0.5510, NDCG = 0.3111\n",
      "Hit Ratio: 0.5510\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.25868186354637146\n",
      "[Training Epoch 8] Batch 2500, Loss 0.2280595898628235\n",
      "[Training Epoch 8] Batch 5000, Loss 0.23509857058525085\n",
      "[Training Epoch 8] Batch 7500, Loss 0.23347318172454834\n",
      "[Training Epoch 8] Batch 10000, Loss 0.2700892686843872\n",
      "[Training Epoch 8] Batch 12500, Loss 0.27603575587272644\n",
      "[Training Epoch 8] Batch 15000, Loss 0.2644680142402649\n",
      "[Training Epoch 8] Batch 17500, Loss 0.19976717233657837\n",
      "[Training Epoch 8] Batch 20000, Loss 0.1829308569431305\n",
      "[Training Epoch 8] Batch 22500, Loss 0.23910421133041382\n",
      "[Training Epoch 8] Batch 25000, Loss 0.26758670806884766\n",
      "[Training Epoch 8] Batch 27500, Loss 0.26943400502204895\n",
      "[Training Epoch 8] Batch 30000, Loss 0.24433329701423645\n",
      "[Evluating Epoch 8] HR = 0.5591, NDCG = 0.3169\n",
      "Hit Ratio: 0.5591\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.2408405989408493\n",
      "[Training Epoch 9] Batch 2500, Loss 0.25638407468795776\n",
      "[Training Epoch 9] Batch 5000, Loss 0.16210733354091644\n",
      "[Training Epoch 9] Batch 7500, Loss 0.24031279981136322\n",
      "[Training Epoch 9] Batch 10000, Loss 0.1964341551065445\n",
      "[Training Epoch 9] Batch 12500, Loss 0.24031957983970642\n",
      "[Training Epoch 9] Batch 15000, Loss 0.2738657593727112\n",
      "[Training Epoch 9] Batch 17500, Loss 0.28505921363830566\n",
      "[Training Epoch 9] Batch 20000, Loss 0.24298328161239624\n",
      "[Training Epoch 9] Batch 22500, Loss 0.20527023077011108\n",
      "[Training Epoch 9] Batch 25000, Loss 0.20865336060523987\n",
      "[Training Epoch 9] Batch 27500, Loss 0.18993017077445984\n",
      "[Training Epoch 9] Batch 30000, Loss 0.2773953378200531\n",
      "[Evluating Epoch 9] HR = 0.5692, NDCG = 0.3245\n",
      "Hit Ratio: 0.5692\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.29201558232307434\n",
      "[Training Epoch 10] Batch 2500, Loss 0.2195483148097992\n",
      "[Training Epoch 10] Batch 5000, Loss 0.2544947564601898\n",
      "[Training Epoch 10] Batch 7500, Loss 0.2570449113845825\n",
      "[Training Epoch 10] Batch 10000, Loss 0.20578935742378235\n",
      "[Training Epoch 10] Batch 12500, Loss 0.18812261521816254\n",
      "[Training Epoch 10] Batch 15000, Loss 0.22567415237426758\n",
      "[Training Epoch 10] Batch 17500, Loss 0.19886961579322815\n",
      "[Training Epoch 10] Batch 20000, Loss 0.2153112292289734\n",
      "[Training Epoch 10] Batch 22500, Loss 0.23675969243049622\n",
      "[Training Epoch 10] Batch 25000, Loss 0.22220948338508606\n",
      "[Training Epoch 10] Batch 27500, Loss 0.21329592168331146\n",
      "[Training Epoch 10] Batch 30000, Loss 0.22973699867725372\n",
      "[Evluating Epoch 10] HR = 0.5684, NDCG = 0.3227\n",
      "Hit Ratio: 0.5684\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.19343172013759613\n",
      "[Training Epoch 11] Batch 2500, Loss 0.22296923398971558\n",
      "[Training Epoch 11] Batch 5000, Loss 0.1980515718460083\n",
      "[Training Epoch 11] Batch 7500, Loss 0.21140894293785095\n",
      "[Training Epoch 11] Batch 10000, Loss 0.19501253962516785\n",
      "[Training Epoch 11] Batch 12500, Loss 0.2456625998020172\n",
      "[Training Epoch 11] Batch 15000, Loss 0.22521033883094788\n",
      "[Training Epoch 11] Batch 17500, Loss 0.2444612681865692\n",
      "[Training Epoch 11] Batch 20000, Loss 0.19598044455051422\n",
      "[Training Epoch 11] Batch 22500, Loss 0.2277776002883911\n",
      "[Training Epoch 11] Batch 25000, Loss 0.14232662320137024\n",
      "[Training Epoch 11] Batch 27500, Loss 0.29461613297462463\n",
      "[Training Epoch 11] Batch 30000, Loss 0.20485064387321472\n",
      "[Evluating Epoch 11] HR = 0.5707, NDCG = 0.3248\n",
      "Hit Ratio: 0.5707\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.21022473275661469\n",
      "[Training Epoch 12] Batch 2500, Loss 0.22011534869670868\n",
      "[Training Epoch 12] Batch 5000, Loss 0.30115407705307007\n",
      "[Training Epoch 12] Batch 7500, Loss 0.25380098819732666\n",
      "[Training Epoch 12] Batch 10000, Loss 0.24489614367485046\n",
      "[Training Epoch 12] Batch 12500, Loss 0.24850505590438843\n",
      "[Training Epoch 12] Batch 15000, Loss 0.22879210114479065\n",
      "[Training Epoch 12] Batch 17500, Loss 0.21515806019306183\n",
      "[Training Epoch 12] Batch 20000, Loss 0.23846517503261566\n",
      "[Training Epoch 12] Batch 22500, Loss 0.22999002039432526\n",
      "[Training Epoch 12] Batch 25000, Loss 0.2114761769771576\n",
      "[Training Epoch 12] Batch 27500, Loss 0.2567550837993622\n",
      "[Training Epoch 12] Batch 30000, Loss 0.2738909125328064\n",
      "[Evluating Epoch 12] HR = 0.5704, NDCG = 0.3238\n",
      "Hit Ratio: 0.5704\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.28775203227996826\n",
      "[Training Epoch 13] Batch 2500, Loss 0.2213921844959259\n",
      "[Training Epoch 13] Batch 5000, Loss 0.249847412109375\n",
      "[Training Epoch 13] Batch 7500, Loss 0.2277333289384842\n",
      "[Training Epoch 13] Batch 10000, Loss 0.20375457406044006\n",
      "[Training Epoch 13] Batch 12500, Loss 0.2598086893558502\n",
      "[Training Epoch 13] Batch 15000, Loss 0.22868463397026062\n",
      "[Training Epoch 13] Batch 17500, Loss 0.19272151589393616\n",
      "[Training Epoch 13] Batch 20000, Loss 0.21369758248329163\n",
      "[Training Epoch 13] Batch 22500, Loss 0.2259376049041748\n",
      "[Training Epoch 13] Batch 25000, Loss 0.3036688566207886\n",
      "[Training Epoch 13] Batch 27500, Loss 0.2140657603740692\n",
      "[Training Epoch 13] Batch 30000, Loss 0.209314227104187\n",
      "[Evluating Epoch 13] HR = 0.5733, NDCG = 0.3277\n",
      "Hit Ratio: 0.5733\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.19210520386695862\n",
      "[Training Epoch 14] Batch 2500, Loss 0.253887414932251\n",
      "[Training Epoch 14] Batch 5000, Loss 0.241485595703125\n",
      "[Training Epoch 14] Batch 7500, Loss 0.21008646488189697\n",
      "[Training Epoch 14] Batch 10000, Loss 0.2387598156929016\n",
      "[Training Epoch 14] Batch 12500, Loss 0.20789435505867004\n",
      "[Training Epoch 14] Batch 15000, Loss 0.1812857687473297\n",
      "[Training Epoch 14] Batch 17500, Loss 0.20661093294620514\n",
      "[Training Epoch 14] Batch 20000, Loss 0.24074654281139374\n",
      "[Training Epoch 14] Batch 22500, Loss 0.19188091158866882\n",
      "[Training Epoch 14] Batch 25000, Loss 0.3350788652896881\n",
      "[Training Epoch 14] Batch 27500, Loss 0.23311609029769897\n",
      "[Training Epoch 14] Batch 30000, Loss 0.23520655930042267\n",
      "[Evluating Epoch 14] HR = 0.5707, NDCG = 0.3255\n",
      "Hit Ratio: 0.5707\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.2524665892124176\n",
      "[Training Epoch 15] Batch 2500, Loss 0.21978279948234558\n",
      "[Training Epoch 15] Batch 5000, Loss 0.23429536819458008\n",
      "[Training Epoch 15] Batch 7500, Loss 0.23899364471435547\n",
      "[Training Epoch 15] Batch 10000, Loss 0.21011397242546082\n",
      "[Training Epoch 15] Batch 12500, Loss 0.25895145535469055\n",
      "[Training Epoch 15] Batch 15000, Loss 0.3176664113998413\n",
      "[Training Epoch 15] Batch 17500, Loss 0.20455460250377655\n",
      "[Training Epoch 15] Batch 20000, Loss 0.19971682131290436\n",
      "[Training Epoch 15] Batch 22500, Loss 0.333761990070343\n",
      "[Training Epoch 15] Batch 25000, Loss 0.19522008299827576\n",
      "[Training Epoch 15] Batch 27500, Loss 0.2299991250038147\n",
      "[Training Epoch 15] Batch 30000, Loss 0.17623193562030792\n",
      "[Evluating Epoch 15] HR = 0.5767, NDCG = 0.3280\n",
      "Hit Ratio: 0.5767\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.22827965021133423\n",
      "[Training Epoch 16] Batch 2500, Loss 0.21391285955905914\n",
      "[Training Epoch 16] Batch 5000, Loss 0.24598851799964905\n",
      "[Training Epoch 16] Batch 7500, Loss 0.23770812153816223\n",
      "[Training Epoch 16] Batch 10000, Loss 0.2067299485206604\n",
      "[Training Epoch 16] Batch 12500, Loss 0.19143569469451904\n",
      "[Training Epoch 16] Batch 15000, Loss 0.24453318119049072\n",
      "[Training Epoch 16] Batch 17500, Loss 0.24523837864398956\n",
      "[Training Epoch 16] Batch 20000, Loss 0.20506122708320618\n",
      "[Training Epoch 16] Batch 22500, Loss 0.20392093062400818\n",
      "[Training Epoch 16] Batch 25000, Loss 0.2584531903266907\n",
      "[Training Epoch 16] Batch 27500, Loss 0.2681523561477661\n",
      "[Training Epoch 16] Batch 30000, Loss 0.2473316192626953\n",
      "[Evluating Epoch 16] HR = 0.5816, NDCG = 0.3293\n",
      "Hit Ratio: 0.5816\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.23192331194877625\n",
      "[Training Epoch 17] Batch 2500, Loss 0.2677367329597473\n",
      "[Training Epoch 17] Batch 5000, Loss 0.23723073303699493\n",
      "[Training Epoch 17] Batch 7500, Loss 0.20449185371398926\n",
      "[Training Epoch 17] Batch 10000, Loss 0.20609210431575775\n",
      "[Training Epoch 17] Batch 12500, Loss 0.18867544829845428\n",
      "[Training Epoch 17] Batch 15000, Loss 0.17707400023937225\n",
      "[Training Epoch 17] Batch 17500, Loss 0.2750469446182251\n",
      "[Training Epoch 17] Batch 20000, Loss 0.20866696536540985\n",
      "[Training Epoch 17] Batch 22500, Loss 0.21104352176189423\n",
      "[Training Epoch 17] Batch 25000, Loss 0.23416467010974884\n",
      "[Training Epoch 17] Batch 27500, Loss 0.18451350927352905\n",
      "[Training Epoch 17] Batch 30000, Loss 0.1791384369134903\n",
      "[Evluating Epoch 17] HR = 0.5861, NDCG = 0.3322\n",
      "Hit Ratio: 0.5861\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.3063032627105713\n",
      "[Training Epoch 18] Batch 2500, Loss 0.15650692582130432\n",
      "[Training Epoch 18] Batch 5000, Loss 0.1740833818912506\n",
      "[Training Epoch 18] Batch 7500, Loss 0.2566153407096863\n",
      "[Training Epoch 18] Batch 10000, Loss 0.25274837017059326\n",
      "[Training Epoch 18] Batch 12500, Loss 0.22046545147895813\n",
      "[Training Epoch 18] Batch 15000, Loss 0.1794041097164154\n",
      "[Training Epoch 18] Batch 17500, Loss 0.20816200971603394\n",
      "[Training Epoch 18] Batch 20000, Loss 0.24484029412269592\n",
      "[Training Epoch 18] Batch 22500, Loss 0.21877866983413696\n",
      "[Training Epoch 18] Batch 25000, Loss 0.29865556955337524\n",
      "[Training Epoch 18] Batch 27500, Loss 0.19009339809417725\n",
      "[Training Epoch 18] Batch 30000, Loss 0.22075700759887695\n",
      "[Evluating Epoch 18] HR = 0.5796, NDCG = 0.3305\n",
      "Hit Ratio: 0.5796\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.2830294668674469\n",
      "[Training Epoch 19] Batch 2500, Loss 0.2834930419921875\n",
      "[Training Epoch 19] Batch 5000, Loss 0.24607327580451965\n",
      "[Training Epoch 19] Batch 7500, Loss 0.21854659914970398\n",
      "[Training Epoch 19] Batch 10000, Loss 0.19506829977035522\n",
      "[Training Epoch 19] Batch 12500, Loss 0.2506084442138672\n",
      "[Training Epoch 19] Batch 15000, Loss 0.21154195070266724\n",
      "[Training Epoch 19] Batch 17500, Loss 0.24270588159561157\n",
      "[Training Epoch 19] Batch 20000, Loss 0.24239391088485718\n",
      "[Training Epoch 19] Batch 22500, Loss 0.22746148705482483\n",
      "[Training Epoch 19] Batch 25000, Loss 0.21855038404464722\n",
      "[Training Epoch 19] Batch 27500, Loss 0.26018768548965454\n",
      "[Training Epoch 19] Batch 30000, Loss 0.17510192096233368\n",
      "[Evluating Epoch 19] HR = 0.5762, NDCG = 0.3276\n",
      "Hit Ratio: 0.5762\n",
      "--------------------------------------------------------------------------------\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=16, bias=True)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=16, out_features=1, bias=True)\n",
      "NeuMF(\n",
      "  (embedding_user_mlp): Embedding(6040, 8)\n",
      "  (embedding_item_mlp): Embedding(3706, 8)\n",
      "  (embedding_user_mf): Embedding(6040, 8)\n",
      "  (embedding_item_mf): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.7572648525238037\n",
      "[Training Epoch 0] Batch 2500, Loss 0.2797883152961731\n",
      "[Training Epoch 0] Batch 5000, Loss 0.2304837703704834\n",
      "[Training Epoch 0] Batch 7500, Loss 0.23000368475914001\n",
      "[Evluating Epoch 0] HR = 0.5672, NDCG = 0.3198\n",
      "Hit Ratio: 0.5672\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.22560074925422668\n",
      "[Training Epoch 1] Batch 2500, Loss 0.21620018780231476\n",
      "[Training Epoch 1] Batch 5000, Loss 0.23030978441238403\n",
      "[Training Epoch 1] Batch 7500, Loss 0.2696148157119751\n",
      "[Evluating Epoch 1] HR = 0.6060, NDCG = 0.3470\n",
      "Hit Ratio: 0.6060\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.2096254527568817\n",
      "[Training Epoch 2] Batch 2500, Loss 0.22295980155467987\n",
      "[Training Epoch 2] Batch 5000, Loss 0.26197201013565063\n",
      "[Training Epoch 2] Batch 7500, Loss 0.2170523703098297\n",
      "[Evluating Epoch 2] HR = 0.6336, NDCG = 0.3648\n",
      "Hit Ratio: 0.6336\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.20414528250694275\n",
      "[Training Epoch 3] Batch 2500, Loss 0.20265427231788635\n",
      "[Training Epoch 3] Batch 5000, Loss 0.21788756549358368\n",
      "[Training Epoch 3] Batch 7500, Loss 0.2231859266757965\n",
      "[Evluating Epoch 3] HR = 0.6376, NDCG = 0.3664\n",
      "Hit Ratio: 0.6376\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.21403658390045166\n",
      "[Training Epoch 4] Batch 2500, Loss 0.22401238977909088\n",
      "[Training Epoch 4] Batch 5000, Loss 0.20214161276817322\n",
      "[Training Epoch 4] Batch 7500, Loss 0.23248104751110077\n",
      "[Evluating Epoch 4] HR = 0.6444, NDCG = 0.3749\n",
      "Hit Ratio: 0.6444\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.1925446093082428\n",
      "[Training Epoch 5] Batch 2500, Loss 0.21131104230880737\n",
      "[Training Epoch 5] Batch 5000, Loss 0.23964262008666992\n",
      "[Training Epoch 5] Batch 7500, Loss 0.24580839276313782\n",
      "[Evluating Epoch 5] HR = 0.6460, NDCG = 0.3778\n",
      "Hit Ratio: 0.6460\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.19411125779151917\n",
      "[Training Epoch 6] Batch 2500, Loss 0.21979999542236328\n",
      "[Training Epoch 6] Batch 5000, Loss 0.2416469007730484\n",
      "[Training Epoch 6] Batch 7500, Loss 0.20635896921157837\n",
      "[Evluating Epoch 6] HR = 0.6510, NDCG = 0.3795\n",
      "Hit Ratio: 0.6510\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.19220499694347382\n",
      "[Training Epoch 7] Batch 2500, Loss 0.2076444774866104\n",
      "[Training Epoch 7] Batch 5000, Loss 0.22268101572990417\n",
      "[Training Epoch 7] Batch 7500, Loss 0.22483152151107788\n",
      "[Evluating Epoch 7] HR = 0.6528, NDCG = 0.3833\n",
      "Hit Ratio: 0.6528\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.18078547716140747\n",
      "[Training Epoch 8] Batch 2500, Loss 0.1755596399307251\n",
      "[Training Epoch 8] Batch 5000, Loss 0.1773787885904312\n",
      "[Training Epoch 8] Batch 7500, Loss 0.18233159184455872\n",
      "[Evluating Epoch 8] HR = 0.6604, NDCG = 0.3886\n",
      "Hit Ratio: 0.6604\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.20017386972904205\n",
      "[Training Epoch 9] Batch 2500, Loss 0.20896892249584198\n",
      "[Training Epoch 9] Batch 5000, Loss 0.2168884575366974\n",
      "[Training Epoch 9] Batch 7500, Loss 0.23285430669784546\n",
      "[Evluating Epoch 9] HR = 0.6631, NDCG = 0.3912\n",
      "Hit Ratio: 0.6631\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.17659375071525574\n",
      "[Training Epoch 10] Batch 2500, Loss 0.22795474529266357\n",
      "[Training Epoch 10] Batch 5000, Loss 0.2107062190771103\n",
      "[Training Epoch 10] Batch 7500, Loss 0.1883896142244339\n",
      "[Evluating Epoch 10] HR = 0.6621, NDCG = 0.3922\n",
      "Hit Ratio: 0.6621\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.17755171656608582\n",
      "[Training Epoch 11] Batch 2500, Loss 0.21459566056728363\n",
      "[Training Epoch 11] Batch 5000, Loss 0.21167030930519104\n",
      "[Training Epoch 11] Batch 7500, Loss 0.20256422460079193\n",
      "[Evluating Epoch 11] HR = 0.6619, NDCG = 0.3929\n",
      "Hit Ratio: 0.6619\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.18990549445152283\n",
      "[Training Epoch 12] Batch 2500, Loss 0.19342459738254547\n",
      "[Training Epoch 12] Batch 5000, Loss 0.2058904767036438\n",
      "[Training Epoch 12] Batch 7500, Loss 0.18665696680545807\n",
      "[Evluating Epoch 12] HR = 0.6652, NDCG = 0.3946\n",
      "Hit Ratio: 0.6652\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.1919257640838623\n",
      "[Training Epoch 13] Batch 2500, Loss 0.16581183671951294\n",
      "[Training Epoch 13] Batch 5000, Loss 0.22183293104171753\n",
      "[Training Epoch 13] Batch 7500, Loss 0.21210139989852905\n",
      "[Evluating Epoch 13] HR = 0.6689, NDCG = 0.3974\n",
      "Hit Ratio: 0.6689\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.2067141830921173\n",
      "[Training Epoch 14] Batch 2500, Loss 0.19319501519203186\n",
      "[Training Epoch 14] Batch 5000, Loss 0.16828981041908264\n",
      "[Training Epoch 14] Batch 7500, Loss 0.16334429383277893\n",
      "[Evluating Epoch 14] HR = 0.6725, NDCG = 0.4003\n",
      "Hit Ratio: 0.6725\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.18957820534706116\n",
      "[Training Epoch 15] Batch 2500, Loss 0.18037080764770508\n",
      "[Training Epoch 15] Batch 5000, Loss 0.18471556901931763\n",
      "[Training Epoch 15] Batch 7500, Loss 0.22457730770111084\n",
      "[Evluating Epoch 15] HR = 0.6684, NDCG = 0.3991\n",
      "Hit Ratio: 0.6684\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.20540118217468262\n",
      "[Training Epoch 16] Batch 2500, Loss 0.19932958483695984\n",
      "[Training Epoch 16] Batch 5000, Loss 0.19293107092380524\n",
      "[Training Epoch 16] Batch 7500, Loss 0.22000277042388916\n",
      "[Evluating Epoch 16] HR = 0.6732, NDCG = 0.4018\n",
      "Hit Ratio: 0.6732\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.2384638488292694\n",
      "[Training Epoch 17] Batch 2500, Loss 0.16601873934268951\n",
      "[Training Epoch 17] Batch 5000, Loss 0.19723722338676453\n",
      "[Training Epoch 17] Batch 7500, Loss 0.19295981526374817\n",
      "[Evluating Epoch 17] HR = 0.6747, NDCG = 0.4038\n",
      "Hit Ratio: 0.6747\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.21544024348258972\n",
      "[Training Epoch 18] Batch 2500, Loss 0.16923877596855164\n",
      "[Training Epoch 18] Batch 5000, Loss 0.18609869480133057\n",
      "[Training Epoch 18] Batch 7500, Loss 0.15541377663612366\n",
      "[Evluating Epoch 18] HR = 0.6724, NDCG = 0.4026\n",
      "Hit Ratio: 0.6724\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.178833469748497\n",
      "[Training Epoch 19] Batch 2500, Loss 0.1926044225692749\n",
      "[Training Epoch 19] Batch 5000, Loss 0.2044021636247635\n",
      "[Training Epoch 19] Batch 7500, Loss 0.1738896369934082\n",
      "[Evluating Epoch 19] HR = 0.6765, NDCG = 0.4070\n",
      "Hit Ratio: 0.6765\n",
      "--------------------------------------------------------------------------------\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "number of negatives: 8\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=8, out_features=1, bias=True)\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.5991106033325195\n",
      "[Training Epoch 0] Batch 2500, Loss 0.2618700861930847\n",
      "[Training Epoch 0] Batch 5000, Loss 0.26138654351234436\n",
      "[Training Epoch 0] Batch 7500, Loss 0.2577971816062927\n",
      "[Evluating Epoch 0] HR = 0.4437, NDCG = 0.2477\n",
      "Hit Ratio: 0.4437\n",
      "NDCG: 0.2477\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.21987885236740112\n",
      "[Training Epoch 1] Batch 2500, Loss 0.263965904712677\n",
      "[Training Epoch 1] Batch 5000, Loss 0.2650921940803528\n",
      "[Training Epoch 1] Batch 7500, Loss 0.25388646125793457\n",
      "[Evluating Epoch 1] HR = 0.4932, NDCG = 0.2761\n",
      "Hit Ratio: 0.4932\n",
      "NDCG: 0.2761\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.23600880801677704\n",
      "[Training Epoch 2] Batch 2500, Loss 0.21880246698856354\n",
      "[Training Epoch 2] Batch 5000, Loss 0.2040957510471344\n",
      "[Training Epoch 2] Batch 7500, Loss 0.2090778648853302\n",
      "[Evluating Epoch 2] HR = 0.5462, NDCG = 0.3084\n",
      "Hit Ratio: 0.5462\n",
      "NDCG: 0.3084\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.21550491452217102\n",
      "[Training Epoch 3] Batch 2500, Loss 0.25549793243408203\n",
      "[Training Epoch 3] Batch 5000, Loss 0.220066100358963\n",
      "[Training Epoch 3] Batch 7500, Loss 0.21173331141471863\n",
      "[Evluating Epoch 3] HR = 0.5874, NDCG = 0.3346\n",
      "Hit Ratio: 0.5874\n",
      "NDCG: 0.3346\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.20299339294433594\n",
      "[Training Epoch 4] Batch 2500, Loss 0.20793244242668152\n",
      "[Training Epoch 4] Batch 5000, Loss 0.17367303371429443\n",
      "[Training Epoch 4] Batch 7500, Loss 0.1966719925403595\n",
      "[Evluating Epoch 4] HR = 0.6179, NDCG = 0.3535\n",
      "Hit Ratio: 0.6179\n",
      "NDCG: 0.3535\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.21323241293430328\n",
      "[Training Epoch 5] Batch 2500, Loss 0.18771710991859436\n",
      "[Training Epoch 5] Batch 5000, Loss 0.23009483516216278\n",
      "[Training Epoch 5] Batch 7500, Loss 0.19242972135543823\n",
      "[Evluating Epoch 5] HR = 0.6252, NDCG = 0.3625\n",
      "Hit Ratio: 0.6252\n",
      "NDCG: 0.3625\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.1984052211046219\n",
      "[Training Epoch 6] Batch 2500, Loss 0.18203139305114746\n",
      "[Training Epoch 6] Batch 5000, Loss 0.19198428094387054\n",
      "[Training Epoch 6] Batch 7500, Loss 0.21579110622406006\n",
      "[Evluating Epoch 6] HR = 0.6326, NDCG = 0.3661\n",
      "Hit Ratio: 0.6326\n",
      "NDCG: 0.3661\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.20921170711517334\n",
      "[Training Epoch 7] Batch 2500, Loss 0.2126719057559967\n",
      "[Training Epoch 7] Batch 5000, Loss 0.17350149154663086\n",
      "[Training Epoch 7] Batch 7500, Loss 0.2069636434316635\n",
      "[Evluating Epoch 7] HR = 0.6391, NDCG = 0.3727\n",
      "Hit Ratio: 0.6391\n",
      "NDCG: 0.3727\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.19946233928203583\n",
      "[Training Epoch 8] Batch 2500, Loss 0.18898798525333405\n",
      "[Training Epoch 8] Batch 5000, Loss 0.19867154955863953\n",
      "[Training Epoch 8] Batch 7500, Loss 0.2090018391609192\n",
      "[Evluating Epoch 8] HR = 0.6424, NDCG = 0.3714\n",
      "Hit Ratio: 0.6424\n",
      "NDCG: 0.3714\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.1871957778930664\n",
      "[Training Epoch 9] Batch 2500, Loss 0.18653255701065063\n",
      "[Training Epoch 9] Batch 5000, Loss 0.20816926658153534\n",
      "[Training Epoch 9] Batch 7500, Loss 0.20781634747982025\n",
      "[Evluating Epoch 9] HR = 0.6391, NDCG = 0.3722\n",
      "Hit Ratio: 0.6391\n",
      "NDCG: 0.3722\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.19078963994979858\n",
      "[Training Epoch 10] Batch 2500, Loss 0.1948118507862091\n",
      "[Training Epoch 10] Batch 5000, Loss 0.18821829557418823\n",
      "[Training Epoch 10] Batch 7500, Loss 0.20111441612243652\n",
      "[Evluating Epoch 10] HR = 0.6459, NDCG = 0.3726\n",
      "Hit Ratio: 0.6459\n",
      "NDCG: 0.3726\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.19400277733802795\n",
      "[Training Epoch 11] Batch 2500, Loss 0.2053622454404831\n",
      "[Training Epoch 11] Batch 5000, Loss 0.18790698051452637\n",
      "[Training Epoch 11] Batch 7500, Loss 0.1799459159374237\n",
      "[Evluating Epoch 11] HR = 0.6397, NDCG = 0.3711\n",
      "Hit Ratio: 0.6397\n",
      "NDCG: 0.3711\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.18307405710220337\n",
      "[Training Epoch 12] Batch 2500, Loss 0.20713183283805847\n",
      "[Training Epoch 12] Batch 5000, Loss 0.1860165297985077\n",
      "[Training Epoch 12] Batch 7500, Loss 0.18284529447555542\n",
      "[Evluating Epoch 12] HR = 0.6435, NDCG = 0.3731\n",
      "Hit Ratio: 0.6435\n",
      "NDCG: 0.3731\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.20118089020252228\n",
      "[Training Epoch 13] Batch 2500, Loss 0.19155029952526093\n",
      "[Training Epoch 13] Batch 5000, Loss 0.20205125212669373\n",
      "[Training Epoch 13] Batch 7500, Loss 0.21644951403141022\n",
      "[Evluating Epoch 13] HR = 0.6421, NDCG = 0.3725\n",
      "Hit Ratio: 0.6421\n",
      "NDCG: 0.3725\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.1739596128463745\n",
      "[Training Epoch 14] Batch 2500, Loss 0.16244374215602875\n",
      "[Training Epoch 14] Batch 5000, Loss 0.21314731240272522\n",
      "[Training Epoch 14] Batch 7500, Loss 0.2197853922843933\n",
      "[Evluating Epoch 14] HR = 0.6457, NDCG = 0.3737\n",
      "Hit Ratio: 0.6457\n",
      "NDCG: 0.3737\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.2318621724843979\n",
      "[Training Epoch 15] Batch 2500, Loss 0.21717621386051178\n",
      "[Training Epoch 15] Batch 5000, Loss 0.18902844190597534\n",
      "[Training Epoch 15] Batch 7500, Loss 0.18899869918823242\n",
      "[Evluating Epoch 15] HR = 0.6485, NDCG = 0.3773\n",
      "Hit Ratio: 0.6485\n",
      "NDCG: 0.3773\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.16703930497169495\n",
      "[Training Epoch 16] Batch 2500, Loss 0.20898506045341492\n",
      "[Training Epoch 16] Batch 5000, Loss 0.17822420597076416\n",
      "[Training Epoch 16] Batch 7500, Loss 0.19679033756256104\n",
      "[Evluating Epoch 16] HR = 0.6447, NDCG = 0.3750\n",
      "Hit Ratio: 0.6447\n",
      "NDCG: 0.3750\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.20119890570640564\n",
      "[Training Epoch 17] Batch 2500, Loss 0.17659467458724976\n",
      "[Training Epoch 17] Batch 5000, Loss 0.1959201693534851\n",
      "[Training Epoch 17] Batch 7500, Loss 0.19043049216270447\n",
      "[Evluating Epoch 17] HR = 0.6464, NDCG = 0.3763\n",
      "Hit Ratio: 0.6464\n",
      "NDCG: 0.3763\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.1992415189743042\n",
      "[Training Epoch 18] Batch 2500, Loss 0.19500014185905457\n",
      "[Training Epoch 18] Batch 5000, Loss 0.2203080952167511\n",
      "[Training Epoch 18] Batch 7500, Loss 0.19381652772426605\n",
      "[Evluating Epoch 18] HR = 0.6472, NDCG = 0.3778\n",
      "Hit Ratio: 0.6472\n",
      "NDCG: 0.3778\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.22035038471221924\n",
      "[Training Epoch 19] Batch 2500, Loss 0.1731993556022644\n",
      "[Training Epoch 19] Batch 5000, Loss 0.19459578394889832\n",
      "[Training Epoch 19] Batch 7500, Loss 0.18038246035575867\n",
      "[Evluating Epoch 19] HR = 0.6465, NDCG = 0.3759\n",
      "Hit Ratio: 0.6465\n",
      "NDCG: 0.3759\n",
      "--------------------------------------------------------------------------------\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=8, out_features=1, bias=True)\n",
      "MLP(\n",
      "  (embedding_user): Embedding(6040, 8)\n",
      "  (embedding_item): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.615700364112854\n",
      "[Training Epoch 0] Batch 2500, Loss 0.26408153772354126\n",
      "[Training Epoch 0] Batch 5000, Loss 0.25750914216041565\n",
      "[Training Epoch 0] Batch 7500, Loss 0.33393409848213196\n",
      "[Training Epoch 0] Batch 10000, Loss 0.2489887923002243\n",
      "[Training Epoch 0] Batch 12500, Loss 0.32353508472442627\n",
      "[Training Epoch 0] Batch 15000, Loss 0.22973781824111938\n",
      "[Training Epoch 0] Batch 17500, Loss 0.27076107263565063\n",
      "[Training Epoch 0] Batch 20000, Loss 0.26538556814193726\n",
      "[Training Epoch 0] Batch 22500, Loss 0.25949162244796753\n",
      "[Training Epoch 0] Batch 25000, Loss 0.22494207322597504\n",
      "[Training Epoch 0] Batch 27500, Loss 0.288999080657959\n",
      "[Training Epoch 0] Batch 30000, Loss 0.2692145109176636\n",
      "[Training Epoch 0] Batch 32500, Loss 0.2723464369773865\n",
      "[Evluating Epoch 0] HR = 0.4474, NDCG = 0.2486\n",
      "Hit Ratio: 0.4474\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.31955307722091675\n",
      "[Training Epoch 1] Batch 2500, Loss 0.3614831566810608\n",
      "[Training Epoch 1] Batch 5000, Loss 0.280245840549469\n",
      "[Training Epoch 1] Batch 7500, Loss 0.22044703364372253\n",
      "[Training Epoch 1] Batch 10000, Loss 0.2388167530298233\n",
      "[Training Epoch 1] Batch 12500, Loss 0.2673839330673218\n",
      "[Training Epoch 1] Batch 15000, Loss 0.2659826874732971\n",
      "[Training Epoch 1] Batch 17500, Loss 0.3026050925254822\n",
      "[Training Epoch 1] Batch 20000, Loss 0.25990813970565796\n",
      "[Training Epoch 1] Batch 22500, Loss 0.24154286086559296\n",
      "[Training Epoch 1] Batch 25000, Loss 0.19408224523067474\n",
      "[Training Epoch 1] Batch 27500, Loss 0.22028252482414246\n",
      "[Training Epoch 1] Batch 30000, Loss 0.21350830793380737\n",
      "[Training Epoch 1] Batch 32500, Loss 0.33364003896713257\n",
      "[Evluating Epoch 1] HR = 0.4735, NDCG = 0.2630\n",
      "Hit Ratio: 0.4735\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.23497003316879272\n",
      "[Training Epoch 2] Batch 2500, Loss 0.2952365279197693\n",
      "[Training Epoch 2] Batch 5000, Loss 0.24503636360168457\n",
      "[Training Epoch 2] Batch 7500, Loss 0.22628724575042725\n",
      "[Training Epoch 2] Batch 10000, Loss 0.23152422904968262\n",
      "[Training Epoch 2] Batch 12500, Loss 0.29157090187072754\n",
      "[Training Epoch 2] Batch 15000, Loss 0.28135979175567627\n",
      "[Training Epoch 2] Batch 17500, Loss 0.23434150218963623\n",
      "[Training Epoch 2] Batch 20000, Loss 0.28977757692337036\n",
      "[Training Epoch 2] Batch 22500, Loss 0.21931485831737518\n",
      "[Training Epoch 2] Batch 25000, Loss 0.2367943674325943\n",
      "[Training Epoch 2] Batch 27500, Loss 0.20322886109352112\n",
      "[Training Epoch 2] Batch 30000, Loss 0.225702702999115\n",
      "[Training Epoch 2] Batch 32500, Loss 0.29125404357910156\n",
      "[Evluating Epoch 2] HR = 0.5045, NDCG = 0.2812\n",
      "Hit Ratio: 0.5045\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.18779413402080536\n",
      "[Training Epoch 3] Batch 2500, Loss 0.17954109609127045\n",
      "[Training Epoch 3] Batch 5000, Loss 0.2596111297607422\n",
      "[Training Epoch 3] Batch 7500, Loss 0.19989903271198273\n",
      "[Training Epoch 3] Batch 10000, Loss 0.2286323606967926\n",
      "[Training Epoch 3] Batch 12500, Loss 0.25671452283859253\n",
      "[Training Epoch 3] Batch 15000, Loss 0.28091156482696533\n",
      "[Training Epoch 3] Batch 17500, Loss 0.29060977697372437\n",
      "[Training Epoch 3] Batch 20000, Loss 0.19543161988258362\n",
      "[Training Epoch 3] Batch 22500, Loss 0.28478312492370605\n",
      "[Training Epoch 3] Batch 25000, Loss 0.18836191296577454\n",
      "[Training Epoch 3] Batch 27500, Loss 0.2398446947336197\n",
      "[Training Epoch 3] Batch 30000, Loss 0.2178971767425537\n",
      "[Training Epoch 3] Batch 32500, Loss 0.26640376448631287\n",
      "[Evluating Epoch 3] HR = 0.5377, NDCG = 0.3022\n",
      "Hit Ratio: 0.5377\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.24662548303604126\n",
      "[Training Epoch 4] Batch 2500, Loss 0.25064265727996826\n",
      "[Training Epoch 4] Batch 5000, Loss 0.18102476000785828\n",
      "[Training Epoch 4] Batch 7500, Loss 0.2741677761077881\n",
      "[Training Epoch 4] Batch 10000, Loss 0.25607335567474365\n",
      "[Training Epoch 4] Batch 12500, Loss 0.23681652545928955\n",
      "[Training Epoch 4] Batch 15000, Loss 0.2122926115989685\n",
      "[Training Epoch 4] Batch 17500, Loss 0.2395644336938858\n",
      "[Training Epoch 4] Batch 20000, Loss 0.20660105347633362\n",
      "[Training Epoch 4] Batch 22500, Loss 0.21709570288658142\n",
      "[Training Epoch 4] Batch 25000, Loss 0.22507596015930176\n",
      "[Training Epoch 4] Batch 27500, Loss 0.28991007804870605\n",
      "[Training Epoch 4] Batch 30000, Loss 0.15352219343185425\n",
      "[Training Epoch 4] Batch 32500, Loss 0.26123425364494324\n",
      "[Evluating Epoch 4] HR = 0.5472, NDCG = 0.3065\n",
      "Hit Ratio: 0.5472\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.24195191264152527\n",
      "[Training Epoch 5] Batch 2500, Loss 0.23640504479408264\n",
      "[Training Epoch 5] Batch 5000, Loss 0.232611745595932\n",
      "[Training Epoch 5] Batch 7500, Loss 0.20752820372581482\n",
      "[Training Epoch 5] Batch 10000, Loss 0.2279684990644455\n",
      "[Training Epoch 5] Batch 12500, Loss 0.24962130188941956\n",
      "[Training Epoch 5] Batch 15000, Loss 0.19952470064163208\n",
      "[Training Epoch 5] Batch 17500, Loss 0.18868108093738556\n",
      "[Training Epoch 5] Batch 20000, Loss 0.21435876190662384\n",
      "[Training Epoch 5] Batch 22500, Loss 0.21808265149593353\n",
      "[Training Epoch 5] Batch 25000, Loss 0.18403245508670807\n",
      "[Training Epoch 5] Batch 27500, Loss 0.23645517230033875\n",
      "[Training Epoch 5] Batch 30000, Loss 0.19068104028701782\n",
      "[Training Epoch 5] Batch 32500, Loss 0.2564508616924286\n",
      "[Evluating Epoch 5] HR = 0.5591, NDCG = 0.3147\n",
      "Hit Ratio: 0.5591\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.23616132140159607\n",
      "[Training Epoch 6] Batch 2500, Loss 0.2769312262535095\n",
      "[Training Epoch 6] Batch 5000, Loss 0.2757243812084198\n",
      "[Training Epoch 6] Batch 7500, Loss 0.30110472440719604\n",
      "[Training Epoch 6] Batch 10000, Loss 0.2619127333164215\n",
      "[Training Epoch 6] Batch 12500, Loss 0.2455165684223175\n",
      "[Training Epoch 6] Batch 15000, Loss 0.2183360457420349\n",
      "[Training Epoch 6] Batch 17500, Loss 0.24136877059936523\n",
      "[Training Epoch 6] Batch 20000, Loss 0.25848063826560974\n",
      "[Training Epoch 6] Batch 22500, Loss 0.19392648339271545\n",
      "[Training Epoch 6] Batch 25000, Loss 0.2834131717681885\n",
      "[Training Epoch 6] Batch 27500, Loss 0.19019873440265656\n",
      "[Training Epoch 6] Batch 30000, Loss 0.23987385630607605\n",
      "[Training Epoch 6] Batch 32500, Loss 0.23033149540424347\n",
      "[Evluating Epoch 6] HR = 0.5629, NDCG = 0.3185\n",
      "Hit Ratio: 0.5629\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.24010266363620758\n",
      "[Training Epoch 7] Batch 2500, Loss 0.21988222002983093\n",
      "[Training Epoch 7] Batch 5000, Loss 0.2552085220813751\n",
      "[Training Epoch 7] Batch 7500, Loss 0.17336289584636688\n",
      "[Training Epoch 7] Batch 10000, Loss 0.20706886053085327\n",
      "[Training Epoch 7] Batch 12500, Loss 0.2244507074356079\n",
      "[Training Epoch 7] Batch 15000, Loss 0.19577902555465698\n",
      "[Training Epoch 7] Batch 17500, Loss 0.24320875108242035\n",
      "[Training Epoch 7] Batch 20000, Loss 0.2005302906036377\n",
      "[Training Epoch 7] Batch 22500, Loss 0.24392274022102356\n",
      "[Training Epoch 7] Batch 25000, Loss 0.234477698802948\n",
      "[Training Epoch 7] Batch 27500, Loss 0.1902315616607666\n",
      "[Training Epoch 7] Batch 30000, Loss 0.224208801984787\n",
      "[Training Epoch 7] Batch 32500, Loss 0.15269386768341064\n",
      "[Evluating Epoch 7] HR = 0.5657, NDCG = 0.3219\n",
      "Hit Ratio: 0.5657\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.1595492660999298\n",
      "[Training Epoch 8] Batch 2500, Loss 0.21660391986370087\n",
      "[Training Epoch 8] Batch 5000, Loss 0.2517215609550476\n",
      "[Training Epoch 8] Batch 7500, Loss 0.2290593683719635\n",
      "[Training Epoch 8] Batch 10000, Loss 0.2113281488418579\n",
      "[Training Epoch 8] Batch 12500, Loss 0.23283031582832336\n",
      "[Training Epoch 8] Batch 15000, Loss 0.20515461266040802\n",
      "[Training Epoch 8] Batch 17500, Loss 0.1734728366136551\n",
      "[Training Epoch 8] Batch 20000, Loss 0.2131989598274231\n",
      "[Training Epoch 8] Batch 22500, Loss 0.1719418317079544\n",
      "[Training Epoch 8] Batch 25000, Loss 0.26635998487472534\n",
      "[Training Epoch 8] Batch 27500, Loss 0.23277753591537476\n",
      "[Training Epoch 8] Batch 30000, Loss 0.19871753454208374\n",
      "[Training Epoch 8] Batch 32500, Loss 0.2376408874988556\n",
      "[Evluating Epoch 8] HR = 0.5616, NDCG = 0.3201\n",
      "Hit Ratio: 0.5616\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.2252008467912674\n",
      "[Training Epoch 9] Batch 2500, Loss 0.20736944675445557\n",
      "[Training Epoch 9] Batch 5000, Loss 0.22583919763565063\n",
      "[Training Epoch 9] Batch 7500, Loss 0.22665518522262573\n",
      "[Training Epoch 9] Batch 10000, Loss 0.21013101935386658\n",
      "[Training Epoch 9] Batch 12500, Loss 0.2156243920326233\n",
      "[Training Epoch 9] Batch 15000, Loss 0.23853419721126556\n",
      "[Training Epoch 9] Batch 17500, Loss 0.2727815806865692\n",
      "[Training Epoch 9] Batch 20000, Loss 0.23402057588100433\n",
      "[Training Epoch 9] Batch 22500, Loss 0.18652907013893127\n",
      "[Training Epoch 9] Batch 25000, Loss 0.23288913071155548\n",
      "[Training Epoch 9] Batch 27500, Loss 0.25439393520355225\n",
      "[Training Epoch 9] Batch 30000, Loss 0.28631502389907837\n",
      "[Training Epoch 9] Batch 32500, Loss 0.22978675365447998\n",
      "[Evluating Epoch 9] HR = 0.5707, NDCG = 0.3252\n",
      "Hit Ratio: 0.5707\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.1876048743724823\n",
      "[Training Epoch 10] Batch 2500, Loss 0.18957623839378357\n",
      "[Training Epoch 10] Batch 5000, Loss 0.2004002332687378\n",
      "[Training Epoch 10] Batch 7500, Loss 0.28843075037002563\n",
      "[Training Epoch 10] Batch 10000, Loss 0.16554254293441772\n",
      "[Training Epoch 10] Batch 12500, Loss 0.21754863858222961\n",
      "[Training Epoch 10] Batch 15000, Loss 0.16831904649734497\n",
      "[Training Epoch 10] Batch 17500, Loss 0.20631784200668335\n",
      "[Training Epoch 10] Batch 20000, Loss 0.2637206017971039\n",
      "[Training Epoch 10] Batch 22500, Loss 0.1706964671611786\n",
      "[Training Epoch 10] Batch 25000, Loss 0.22395870089530945\n",
      "[Training Epoch 10] Batch 27500, Loss 0.24057474732398987\n",
      "[Training Epoch 10] Batch 30000, Loss 0.2483799159526825\n",
      "[Training Epoch 10] Batch 32500, Loss 0.2247546911239624\n",
      "[Evluating Epoch 10] HR = 0.5722, NDCG = 0.3277\n",
      "Hit Ratio: 0.5722\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.15691405534744263\n",
      "[Training Epoch 11] Batch 2500, Loss 0.21795496344566345\n",
      "[Training Epoch 11] Batch 5000, Loss 0.24375012516975403\n",
      "[Training Epoch 11] Batch 7500, Loss 0.22049017250537872\n",
      "[Training Epoch 11] Batch 10000, Loss 0.25759759545326233\n",
      "[Training Epoch 11] Batch 12500, Loss 0.22209066152572632\n",
      "[Training Epoch 11] Batch 15000, Loss 0.20178437232971191\n",
      "[Training Epoch 11] Batch 17500, Loss 0.13133767247200012\n",
      "[Training Epoch 11] Batch 20000, Loss 0.21643905341625214\n",
      "[Training Epoch 11] Batch 22500, Loss 0.17448702454566956\n",
      "[Training Epoch 11] Batch 25000, Loss 0.23798145353794098\n",
      "[Training Epoch 11] Batch 27500, Loss 0.2125771939754486\n",
      "[Training Epoch 11] Batch 30000, Loss 0.2340741902589798\n",
      "[Training Epoch 11] Batch 32500, Loss 0.2308335304260254\n",
      "[Evluating Epoch 11] HR = 0.5781, NDCG = 0.3310\n",
      "Hit Ratio: 0.5781\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.20084547996520996\n",
      "[Training Epoch 12] Batch 2500, Loss 0.1700114905834198\n",
      "[Training Epoch 12] Batch 5000, Loss 0.18704771995544434\n",
      "[Training Epoch 12] Batch 7500, Loss 0.2339588701725006\n",
      "[Training Epoch 12] Batch 10000, Loss 0.18736663460731506\n",
      "[Training Epoch 12] Batch 12500, Loss 0.1906939446926117\n",
      "[Training Epoch 12] Batch 15000, Loss 0.20292645692825317\n",
      "[Training Epoch 12] Batch 17500, Loss 0.18410611152648926\n",
      "[Training Epoch 12] Batch 20000, Loss 0.3331940770149231\n",
      "[Training Epoch 12] Batch 22500, Loss 0.21749348938465118\n",
      "[Training Epoch 12] Batch 25000, Loss 0.20176416635513306\n",
      "[Training Epoch 12] Batch 27500, Loss 0.25288307666778564\n",
      "[Training Epoch 12] Batch 30000, Loss 0.18206940591335297\n",
      "[Training Epoch 12] Batch 32500, Loss 0.2239513099193573\n",
      "[Evluating Epoch 12] HR = 0.5826, NDCG = 0.3321\n",
      "Hit Ratio: 0.5826\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.24592837691307068\n",
      "[Training Epoch 13] Batch 2500, Loss 0.22413784265518188\n",
      "[Training Epoch 13] Batch 5000, Loss 0.19926443696022034\n",
      "[Training Epoch 13] Batch 7500, Loss 0.18655326962471008\n",
      "[Training Epoch 13] Batch 10000, Loss 0.2522607445716858\n",
      "[Training Epoch 13] Batch 12500, Loss 0.1775789111852646\n",
      "[Training Epoch 13] Batch 15000, Loss 0.251495361328125\n",
      "[Training Epoch 13] Batch 17500, Loss 0.28416508436203003\n",
      "[Training Epoch 13] Batch 20000, Loss 0.2381380945444107\n",
      "[Training Epoch 13] Batch 22500, Loss 0.199168860912323\n",
      "[Training Epoch 13] Batch 25000, Loss 0.2112387716770172\n",
      "[Training Epoch 13] Batch 27500, Loss 0.22586438059806824\n",
      "[Training Epoch 13] Batch 30000, Loss 0.23483631014823914\n",
      "[Training Epoch 13] Batch 32500, Loss 0.2006424367427826\n",
      "[Evluating Epoch 13] HR = 0.5881, NDCG = 0.3367\n",
      "Hit Ratio: 0.5881\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.1778014451265335\n",
      "[Training Epoch 14] Batch 2500, Loss 0.21163880825042725\n",
      "[Training Epoch 14] Batch 5000, Loss 0.23160654306411743\n",
      "[Training Epoch 14] Batch 7500, Loss 0.23407141864299774\n",
      "[Training Epoch 14] Batch 10000, Loss 0.28161460161209106\n",
      "[Training Epoch 14] Batch 12500, Loss 0.23657917976379395\n",
      "[Training Epoch 14] Batch 15000, Loss 0.19423335790634155\n",
      "[Training Epoch 14] Batch 17500, Loss 0.18682163953781128\n",
      "[Training Epoch 14] Batch 20000, Loss 0.17694292962551117\n",
      "[Training Epoch 14] Batch 22500, Loss 0.20986030995845795\n",
      "[Training Epoch 14] Batch 25000, Loss 0.17896604537963867\n",
      "[Training Epoch 14] Batch 27500, Loss 0.180461585521698\n",
      "[Training Epoch 14] Batch 30000, Loss 0.23619762063026428\n",
      "[Training Epoch 14] Batch 32500, Loss 0.1575898826122284\n",
      "[Evluating Epoch 14] HR = 0.5868, NDCG = 0.3374\n",
      "Hit Ratio: 0.5868\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.2007938176393509\n",
      "[Training Epoch 15] Batch 2500, Loss 0.20074495673179626\n",
      "[Training Epoch 15] Batch 5000, Loss 0.22765004634857178\n",
      "[Training Epoch 15] Batch 7500, Loss 0.16710437834262848\n",
      "[Training Epoch 15] Batch 10000, Loss 0.2779398560523987\n",
      "[Training Epoch 15] Batch 12500, Loss 0.27922576665878296\n",
      "[Training Epoch 15] Batch 15000, Loss 0.27738866209983826\n",
      "[Training Epoch 15] Batch 17500, Loss 0.16740939021110535\n",
      "[Training Epoch 15] Batch 20000, Loss 0.19870352745056152\n",
      "[Training Epoch 15] Batch 22500, Loss 0.20493286848068237\n",
      "[Training Epoch 15] Batch 25000, Loss 0.19907262921333313\n",
      "[Training Epoch 15] Batch 27500, Loss 0.22505074739456177\n",
      "[Training Epoch 15] Batch 30000, Loss 0.2653637230396271\n",
      "[Training Epoch 15] Batch 32500, Loss 0.23180562257766724\n",
      "[Evluating Epoch 15] HR = 0.5899, NDCG = 0.3392\n",
      "Hit Ratio: 0.5899\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.216923326253891\n",
      "[Training Epoch 16] Batch 2500, Loss 0.19018715620040894\n",
      "[Training Epoch 16] Batch 5000, Loss 0.18632182478904724\n",
      "[Training Epoch 16] Batch 7500, Loss 0.16678626835346222\n",
      "[Training Epoch 16] Batch 10000, Loss 0.26039355993270874\n",
      "[Training Epoch 16] Batch 12500, Loss 0.15168198943138123\n",
      "[Training Epoch 16] Batch 15000, Loss 0.22137972712516785\n",
      "[Training Epoch 16] Batch 17500, Loss 0.25039705634117126\n",
      "[Training Epoch 16] Batch 20000, Loss 0.2773335576057434\n",
      "[Training Epoch 16] Batch 22500, Loss 0.17944547533988953\n",
      "[Training Epoch 16] Batch 25000, Loss 0.2017802894115448\n",
      "[Training Epoch 16] Batch 27500, Loss 0.18089231848716736\n",
      "[Training Epoch 16] Batch 30000, Loss 0.19240081310272217\n",
      "[Training Epoch 16] Batch 32500, Loss 0.21486644446849823\n",
      "[Evluating Epoch 16] HR = 0.5889, NDCG = 0.3379\n",
      "Hit Ratio: 0.5889\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.18292447924613953\n",
      "[Training Epoch 17] Batch 2500, Loss 0.2010689377784729\n",
      "[Training Epoch 17] Batch 5000, Loss 0.19861502945423126\n",
      "[Training Epoch 17] Batch 7500, Loss 0.22907117009162903\n",
      "[Training Epoch 17] Batch 10000, Loss 0.2604045867919922\n",
      "[Training Epoch 17] Batch 12500, Loss 0.26430660486221313\n",
      "[Training Epoch 17] Batch 15000, Loss 0.2583122253417969\n",
      "[Training Epoch 17] Batch 17500, Loss 0.2546164095401764\n",
      "[Training Epoch 17] Batch 20000, Loss 0.1371610015630722\n",
      "[Training Epoch 17] Batch 22500, Loss 0.16523300111293793\n",
      "[Training Epoch 17] Batch 25000, Loss 0.18132822215557098\n",
      "[Training Epoch 17] Batch 27500, Loss 0.17554396390914917\n",
      "[Training Epoch 17] Batch 30000, Loss 0.22060565650463104\n",
      "[Training Epoch 17] Batch 32500, Loss 0.20577864348888397\n",
      "[Evluating Epoch 17] HR = 0.5881, NDCG = 0.3388\n",
      "Hit Ratio: 0.5881\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.28818613290786743\n",
      "[Training Epoch 18] Batch 2500, Loss 0.16271084547042847\n",
      "[Training Epoch 18] Batch 5000, Loss 0.32878684997558594\n",
      "[Training Epoch 18] Batch 7500, Loss 0.14652620255947113\n",
      "[Training Epoch 18] Batch 10000, Loss 0.24177642166614532\n",
      "[Training Epoch 18] Batch 12500, Loss 0.18515166640281677\n",
      "[Training Epoch 18] Batch 15000, Loss 0.22486895322799683\n",
      "[Training Epoch 18] Batch 17500, Loss 0.2422289401292801\n",
      "[Training Epoch 18] Batch 20000, Loss 0.26847127079963684\n",
      "[Training Epoch 18] Batch 22500, Loss 0.30923187732696533\n",
      "[Training Epoch 18] Batch 25000, Loss 0.2394370436668396\n",
      "[Training Epoch 18] Batch 27500, Loss 0.21563240885734558\n",
      "[Training Epoch 18] Batch 30000, Loss 0.17923812568187714\n",
      "[Training Epoch 18] Batch 32500, Loss 0.15721619129180908\n",
      "[Evluating Epoch 18] HR = 0.5909, NDCG = 0.3389\n",
      "Hit Ratio: 0.5909\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.2520500421524048\n",
      "[Training Epoch 19] Batch 2500, Loss 0.24495261907577515\n",
      "[Training Epoch 19] Batch 5000, Loss 0.27248287200927734\n",
      "[Training Epoch 19] Batch 7500, Loss 0.22026902437210083\n",
      "[Training Epoch 19] Batch 10000, Loss 0.16710509359836578\n",
      "[Training Epoch 19] Batch 12500, Loss 0.2207181602716446\n",
      "[Training Epoch 19] Batch 15000, Loss 0.25426459312438965\n",
      "[Training Epoch 19] Batch 17500, Loss 0.20818167924880981\n",
      "[Training Epoch 19] Batch 20000, Loss 0.25132015347480774\n",
      "[Training Epoch 19] Batch 22500, Loss 0.18594956398010254\n",
      "[Training Epoch 19] Batch 25000, Loss 0.20550379157066345\n",
      "[Training Epoch 19] Batch 27500, Loss 0.1881762593984604\n",
      "[Training Epoch 19] Batch 30000, Loss 0.1805845946073532\n",
      "[Training Epoch 19] Batch 32500, Loss 0.23459607362747192\n",
      "[Evluating Epoch 19] HR = 0.5942, NDCG = 0.3410\n",
      "Hit Ratio: 0.5942\n",
      "--------------------------------------------------------------------------------\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=16, bias=True)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=16, out_features=1, bias=True)\n",
      "NeuMF(\n",
      "  (embedding_user_mlp): Embedding(6040, 8)\n",
      "  (embedding_item_mlp): Embedding(3706, 8)\n",
      "  (embedding_user_mf): Embedding(6040, 8)\n",
      "  (embedding_item_mf): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.7380759716033936\n",
      "[Training Epoch 0] Batch 2500, Loss 0.2555084824562073\n",
      "[Training Epoch 0] Batch 5000, Loss 0.22285380959510803\n",
      "[Training Epoch 0] Batch 7500, Loss 0.2206418216228485\n",
      "[Evluating Epoch 0] HR = 0.5618, NDCG = 0.3161\n",
      "Hit Ratio: 0.5618\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.23404020071029663\n",
      "[Training Epoch 1] Batch 2500, Loss 0.23275113105773926\n",
      "[Training Epoch 1] Batch 5000, Loss 0.23158049583435059\n",
      "[Training Epoch 1] Batch 7500, Loss 0.21927708387374878\n",
      "[Evluating Epoch 1] HR = 0.6010, NDCG = 0.3379\n",
      "Hit Ratio: 0.6010\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.22053761780261993\n",
      "[Training Epoch 2] Batch 2500, Loss 0.2076919972896576\n",
      "[Training Epoch 2] Batch 5000, Loss 0.19605156779289246\n",
      "[Training Epoch 2] Batch 7500, Loss 0.18505319952964783\n",
      "[Evluating Epoch 2] HR = 0.6056, NDCG = 0.3460\n",
      "Hit Ratio: 0.6056\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.19293789565563202\n",
      "[Training Epoch 3] Batch 2500, Loss 0.19887009263038635\n",
      "[Training Epoch 3] Batch 5000, Loss 0.19759678840637207\n",
      "[Training Epoch 3] Batch 7500, Loss 0.22830983996391296\n",
      "[Evluating Epoch 3] HR = 0.6123, NDCG = 0.3509\n",
      "Hit Ratio: 0.6123\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.1803516298532486\n",
      "[Training Epoch 4] Batch 2500, Loss 0.19391554594039917\n",
      "[Training Epoch 4] Batch 5000, Loss 0.18626978993415833\n",
      "[Training Epoch 4] Batch 7500, Loss 0.2150496542453766\n",
      "[Evluating Epoch 4] HR = 0.6199, NDCG = 0.3572\n",
      "Hit Ratio: 0.6199\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.21148920059204102\n",
      "[Training Epoch 5] Batch 2500, Loss 0.17117246985435486\n",
      "[Training Epoch 5] Batch 5000, Loss 0.2148546278476715\n",
      "[Training Epoch 5] Batch 7500, Loss 0.19253498315811157\n",
      "[Evluating Epoch 5] HR = 0.6276, NDCG = 0.3615\n",
      "Hit Ratio: 0.6276\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.22212187945842743\n",
      "[Training Epoch 6] Batch 2500, Loss 0.2089153230190277\n",
      "[Training Epoch 6] Batch 5000, Loss 0.22180582582950592\n",
      "[Training Epoch 6] Batch 7500, Loss 0.20921817421913147\n",
      "[Evluating Epoch 6] HR = 0.6373, NDCG = 0.3705\n",
      "Hit Ratio: 0.6373\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.17503246665000916\n",
      "[Training Epoch 7] Batch 2500, Loss 0.204043909907341\n",
      "[Training Epoch 7] Batch 5000, Loss 0.1711891144514084\n",
      "[Training Epoch 7] Batch 7500, Loss 0.1936485469341278\n",
      "[Evluating Epoch 7] HR = 0.6497, NDCG = 0.3779\n",
      "Hit Ratio: 0.6497\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.20733517408370972\n",
      "[Training Epoch 8] Batch 2500, Loss 0.2106204479932785\n",
      "[Training Epoch 8] Batch 5000, Loss 0.19771867990493774\n",
      "[Training Epoch 8] Batch 7500, Loss 0.18811477720737457\n",
      "[Evluating Epoch 8] HR = 0.6561, NDCG = 0.3844\n",
      "Hit Ratio: 0.6561\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.17033255100250244\n",
      "[Training Epoch 9] Batch 2500, Loss 0.21799075603485107\n",
      "[Training Epoch 9] Batch 5000, Loss 0.19240477681159973\n",
      "[Training Epoch 9] Batch 7500, Loss 0.20320653915405273\n",
      "[Evluating Epoch 9] HR = 0.6581, NDCG = 0.3861\n",
      "Hit Ratio: 0.6581\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.18758749961853027\n",
      "[Training Epoch 10] Batch 2500, Loss 0.18057462573051453\n",
      "[Training Epoch 10] Batch 5000, Loss 0.22128167748451233\n",
      "[Training Epoch 10] Batch 7500, Loss 0.19261720776557922\n",
      "[Evluating Epoch 10] HR = 0.6621, NDCG = 0.3881\n",
      "Hit Ratio: 0.6621\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.18204659223556519\n",
      "[Training Epoch 11] Batch 2500, Loss 0.20583832263946533\n",
      "[Training Epoch 11] Batch 5000, Loss 0.17045320570468903\n",
      "[Training Epoch 11] Batch 7500, Loss 0.19011835753917694\n",
      "[Evluating Epoch 11] HR = 0.6621, NDCG = 0.3912\n",
      "Hit Ratio: 0.6621\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.18002168834209442\n",
      "[Training Epoch 12] Batch 2500, Loss 0.18573014438152313\n",
      "[Training Epoch 12] Batch 5000, Loss 0.1665438711643219\n",
      "[Training Epoch 12] Batch 7500, Loss 0.19531574845314026\n",
      "[Evluating Epoch 12] HR = 0.6646, NDCG = 0.3931\n",
      "Hit Ratio: 0.6646\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.17244952917099\n",
      "[Training Epoch 13] Batch 2500, Loss 0.19147247076034546\n",
      "[Training Epoch 13] Batch 5000, Loss 0.19334694743156433\n",
      "[Training Epoch 13] Batch 7500, Loss 0.18129602074623108\n",
      "[Evluating Epoch 13] HR = 0.6679, NDCG = 0.3926\n",
      "Hit Ratio: 0.6679\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.17735517024993896\n",
      "[Training Epoch 14] Batch 2500, Loss 0.16757424175739288\n",
      "[Training Epoch 14] Batch 5000, Loss 0.1831512451171875\n",
      "[Training Epoch 14] Batch 7500, Loss 0.21770569682121277\n",
      "[Evluating Epoch 14] HR = 0.6719, NDCG = 0.3969\n",
      "Hit Ratio: 0.6719\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.19526124000549316\n",
      "[Training Epoch 15] Batch 2500, Loss 0.17694643139839172\n",
      "[Training Epoch 15] Batch 5000, Loss 0.18663090467453003\n",
      "[Training Epoch 15] Batch 7500, Loss 0.18951430916786194\n",
      "[Evluating Epoch 15] HR = 0.6674, NDCG = 0.3949\n",
      "Hit Ratio: 0.6674\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.1997442990541458\n",
      "[Training Epoch 16] Batch 2500, Loss 0.19193026423454285\n",
      "[Training Epoch 16] Batch 5000, Loss 0.1866765320301056\n",
      "[Training Epoch 16] Batch 7500, Loss 0.1621907651424408\n",
      "[Evluating Epoch 16] HR = 0.6735, NDCG = 0.4014\n",
      "Hit Ratio: 0.6735\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.19665980339050293\n",
      "[Training Epoch 17] Batch 2500, Loss 0.18590831756591797\n",
      "[Training Epoch 17] Batch 5000, Loss 0.19773855805397034\n",
      "[Training Epoch 17] Batch 7500, Loss 0.20188061892986298\n",
      "[Evluating Epoch 17] HR = 0.6740, NDCG = 0.4015\n",
      "Hit Ratio: 0.6740\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.19618596136569977\n",
      "[Training Epoch 18] Batch 2500, Loss 0.168267160654068\n",
      "[Training Epoch 18] Batch 5000, Loss 0.18602395057678223\n",
      "[Training Epoch 18] Batch 7500, Loss 0.19263732433319092\n",
      "[Evluating Epoch 18] HR = 0.6767, NDCG = 0.4033\n",
      "Hit Ratio: 0.6767\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.18415459990501404\n",
      "[Training Epoch 19] Batch 2500, Loss 0.17846611142158508\n",
      "[Training Epoch 19] Batch 5000, Loss 0.19717463850975037\n",
      "[Training Epoch 19] Batch 7500, Loss 0.20698188245296478\n",
      "[Evluating Epoch 19] HR = 0.6735, NDCG = 0.4023\n",
      "Hit Ratio: 0.6735\n",
      "--------------------------------------------------------------------------------\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "number of negatives: 9\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=8, out_features=1, bias=True)\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.6510772705078125\n",
      "[Training Epoch 0] Batch 2500, Loss 0.24804149568080902\n",
      "[Training Epoch 0] Batch 5000, Loss 0.22607707977294922\n",
      "[Training Epoch 0] Batch 7500, Loss 0.27755478024482727\n",
      "[Evluating Epoch 0] HR = 0.4664, NDCG = 0.2584\n",
      "Hit Ratio: 0.4664\n",
      "NDCG: 0.2584\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.24888600409030914\n",
      "[Training Epoch 1] Batch 2500, Loss 0.21492311358451843\n",
      "[Training Epoch 1] Batch 5000, Loss 0.2131618708372116\n",
      "[Training Epoch 1] Batch 7500, Loss 0.20973746478557587\n",
      "[Evluating Epoch 1] HR = 0.5084, NDCG = 0.2828\n",
      "Hit Ratio: 0.5084\n",
      "NDCG: 0.2828\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.25995078682899475\n",
      "[Training Epoch 2] Batch 2500, Loss 0.20563086867332458\n",
      "[Training Epoch 2] Batch 5000, Loss 0.22278805077075958\n",
      "[Training Epoch 2] Batch 7500, Loss 0.20785163342952728\n",
      "[Evluating Epoch 2] HR = 0.5677, NDCG = 0.3161\n",
      "Hit Ratio: 0.5677\n",
      "NDCG: 0.3161\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.2009499967098236\n",
      "[Training Epoch 3] Batch 2500, Loss 0.18275386095046997\n",
      "[Training Epoch 3] Batch 5000, Loss 0.19143055379390717\n",
      "[Training Epoch 3] Batch 7500, Loss 0.20585396885871887\n",
      "[Evluating Epoch 3] HR = 0.5826, NDCG = 0.3264\n",
      "Hit Ratio: 0.5826\n",
      "NDCG: 0.3264\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.19452112913131714\n",
      "[Training Epoch 4] Batch 2500, Loss 0.20791813731193542\n",
      "[Training Epoch 4] Batch 5000, Loss 0.19907858967781067\n",
      "[Training Epoch 4] Batch 7500, Loss 0.1778416633605957\n",
      "[Evluating Epoch 4] HR = 0.5912, NDCG = 0.3329\n",
      "Hit Ratio: 0.5912\n",
      "NDCG: 0.3329\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.18422487378120422\n",
      "[Training Epoch 5] Batch 2500, Loss 0.18936125934123993\n",
      "[Training Epoch 5] Batch 5000, Loss 0.16283103823661804\n",
      "[Training Epoch 5] Batch 7500, Loss 0.21898040175437927\n",
      "[Evluating Epoch 5] HR = 0.6119, NDCG = 0.3470\n",
      "Hit Ratio: 0.6119\n",
      "NDCG: 0.3470\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.18224596977233887\n",
      "[Training Epoch 6] Batch 2500, Loss 0.18792983889579773\n",
      "[Training Epoch 6] Batch 5000, Loss 0.1813092827796936\n",
      "[Training Epoch 6] Batch 7500, Loss 0.20599882304668427\n",
      "[Evluating Epoch 6] HR = 0.6258, NDCG = 0.3591\n",
      "Hit Ratio: 0.6258\n",
      "NDCG: 0.3591\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.20312806963920593\n",
      "[Training Epoch 7] Batch 2500, Loss 0.16172747313976288\n",
      "[Training Epoch 7] Batch 5000, Loss 0.19925060868263245\n",
      "[Training Epoch 7] Batch 7500, Loss 0.15058207511901855\n",
      "[Evluating Epoch 7] HR = 0.6290, NDCG = 0.3612\n",
      "Hit Ratio: 0.6290\n",
      "NDCG: 0.3612\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.1894291341304779\n",
      "[Training Epoch 8] Batch 2500, Loss 0.18577539920806885\n",
      "[Training Epoch 8] Batch 5000, Loss 0.18157356977462769\n",
      "[Training Epoch 8] Batch 7500, Loss 0.21126896142959595\n",
      "[Evluating Epoch 8] HR = 0.6364, NDCG = 0.3664\n",
      "Hit Ratio: 0.6364\n",
      "NDCG: 0.3664\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.1810283660888672\n",
      "[Training Epoch 9] Batch 2500, Loss 0.1673932671546936\n",
      "[Training Epoch 9] Batch 5000, Loss 0.15853548049926758\n",
      "[Training Epoch 9] Batch 7500, Loss 0.1733197271823883\n",
      "[Evluating Epoch 9] HR = 0.6359, NDCG = 0.3647\n",
      "Hit Ratio: 0.6359\n",
      "NDCG: 0.3647\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.19349505007266998\n",
      "[Training Epoch 10] Batch 2500, Loss 0.1621406227350235\n",
      "[Training Epoch 10] Batch 5000, Loss 0.1747281551361084\n",
      "[Training Epoch 10] Batch 7500, Loss 0.20505017042160034\n",
      "[Evluating Epoch 10] HR = 0.6354, NDCG = 0.3679\n",
      "Hit Ratio: 0.6354\n",
      "NDCG: 0.3679\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.17977099120616913\n",
      "[Training Epoch 11] Batch 2500, Loss 0.20657289028167725\n",
      "[Training Epoch 11] Batch 5000, Loss 0.18229496479034424\n",
      "[Training Epoch 11] Batch 7500, Loss 0.183049276471138\n",
      "[Evluating Epoch 11] HR = 0.6430, NDCG = 0.3707\n",
      "Hit Ratio: 0.6430\n",
      "NDCG: 0.3707\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.19907306134700775\n",
      "[Training Epoch 12] Batch 2500, Loss 0.18212632834911346\n",
      "[Training Epoch 12] Batch 5000, Loss 0.2366812825202942\n",
      "[Training Epoch 12] Batch 7500, Loss 0.20231893658638\n",
      "[Evluating Epoch 12] HR = 0.6397, NDCG = 0.3707\n",
      "Hit Ratio: 0.6397\n",
      "NDCG: 0.3707\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.19705671072006226\n",
      "[Training Epoch 13] Batch 2500, Loss 0.2073286771774292\n",
      "[Training Epoch 13] Batch 5000, Loss 0.18915043771266937\n",
      "[Training Epoch 13] Batch 7500, Loss 0.20698131620883942\n",
      "[Evluating Epoch 13] HR = 0.6417, NDCG = 0.3689\n",
      "Hit Ratio: 0.6417\n",
      "NDCG: 0.3689\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.19870342314243317\n",
      "[Training Epoch 14] Batch 2500, Loss 0.20725730061531067\n",
      "[Training Epoch 14] Batch 5000, Loss 0.18645498156547546\n",
      "[Training Epoch 14] Batch 7500, Loss 0.19118478894233704\n",
      "[Evluating Epoch 14] HR = 0.6422, NDCG = 0.3720\n",
      "Hit Ratio: 0.6422\n",
      "NDCG: 0.3720\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.16651412844657898\n",
      "[Training Epoch 15] Batch 2500, Loss 0.18106305599212646\n",
      "[Training Epoch 15] Batch 5000, Loss 0.19517387449741364\n",
      "[Training Epoch 15] Batch 7500, Loss 0.18625450134277344\n",
      "[Evluating Epoch 15] HR = 0.6412, NDCG = 0.3722\n",
      "Hit Ratio: 0.6412\n",
      "NDCG: 0.3722\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.17281730473041534\n",
      "[Training Epoch 16] Batch 2500, Loss 0.1710066795349121\n",
      "[Training Epoch 16] Batch 5000, Loss 0.1748107522726059\n",
      "[Training Epoch 16] Batch 7500, Loss 0.19631409645080566\n",
      "[Evluating Epoch 16] HR = 0.6411, NDCG = 0.3736\n",
      "Hit Ratio: 0.6411\n",
      "NDCG: 0.3736\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.17212629318237305\n",
      "[Training Epoch 17] Batch 2500, Loss 0.18926912546157837\n",
      "[Training Epoch 17] Batch 5000, Loss 0.1953597366809845\n",
      "[Training Epoch 17] Batch 7500, Loss 0.17051276564598083\n",
      "[Evluating Epoch 17] HR = 0.6462, NDCG = 0.3718\n",
      "Hit Ratio: 0.6462\n",
      "NDCG: 0.3718\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.20458683371543884\n",
      "[Training Epoch 18] Batch 2500, Loss 0.1905777007341385\n",
      "[Training Epoch 18] Batch 5000, Loss 0.20058688521385193\n",
      "[Training Epoch 18] Batch 7500, Loss 0.1623384952545166\n",
      "[Evluating Epoch 18] HR = 0.6429, NDCG = 0.3727\n",
      "Hit Ratio: 0.6429\n",
      "NDCG: 0.3727\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.18882939219474792\n",
      "[Training Epoch 19] Batch 2500, Loss 0.188787579536438\n",
      "[Training Epoch 19] Batch 5000, Loss 0.15505224466323853\n",
      "[Training Epoch 19] Batch 7500, Loss 0.18053099513053894\n",
      "[Evluating Epoch 19] HR = 0.6444, NDCG = 0.3728\n",
      "Hit Ratio: 0.6444\n",
      "NDCG: 0.3728\n",
      "--------------------------------------------------------------------------------\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=8, out_features=1, bias=True)\n",
      "MLP(\n",
      "  (embedding_user): Embedding(6040, 8)\n",
      "  (embedding_item): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.5810130834579468\n",
      "[Training Epoch 0] Batch 2500, Loss 0.19494697451591492\n",
      "[Training Epoch 0] Batch 5000, Loss 0.2744312286376953\n",
      "[Training Epoch 0] Batch 7500, Loss 0.21555131673812866\n",
      "[Training Epoch 0] Batch 10000, Loss 0.2203117311000824\n",
      "[Training Epoch 0] Batch 12500, Loss 0.15878137946128845\n",
      "[Training Epoch 0] Batch 15000, Loss 0.186568945646286\n",
      "[Training Epoch 0] Batch 17500, Loss 0.30806174874305725\n",
      "[Training Epoch 0] Batch 20000, Loss 0.2771783769130707\n",
      "[Training Epoch 0] Batch 22500, Loss 0.2392418384552002\n",
      "[Training Epoch 0] Batch 25000, Loss 0.23818492889404297\n",
      "[Training Epoch 0] Batch 27500, Loss 0.2596779465675354\n",
      "[Training Epoch 0] Batch 30000, Loss 0.2003781795501709\n",
      "[Training Epoch 0] Batch 32500, Loss 0.2505708336830139\n",
      "[Training Epoch 0] Batch 35000, Loss 0.19738313555717468\n",
      "[Training Epoch 0] Batch 37500, Loss 0.2137998640537262\n",
      "[Evluating Epoch 0] HR = 0.4465, NDCG = 0.2453\n",
      "Hit Ratio: 0.4465\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.2485557496547699\n",
      "[Training Epoch 1] Batch 2500, Loss 0.230045348405838\n",
      "[Training Epoch 1] Batch 5000, Loss 0.21823030710220337\n",
      "[Training Epoch 1] Batch 7500, Loss 0.28493571281433105\n",
      "[Training Epoch 1] Batch 10000, Loss 0.24035415053367615\n",
      "[Training Epoch 1] Batch 12500, Loss 0.22194921970367432\n",
      "[Training Epoch 1] Batch 15000, Loss 0.2360401749610901\n",
      "[Training Epoch 1] Batch 17500, Loss 0.28893840312957764\n",
      "[Training Epoch 1] Batch 20000, Loss 0.2875845432281494\n",
      "[Training Epoch 1] Batch 22500, Loss 0.2717241048812866\n",
      "[Training Epoch 1] Batch 25000, Loss 0.1957452893257141\n",
      "[Training Epoch 1] Batch 27500, Loss 0.23244495689868927\n",
      "[Training Epoch 1] Batch 30000, Loss 0.2288486212491989\n",
      "[Training Epoch 1] Batch 32500, Loss 0.2604649066925049\n",
      "[Training Epoch 1] Batch 35000, Loss 0.2851027250289917\n",
      "[Training Epoch 1] Batch 37500, Loss 0.23113468289375305\n",
      "[Evluating Epoch 1] HR = 0.4728, NDCG = 0.2629\n",
      "Hit Ratio: 0.4728\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.24525858461856842\n",
      "[Training Epoch 2] Batch 2500, Loss 0.2836076021194458\n",
      "[Training Epoch 2] Batch 5000, Loss 0.21070510149002075\n",
      "[Training Epoch 2] Batch 7500, Loss 0.20130190253257751\n",
      "[Training Epoch 2] Batch 10000, Loss 0.19037938117980957\n",
      "[Training Epoch 2] Batch 12500, Loss 0.21072858572006226\n",
      "[Training Epoch 2] Batch 15000, Loss 0.22364334762096405\n",
      "[Training Epoch 2] Batch 17500, Loss 0.24906569719314575\n",
      "[Training Epoch 2] Batch 20000, Loss 0.17709684371948242\n",
      "[Training Epoch 2] Batch 22500, Loss 0.2171899676322937\n",
      "[Training Epoch 2] Batch 25000, Loss 0.2013549655675888\n",
      "[Training Epoch 2] Batch 27500, Loss 0.2478262037038803\n",
      "[Training Epoch 2] Batch 30000, Loss 0.2021486759185791\n",
      "[Training Epoch 2] Batch 32500, Loss 0.18918296694755554\n",
      "[Training Epoch 2] Batch 35000, Loss 0.2477014660835266\n",
      "[Training Epoch 2] Batch 37500, Loss 0.25158026814460754\n",
      "[Evluating Epoch 2] HR = 0.5098, NDCG = 0.2849\n",
      "Hit Ratio: 0.5098\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.24112334847450256\n",
      "[Training Epoch 3] Batch 2500, Loss 0.17414426803588867\n",
      "[Training Epoch 3] Batch 5000, Loss 0.22931408882141113\n",
      "[Training Epoch 3] Batch 7500, Loss 0.22846151888370514\n",
      "[Training Epoch 3] Batch 10000, Loss 0.31498983502388\n",
      "[Training Epoch 3] Batch 12500, Loss 0.2844783365726471\n",
      "[Training Epoch 3] Batch 15000, Loss 0.2545790672302246\n",
      "[Training Epoch 3] Batch 17500, Loss 0.1775471419095993\n",
      "[Training Epoch 3] Batch 20000, Loss 0.1879807859659195\n",
      "[Training Epoch 3] Batch 22500, Loss 0.21289891004562378\n",
      "[Training Epoch 3] Batch 25000, Loss 0.22616074979305267\n",
      "[Training Epoch 3] Batch 27500, Loss 0.24326221644878387\n",
      "[Training Epoch 3] Batch 30000, Loss 0.2141537070274353\n",
      "[Training Epoch 3] Batch 32500, Loss 0.17185595631599426\n",
      "[Training Epoch 3] Batch 35000, Loss 0.21753087639808655\n",
      "[Training Epoch 3] Batch 37500, Loss 0.1979435533285141\n",
      "[Evluating Epoch 3] HR = 0.5401, NDCG = 0.3013\n",
      "Hit Ratio: 0.5401\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.1827804297208786\n",
      "[Training Epoch 4] Batch 2500, Loss 0.18968448042869568\n",
      "[Training Epoch 4] Batch 5000, Loss 0.19861629605293274\n",
      "[Training Epoch 4] Batch 7500, Loss 0.25572991371154785\n",
      "[Training Epoch 4] Batch 10000, Loss 0.23627527058124542\n",
      "[Training Epoch 4] Batch 12500, Loss 0.2023119479417801\n",
      "[Training Epoch 4] Batch 15000, Loss 0.24878525733947754\n",
      "[Training Epoch 4] Batch 17500, Loss 0.18482500314712524\n",
      "[Training Epoch 4] Batch 20000, Loss 0.18072384595870972\n",
      "[Training Epoch 4] Batch 22500, Loss 0.20077259838581085\n",
      "[Training Epoch 4] Batch 25000, Loss 0.22308476269245148\n",
      "[Training Epoch 4] Batch 27500, Loss 0.17491364479064941\n",
      "[Training Epoch 4] Batch 30000, Loss 0.19197848439216614\n",
      "[Training Epoch 4] Batch 32500, Loss 0.17835897207260132\n",
      "[Training Epoch 4] Batch 35000, Loss 0.17755255103111267\n",
      "[Training Epoch 4] Batch 37500, Loss 0.20544986426830292\n",
      "[Evluating Epoch 4] HR = 0.5538, NDCG = 0.3108\n",
      "Hit Ratio: 0.5538\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.29507023096084595\n",
      "[Training Epoch 5] Batch 2500, Loss 0.17356134951114655\n",
      "[Training Epoch 5] Batch 5000, Loss 0.21279501914978027\n",
      "[Training Epoch 5] Batch 7500, Loss 0.21617327630519867\n",
      "[Training Epoch 5] Batch 10000, Loss 0.2637059986591339\n",
      "[Training Epoch 5] Batch 12500, Loss 0.19873227179050446\n",
      "[Training Epoch 5] Batch 15000, Loss 0.1418934464454651\n",
      "[Training Epoch 5] Batch 17500, Loss 0.19383227825164795\n",
      "[Training Epoch 5] Batch 20000, Loss 0.26183074712753296\n",
      "[Training Epoch 5] Batch 22500, Loss 0.21597877144813538\n",
      "[Training Epoch 5] Batch 25000, Loss 0.20141589641571045\n",
      "[Training Epoch 5] Batch 27500, Loss 0.17340871691703796\n",
      "[Training Epoch 5] Batch 30000, Loss 0.2032812237739563\n",
      "[Training Epoch 5] Batch 32500, Loss 0.2313026338815689\n",
      "[Training Epoch 5] Batch 35000, Loss 0.1969756782054901\n",
      "[Training Epoch 5] Batch 37500, Loss 0.18660461902618408\n",
      "[Evluating Epoch 5] HR = 0.5571, NDCG = 0.3122\n",
      "Hit Ratio: 0.5571\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.2382892519235611\n",
      "[Training Epoch 6] Batch 2500, Loss 0.1515968143939972\n",
      "[Training Epoch 6] Batch 5000, Loss 0.14806611835956573\n",
      "[Training Epoch 6] Batch 7500, Loss 0.17506524920463562\n",
      "[Training Epoch 6] Batch 10000, Loss 0.20515723526477814\n",
      "[Training Epoch 6] Batch 12500, Loss 0.19825510680675507\n",
      "[Training Epoch 6] Batch 15000, Loss 0.22382980585098267\n",
      "[Training Epoch 6] Batch 17500, Loss 0.2400534301996231\n",
      "[Training Epoch 6] Batch 20000, Loss 0.12490475177764893\n",
      "[Training Epoch 6] Batch 22500, Loss 0.25003165006637573\n",
      "[Training Epoch 6] Batch 25000, Loss 0.21321269869804382\n",
      "[Training Epoch 6] Batch 27500, Loss 0.2066217064857483\n",
      "[Training Epoch 6] Batch 30000, Loss 0.17282217741012573\n",
      "[Training Epoch 6] Batch 32500, Loss 0.20214444398880005\n",
      "[Training Epoch 6] Batch 35000, Loss 0.21234655380249023\n",
      "[Training Epoch 6] Batch 37500, Loss 0.1776498258113861\n",
      "[Evluating Epoch 6] HR = 0.5583, NDCG = 0.3143\n",
      "Hit Ratio: 0.5583\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.16660453379154205\n",
      "[Training Epoch 7] Batch 2500, Loss 0.1951868236064911\n",
      "[Training Epoch 7] Batch 5000, Loss 0.19319191575050354\n",
      "[Training Epoch 7] Batch 7500, Loss 0.22957508265972137\n",
      "[Training Epoch 7] Batch 10000, Loss 0.21296891570091248\n",
      "[Training Epoch 7] Batch 12500, Loss 0.19709157943725586\n",
      "[Training Epoch 7] Batch 15000, Loss 0.2286292016506195\n",
      "[Training Epoch 7] Batch 17500, Loss 0.1816367357969284\n",
      "[Training Epoch 7] Batch 20000, Loss 0.17835816740989685\n",
      "[Training Epoch 7] Batch 22500, Loss 0.20203807950019836\n",
      "[Training Epoch 7] Batch 25000, Loss 0.2177801877260208\n",
      "[Training Epoch 7] Batch 27500, Loss 0.2143249362707138\n",
      "[Training Epoch 7] Batch 30000, Loss 0.21037563681602478\n",
      "[Training Epoch 7] Batch 32500, Loss 0.19145026803016663\n",
      "[Training Epoch 7] Batch 35000, Loss 0.15552866458892822\n",
      "[Training Epoch 7] Batch 37500, Loss 0.31037330627441406\n",
      "[Evluating Epoch 7] HR = 0.5682, NDCG = 0.3169\n",
      "Hit Ratio: 0.5682\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.2542365789413452\n",
      "[Training Epoch 8] Batch 2500, Loss 0.20441174507141113\n",
      "[Training Epoch 8] Batch 5000, Loss 0.2062026709318161\n",
      "[Training Epoch 8] Batch 7500, Loss 0.22060012817382812\n",
      "[Training Epoch 8] Batch 10000, Loss 0.20300404727458954\n",
      "[Training Epoch 8] Batch 12500, Loss 0.1725831776857376\n",
      "[Training Epoch 8] Batch 15000, Loss 0.16678214073181152\n",
      "[Training Epoch 8] Batch 17500, Loss 0.23074893653392792\n",
      "[Training Epoch 8] Batch 20000, Loss 0.20092609524726868\n",
      "[Training Epoch 8] Batch 22500, Loss 0.22195208072662354\n",
      "[Training Epoch 8] Batch 25000, Loss 0.21405670046806335\n",
      "[Training Epoch 8] Batch 27500, Loss 0.2538785934448242\n",
      "[Training Epoch 8] Batch 30000, Loss 0.20054370164871216\n",
      "[Training Epoch 8] Batch 32500, Loss 0.21882398426532745\n",
      "[Training Epoch 8] Batch 35000, Loss 0.21731440722942352\n",
      "[Training Epoch 8] Batch 37500, Loss 0.20097795128822327\n",
      "[Evluating Epoch 8] HR = 0.5763, NDCG = 0.3236\n",
      "Hit Ratio: 0.5763\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.24722500145435333\n",
      "[Training Epoch 9] Batch 2500, Loss 0.17060744762420654\n",
      "[Training Epoch 9] Batch 5000, Loss 0.21780166029930115\n",
      "[Training Epoch 9] Batch 7500, Loss 0.22684133052825928\n",
      "[Training Epoch 9] Batch 10000, Loss 0.1766931414604187\n",
      "[Training Epoch 9] Batch 12500, Loss 0.1939428746700287\n",
      "[Training Epoch 9] Batch 15000, Loss 0.2155204713344574\n",
      "[Training Epoch 9] Batch 17500, Loss 0.233953595161438\n",
      "[Training Epoch 9] Batch 20000, Loss 0.20177054405212402\n",
      "[Training Epoch 9] Batch 22500, Loss 0.1511346697807312\n",
      "[Training Epoch 9] Batch 25000, Loss 0.21955588459968567\n",
      "[Training Epoch 9] Batch 27500, Loss 0.2048538476228714\n",
      "[Training Epoch 9] Batch 30000, Loss 0.19617588818073273\n",
      "[Training Epoch 9] Batch 32500, Loss 0.25601255893707275\n",
      "[Training Epoch 9] Batch 35000, Loss 0.22151970863342285\n",
      "[Training Epoch 9] Batch 37500, Loss 0.15111413598060608\n",
      "[Evluating Epoch 9] HR = 0.5755, NDCG = 0.3219\n",
      "Hit Ratio: 0.5755\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.24729353189468384\n",
      "[Training Epoch 10] Batch 2500, Loss 0.18643471598625183\n",
      "[Training Epoch 10] Batch 5000, Loss 0.2703065872192383\n",
      "[Training Epoch 10] Batch 7500, Loss 0.2111806422472\n",
      "[Training Epoch 10] Batch 10000, Loss 0.2499670684337616\n",
      "[Training Epoch 10] Batch 12500, Loss 0.16299018263816833\n",
      "[Training Epoch 10] Batch 15000, Loss 0.23850005865097046\n",
      "[Training Epoch 10] Batch 17500, Loss 0.2415803074836731\n",
      "[Training Epoch 10] Batch 20000, Loss 0.19762375950813293\n",
      "[Training Epoch 10] Batch 22500, Loss 0.18909995257854462\n",
      "[Training Epoch 10] Batch 25000, Loss 0.19784945249557495\n",
      "[Training Epoch 10] Batch 27500, Loss 0.22190827131271362\n",
      "[Training Epoch 10] Batch 30000, Loss 0.13264676928520203\n",
      "[Training Epoch 10] Batch 32500, Loss 0.23224860429763794\n",
      "[Training Epoch 10] Batch 35000, Loss 0.17289814352989197\n",
      "[Training Epoch 10] Batch 37500, Loss 0.26591938734054565\n",
      "[Evluating Epoch 10] HR = 0.5712, NDCG = 0.3247\n",
      "Hit Ratio: 0.5712\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.17963868379592896\n",
      "[Training Epoch 11] Batch 2500, Loss 0.21949204802513123\n",
      "[Training Epoch 11] Batch 5000, Loss 0.1906215250492096\n",
      "[Training Epoch 11] Batch 7500, Loss 0.1971142441034317\n",
      "[Training Epoch 11] Batch 10000, Loss 0.15197521448135376\n",
      "[Training Epoch 11] Batch 12500, Loss 0.17296472191810608\n",
      "[Training Epoch 11] Batch 15000, Loss 0.2168535441160202\n",
      "[Training Epoch 11] Batch 17500, Loss 0.17125490307807922\n",
      "[Training Epoch 11] Batch 20000, Loss 0.19336089491844177\n",
      "[Training Epoch 11] Batch 22500, Loss 0.15675048530101776\n",
      "[Training Epoch 11] Batch 25000, Loss 0.1675245761871338\n",
      "[Training Epoch 11] Batch 27500, Loss 0.1648866981267929\n",
      "[Training Epoch 11] Batch 30000, Loss 0.18313679099082947\n",
      "[Training Epoch 11] Batch 32500, Loss 0.20628949999809265\n",
      "[Training Epoch 11] Batch 35000, Loss 0.1901680827140808\n",
      "[Training Epoch 11] Batch 37500, Loss 0.17785368859767914\n",
      "[Evluating Epoch 11] HR = 0.5767, NDCG = 0.3287\n",
      "Hit Ratio: 0.5767\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.16873005032539368\n",
      "[Training Epoch 12] Batch 2500, Loss 0.18390130996704102\n",
      "[Training Epoch 12] Batch 5000, Loss 0.18094661831855774\n",
      "[Training Epoch 12] Batch 7500, Loss 0.2127046287059784\n",
      "[Training Epoch 12] Batch 10000, Loss 0.17784181237220764\n",
      "[Training Epoch 12] Batch 12500, Loss 0.20499339699745178\n",
      "[Training Epoch 12] Batch 15000, Loss 0.22311070561408997\n",
      "[Training Epoch 12] Batch 17500, Loss 0.1375901848077774\n",
      "[Training Epoch 12] Batch 20000, Loss 0.18104900419712067\n",
      "[Training Epoch 12] Batch 22500, Loss 0.19066736102104187\n",
      "[Training Epoch 12] Batch 25000, Loss 0.19757258892059326\n",
      "[Training Epoch 12] Batch 27500, Loss 0.20284174382686615\n",
      "[Training Epoch 12] Batch 30000, Loss 0.16982613503932953\n",
      "[Training Epoch 12] Batch 32500, Loss 0.178301602602005\n",
      "[Training Epoch 12] Batch 35000, Loss 0.2162388414144516\n",
      "[Training Epoch 12] Batch 37500, Loss 0.1739930808544159\n",
      "[Evluating Epoch 12] HR = 0.5815, NDCG = 0.3305\n",
      "Hit Ratio: 0.5815\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.17667193710803986\n",
      "[Training Epoch 13] Batch 2500, Loss 0.18953213095664978\n",
      "[Training Epoch 13] Batch 5000, Loss 0.17676106095314026\n",
      "[Training Epoch 13] Batch 7500, Loss 0.16963569819927216\n",
      "[Training Epoch 13] Batch 10000, Loss 0.1874484121799469\n",
      "[Training Epoch 13] Batch 12500, Loss 0.17354047298431396\n",
      "[Training Epoch 13] Batch 15000, Loss 0.25388652086257935\n",
      "[Training Epoch 13] Batch 17500, Loss 0.2208244800567627\n",
      "[Training Epoch 13] Batch 20000, Loss 0.22715751826763153\n",
      "[Training Epoch 13] Batch 22500, Loss 0.22025582194328308\n",
      "[Training Epoch 13] Batch 25000, Loss 0.18256457149982452\n",
      "[Training Epoch 13] Batch 27500, Loss 0.20817813277244568\n",
      "[Training Epoch 13] Batch 30000, Loss 0.24604105949401855\n",
      "[Training Epoch 13] Batch 32500, Loss 0.17235375940799713\n",
      "[Training Epoch 13] Batch 35000, Loss 0.2429460883140564\n",
      "[Training Epoch 13] Batch 37500, Loss 0.22139599919319153\n",
      "[Evluating Epoch 13] HR = 0.5720, NDCG = 0.3254\n",
      "Hit Ratio: 0.5720\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.1399223804473877\n",
      "[Training Epoch 14] Batch 2500, Loss 0.19695085287094116\n",
      "[Training Epoch 14] Batch 5000, Loss 0.19991621375083923\n",
      "[Training Epoch 14] Batch 7500, Loss 0.15997551381587982\n",
      "[Training Epoch 14] Batch 10000, Loss 0.21960657835006714\n",
      "[Training Epoch 14] Batch 12500, Loss 0.24163781106472015\n",
      "[Training Epoch 14] Batch 15000, Loss 0.19884571433067322\n",
      "[Training Epoch 14] Batch 17500, Loss 0.174775168299675\n",
      "[Training Epoch 14] Batch 20000, Loss 0.23872050642967224\n",
      "[Training Epoch 14] Batch 22500, Loss 0.19198890030384064\n",
      "[Training Epoch 14] Batch 25000, Loss 0.2538757622241974\n",
      "[Training Epoch 14] Batch 27500, Loss 0.17029969394207\n",
      "[Training Epoch 14] Batch 30000, Loss 0.27465641498565674\n",
      "[Training Epoch 14] Batch 32500, Loss 0.22264254093170166\n",
      "[Training Epoch 14] Batch 35000, Loss 0.1675167679786682\n",
      "[Training Epoch 14] Batch 37500, Loss 0.2295636385679245\n",
      "[Evluating Epoch 14] HR = 0.5811, NDCG = 0.3316\n",
      "Hit Ratio: 0.5811\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.17629577219486237\n",
      "[Training Epoch 15] Batch 2500, Loss 0.22806423902511597\n",
      "[Training Epoch 15] Batch 5000, Loss 0.14546161890029907\n",
      "[Training Epoch 15] Batch 7500, Loss 0.1728842705488205\n",
      "[Training Epoch 15] Batch 10000, Loss 0.2575582265853882\n",
      "[Training Epoch 15] Batch 12500, Loss 0.27195554971694946\n",
      "[Training Epoch 15] Batch 15000, Loss 0.25852811336517334\n",
      "[Training Epoch 15] Batch 17500, Loss 0.1585155427455902\n",
      "[Training Epoch 15] Batch 20000, Loss 0.24656307697296143\n",
      "[Training Epoch 15] Batch 22500, Loss 0.1837245523929596\n",
      "[Training Epoch 15] Batch 25000, Loss 0.19260141253471375\n",
      "[Training Epoch 15] Batch 27500, Loss 0.32116058468818665\n",
      "[Training Epoch 15] Batch 30000, Loss 0.18051087856292725\n",
      "[Training Epoch 15] Batch 32500, Loss 0.25081443786621094\n",
      "[Training Epoch 15] Batch 35000, Loss 0.16794274747371674\n",
      "[Training Epoch 15] Batch 37500, Loss 0.14257001876831055\n",
      "[Evluating Epoch 15] HR = 0.5829, NDCG = 0.3299\n",
      "Hit Ratio: 0.5829\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.21199333667755127\n",
      "[Training Epoch 16] Batch 2500, Loss 0.24437449872493744\n",
      "[Training Epoch 16] Batch 5000, Loss 0.23177684843540192\n",
      "[Training Epoch 16] Batch 7500, Loss 0.18032050132751465\n",
      "[Training Epoch 16] Batch 10000, Loss 0.16915293037891388\n",
      "[Training Epoch 16] Batch 12500, Loss 0.15540897846221924\n",
      "[Training Epoch 16] Batch 15000, Loss 0.18297304213047028\n",
      "[Training Epoch 16] Batch 17500, Loss 0.17237597703933716\n",
      "[Training Epoch 16] Batch 20000, Loss 0.24848459661006927\n",
      "[Training Epoch 16] Batch 22500, Loss 0.17522719502449036\n",
      "[Training Epoch 16] Batch 25000, Loss 0.1771872341632843\n",
      "[Training Epoch 16] Batch 27500, Loss 0.1899637132883072\n",
      "[Training Epoch 16] Batch 30000, Loss 0.20163093507289886\n",
      "[Training Epoch 16] Batch 32500, Loss 0.2626340985298157\n",
      "[Training Epoch 16] Batch 35000, Loss 0.18291163444519043\n",
      "[Training Epoch 16] Batch 37500, Loss 0.161675363779068\n",
      "[Evluating Epoch 16] HR = 0.5767, NDCG = 0.3295\n",
      "Hit Ratio: 0.5767\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.2219245731830597\n",
      "[Training Epoch 17] Batch 2500, Loss 0.1799764484167099\n",
      "[Training Epoch 17] Batch 5000, Loss 0.1575029194355011\n",
      "[Training Epoch 17] Batch 7500, Loss 0.18222372233867645\n",
      "[Training Epoch 17] Batch 10000, Loss 0.15491198003292084\n",
      "[Training Epoch 17] Batch 12500, Loss 0.1678711473941803\n",
      "[Training Epoch 17] Batch 15000, Loss 0.13690508902072906\n",
      "[Training Epoch 17] Batch 17500, Loss 0.22635580599308014\n",
      "[Training Epoch 17] Batch 20000, Loss 0.18308301270008087\n",
      "[Training Epoch 17] Batch 22500, Loss 0.25422170758247375\n",
      "[Training Epoch 17] Batch 25000, Loss 0.23094084858894348\n",
      "[Training Epoch 17] Batch 27500, Loss 0.20243136584758759\n",
      "[Training Epoch 17] Batch 30000, Loss 0.20394332706928253\n",
      "[Training Epoch 17] Batch 32500, Loss 0.1776716709136963\n",
      "[Training Epoch 17] Batch 35000, Loss 0.17198637127876282\n",
      "[Training Epoch 17] Batch 37500, Loss 0.19104540348052979\n",
      "[Evluating Epoch 17] HR = 0.5828, NDCG = 0.3300\n",
      "Hit Ratio: 0.5828\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.19912326335906982\n",
      "[Training Epoch 18] Batch 2500, Loss 0.21634039282798767\n",
      "[Training Epoch 18] Batch 5000, Loss 0.2300347238779068\n",
      "[Training Epoch 18] Batch 7500, Loss 0.2415216863155365\n",
      "[Training Epoch 18] Batch 10000, Loss 0.1850268840789795\n",
      "[Training Epoch 18] Batch 12500, Loss 0.18903174996376038\n",
      "[Training Epoch 18] Batch 15000, Loss 0.23940011858940125\n",
      "[Training Epoch 18] Batch 17500, Loss 0.1602206826210022\n",
      "[Training Epoch 18] Batch 20000, Loss 0.1700022667646408\n",
      "[Training Epoch 18] Batch 22500, Loss 0.2341083586215973\n",
      "[Training Epoch 18] Batch 25000, Loss 0.23644432425498962\n",
      "[Training Epoch 18] Batch 27500, Loss 0.20275075733661652\n",
      "[Training Epoch 18] Batch 30000, Loss 0.1730528026819229\n",
      "[Training Epoch 18] Batch 32500, Loss 0.2440338134765625\n",
      "[Training Epoch 18] Batch 35000, Loss 0.195713609457016\n",
      "[Training Epoch 18] Batch 37500, Loss 0.18242374062538147\n",
      "[Evluating Epoch 18] HR = 0.5828, NDCG = 0.3320\n",
      "Hit Ratio: 0.5828\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.17750973999500275\n",
      "[Training Epoch 19] Batch 2500, Loss 0.17804548144340515\n",
      "[Training Epoch 19] Batch 5000, Loss 0.19699345529079437\n",
      "[Training Epoch 19] Batch 7500, Loss 0.24316009879112244\n",
      "[Training Epoch 19] Batch 10000, Loss 0.16537857055664062\n",
      "[Training Epoch 19] Batch 12500, Loss 0.19934922456741333\n",
      "[Training Epoch 19] Batch 15000, Loss 0.1851549744606018\n",
      "[Training Epoch 19] Batch 17500, Loss 0.20273858308792114\n",
      "[Training Epoch 19] Batch 20000, Loss 0.1432577669620514\n",
      "[Training Epoch 19] Batch 22500, Loss 0.19334489107131958\n",
      "[Training Epoch 19] Batch 25000, Loss 0.21657341718673706\n",
      "[Training Epoch 19] Batch 27500, Loss 0.19514575600624084\n",
      "[Training Epoch 19] Batch 30000, Loss 0.23650923371315002\n",
      "[Training Epoch 19] Batch 32500, Loss 0.16292427480220795\n",
      "[Training Epoch 19] Batch 35000, Loss 0.2674786448478699\n",
      "[Training Epoch 19] Batch 37500, Loss 0.17972376942634583\n",
      "[Evluating Epoch 19] HR = 0.5839, NDCG = 0.3323\n",
      "Hit Ratio: 0.5839\n",
      "--------------------------------------------------------------------------------\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=32, bias=True)\n",
      "Linear(in_features=32, out_features=16, bias=True)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=16, out_features=1, bias=True)\n",
      "NeuMF(\n",
      "  (embedding_user_mlp): Embedding(6040, 8)\n",
      "  (embedding_item_mlp): Embedding(3706, 8)\n",
      "  (embedding_user_mf): Embedding(6040, 8)\n",
      "  (embedding_item_mf): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.6700184941291809\n",
      "[Training Epoch 0] Batch 2500, Loss 0.20985819399356842\n",
      "[Training Epoch 0] Batch 5000, Loss 0.2077006995677948\n",
      "[Training Epoch 0] Batch 7500, Loss 0.23628413677215576\n",
      "[Evluating Epoch 0] HR = 0.5697, NDCG = 0.3239\n",
      "Hit Ratio: 0.5697\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.18462863564491272\n",
      "[Training Epoch 1] Batch 2500, Loss 0.19560909271240234\n",
      "[Training Epoch 1] Batch 5000, Loss 0.21941494941711426\n",
      "[Training Epoch 1] Batch 7500, Loss 0.19821512699127197\n",
      "[Evluating Epoch 1] HR = 0.6166, NDCG = 0.3565\n",
      "Hit Ratio: 0.6166\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.18538179993629456\n",
      "[Training Epoch 2] Batch 2500, Loss 0.1843346357345581\n",
      "[Training Epoch 2] Batch 5000, Loss 0.1732773631811142\n",
      "[Training Epoch 2] Batch 7500, Loss 0.16938427090644836\n",
      "[Evluating Epoch 2] HR = 0.6381, NDCG = 0.3686\n",
      "Hit Ratio: 0.6381\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.1602381467819214\n",
      "[Training Epoch 3] Batch 2500, Loss 0.1958090215921402\n",
      "[Training Epoch 3] Batch 5000, Loss 0.1900278925895691\n",
      "[Training Epoch 3] Batch 7500, Loss 0.17245334386825562\n",
      "[Evluating Epoch 3] HR = 0.6394, NDCG = 0.3709\n",
      "Hit Ratio: 0.6394\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.20425961911678314\n",
      "[Training Epoch 4] Batch 2500, Loss 0.17671328783035278\n",
      "[Training Epoch 4] Batch 5000, Loss 0.19042536616325378\n",
      "[Training Epoch 4] Batch 7500, Loss 0.17088446021080017\n",
      "[Evluating Epoch 4] HR = 0.6472, NDCG = 0.3767\n",
      "Hit Ratio: 0.6472\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.18143439292907715\n",
      "[Training Epoch 5] Batch 2500, Loss 0.18645374476909637\n",
      "[Training Epoch 5] Batch 5000, Loss 0.17320336401462555\n",
      "[Training Epoch 5] Batch 7500, Loss 0.18028584122657776\n",
      "[Evluating Epoch 5] HR = 0.6522, NDCG = 0.3854\n",
      "Hit Ratio: 0.6522\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.18332728743553162\n",
      "[Training Epoch 6] Batch 2500, Loss 0.1903562694787979\n",
      "[Training Epoch 6] Batch 5000, Loss 0.18203485012054443\n",
      "[Training Epoch 6] Batch 7500, Loss 0.18198224902153015\n",
      "[Evluating Epoch 6] HR = 0.6578, NDCG = 0.3878\n",
      "Hit Ratio: 0.6578\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.17529766261577606\n",
      "[Training Epoch 7] Batch 2500, Loss 0.17848753929138184\n",
      "[Training Epoch 7] Batch 5000, Loss 0.19021925330162048\n",
      "[Training Epoch 7] Batch 7500, Loss 0.206959530711174\n",
      "[Evluating Epoch 7] HR = 0.6594, NDCG = 0.3882\n",
      "Hit Ratio: 0.6594\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.17094245553016663\n",
      "[Training Epoch 8] Batch 2500, Loss 0.1591351181268692\n",
      "[Training Epoch 8] Batch 5000, Loss 0.17135190963745117\n",
      "[Training Epoch 8] Batch 7500, Loss 0.1871616691350937\n",
      "[Evluating Epoch 8] HR = 0.6649, NDCG = 0.3926\n",
      "Hit Ratio: 0.6649\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.17967578768730164\n",
      "[Training Epoch 9] Batch 2500, Loss 0.1856919229030609\n",
      "[Training Epoch 9] Batch 5000, Loss 0.1798015981912613\n",
      "[Training Epoch 9] Batch 7500, Loss 0.18528962135314941\n",
      "[Evluating Epoch 9] HR = 0.6694, NDCG = 0.3959\n",
      "Hit Ratio: 0.6694\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.19187799096107483\n",
      "[Training Epoch 10] Batch 2500, Loss 0.1939067542552948\n",
      "[Training Epoch 10] Batch 5000, Loss 0.1594226211309433\n",
      "[Training Epoch 10] Batch 7500, Loss 0.17740803956985474\n",
      "[Evluating Epoch 10] HR = 0.6682, NDCG = 0.3963\n",
      "Hit Ratio: 0.6682\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.15121005475521088\n",
      "[Training Epoch 11] Batch 2500, Loss 0.16846324503421783\n",
      "[Training Epoch 11] Batch 5000, Loss 0.17774692177772522\n",
      "[Training Epoch 11] Batch 7500, Loss 0.2030489444732666\n",
      "[Evluating Epoch 11] HR = 0.6737, NDCG = 0.3994\n",
      "Hit Ratio: 0.6737\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.17252567410469055\n",
      "[Training Epoch 12] Batch 2500, Loss 0.1949305236339569\n",
      "[Training Epoch 12] Batch 5000, Loss 0.18791383504867554\n",
      "[Training Epoch 12] Batch 7500, Loss 0.17892569303512573\n",
      "[Evluating Epoch 12] HR = 0.6765, NDCG = 0.4025\n",
      "Hit Ratio: 0.6765\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.1826973408460617\n",
      "[Training Epoch 13] Batch 2500, Loss 0.1779824197292328\n",
      "[Training Epoch 13] Batch 5000, Loss 0.19855713844299316\n",
      "[Training Epoch 13] Batch 7500, Loss 0.19097529351711273\n",
      "[Evluating Epoch 13] HR = 0.6742, NDCG = 0.4048\n",
      "Hit Ratio: 0.6742\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.14908179640769958\n",
      "[Training Epoch 14] Batch 2500, Loss 0.17786140739917755\n",
      "[Training Epoch 14] Batch 5000, Loss 0.16397449374198914\n",
      "[Training Epoch 14] Batch 7500, Loss 0.17347897589206696\n",
      "[Evluating Epoch 14] HR = 0.6738, NDCG = 0.4036\n",
      "Hit Ratio: 0.6738\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.18211784958839417\n",
      "[Training Epoch 15] Batch 2500, Loss 0.1841897815465927\n",
      "[Training Epoch 15] Batch 5000, Loss 0.158391535282135\n",
      "[Training Epoch 15] Batch 7500, Loss 0.1705332100391388\n",
      "[Evluating Epoch 15] HR = 0.6750, NDCG = 0.4046\n",
      "Hit Ratio: 0.6750\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.17920222878456116\n",
      "[Training Epoch 16] Batch 2500, Loss 0.2043907344341278\n",
      "[Training Epoch 16] Batch 5000, Loss 0.15705081820487976\n",
      "[Training Epoch 16] Batch 7500, Loss 0.19640648365020752\n",
      "[Evluating Epoch 16] HR = 0.6803, NDCG = 0.4079\n",
      "Hit Ratio: 0.6803\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.16389359533786774\n",
      "[Training Epoch 17] Batch 2500, Loss 0.16827425360679626\n",
      "[Training Epoch 17] Batch 5000, Loss 0.20054805278778076\n",
      "[Training Epoch 17] Batch 7500, Loss 0.16572552919387817\n",
      "[Evluating Epoch 17] HR = 0.6828, NDCG = 0.4100\n",
      "Hit Ratio: 0.6828\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.18794327974319458\n",
      "[Training Epoch 18] Batch 2500, Loss 0.16457028687000275\n",
      "[Training Epoch 18] Batch 5000, Loss 0.17163708806037903\n",
      "[Training Epoch 18] Batch 7500, Loss 0.14783212542533875\n",
      "[Evluating Epoch 18] HR = 0.6863, NDCG = 0.4098\n",
      "Hit Ratio: 0.6863\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.17805609107017517\n",
      "[Training Epoch 19] Batch 2500, Loss 0.17769378423690796\n",
      "[Training Epoch 19] Batch 5000, Loss 0.1759522259235382\n",
      "[Training Epoch 19] Batch 7500, Loss 0.17729084193706512\n",
      "[Evluating Epoch 19] HR = 0.6801, NDCG = 0.4087\n",
      "Hit Ratio: 0.6801\n",
      "--------------------------------------------------------------------------------\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "number of negatives: 10\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=8, out_features=1, bias=True)\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.6132986545562744\n",
      "[Training Epoch 0] Batch 2500, Loss 0.21990448236465454\n",
      "[Training Epoch 0] Batch 5000, Loss 0.21022821962833405\n",
      "[Training Epoch 0] Batch 7500, Loss 0.26182568073272705\n",
      "[Training Epoch 0] Batch 10000, Loss 0.2392158955335617\n",
      "[Evluating Epoch 0] HR = 0.4488, NDCG = 0.2504\n",
      "Hit Ratio: 0.4488\n",
      "NDCG: 0.2504\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 1] Batch 0, Loss 0.22426436841487885\n",
      "[Training Epoch 1] Batch 2500, Loss 0.23538224399089813\n",
      "[Training Epoch 1] Batch 5000, Loss 0.2218826711177826\n",
      "[Training Epoch 1] Batch 7500, Loss 0.2150592803955078\n",
      "[Training Epoch 1] Batch 10000, Loss 0.19038450717926025\n",
      "[Evluating Epoch 1] HR = 0.5046, NDCG = 0.2818\n",
      "Hit Ratio: 0.5046\n",
      "NDCG: 0.2818\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 2] Batch 0, Loss 0.19732807576656342\n",
      "[Training Epoch 2] Batch 2500, Loss 0.22040623426437378\n",
      "[Training Epoch 2] Batch 5000, Loss 0.19562798738479614\n",
      "[Training Epoch 2] Batch 7500, Loss 0.18525376915931702\n",
      "[Training Epoch 2] Batch 10000, Loss 0.18082919716835022\n",
      "[Evluating Epoch 2] HR = 0.5490, NDCG = 0.3093\n",
      "Hit Ratio: 0.5490\n",
      "NDCG: 0.3093\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 3] Batch 0, Loss 0.19175389409065247\n",
      "[Training Epoch 3] Batch 2500, Loss 0.18320134282112122\n",
      "[Training Epoch 3] Batch 5000, Loss 0.18945151567459106\n",
      "[Training Epoch 3] Batch 7500, Loss 0.1571841537952423\n",
      "[Training Epoch 3] Batch 10000, Loss 0.17349378764629364\n",
      "[Evluating Epoch 3] HR = 0.5737, NDCG = 0.3252\n",
      "Hit Ratio: 0.5737\n",
      "NDCG: 0.3252\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 4] Batch 0, Loss 0.1924927532672882\n",
      "[Training Epoch 4] Batch 2500, Loss 0.23316651582717896\n",
      "[Training Epoch 4] Batch 5000, Loss 0.16809473931789398\n",
      "[Training Epoch 4] Batch 7500, Loss 0.18907380104064941\n",
      "[Training Epoch 4] Batch 10000, Loss 0.1604263186454773\n",
      "[Evluating Epoch 4] HR = 0.6051, NDCG = 0.3470\n",
      "Hit Ratio: 0.6051\n",
      "NDCG: 0.3470\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 5] Batch 0, Loss 0.1789068877696991\n",
      "[Training Epoch 5] Batch 2500, Loss 0.18283504247665405\n",
      "[Training Epoch 5] Batch 5000, Loss 0.21122170984745026\n",
      "[Training Epoch 5] Batch 7500, Loss 0.16045241057872772\n",
      "[Training Epoch 5] Batch 10000, Loss 0.17270858585834503\n",
      "[Evluating Epoch 5] HR = 0.6212, NDCG = 0.3572\n",
      "Hit Ratio: 0.6212\n",
      "NDCG: 0.3572\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 6] Batch 0, Loss 0.18194222450256348\n",
      "[Training Epoch 6] Batch 2500, Loss 0.18504439294338226\n",
      "[Training Epoch 6] Batch 5000, Loss 0.15390172600746155\n",
      "[Training Epoch 6] Batch 7500, Loss 0.17430885136127472\n",
      "[Training Epoch 6] Batch 10000, Loss 0.2034575343132019\n",
      "[Evluating Epoch 6] HR = 0.6283, NDCG = 0.3624\n",
      "Hit Ratio: 0.6283\n",
      "NDCG: 0.3624\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 7] Batch 0, Loss 0.18078747391700745\n",
      "[Training Epoch 7] Batch 2500, Loss 0.14340123534202576\n",
      "[Training Epoch 7] Batch 5000, Loss 0.14876699447631836\n",
      "[Training Epoch 7] Batch 7500, Loss 0.2026570737361908\n",
      "[Training Epoch 7] Batch 10000, Loss 0.18110720813274384\n",
      "[Evluating Epoch 7] HR = 0.6379, NDCG = 0.3677\n",
      "Hit Ratio: 0.6379\n",
      "NDCG: 0.3677\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 8] Batch 0, Loss 0.16140444576740265\n",
      "[Training Epoch 8] Batch 2500, Loss 0.18078137934207916\n",
      "[Training Epoch 8] Batch 5000, Loss 0.1688343584537506\n",
      "[Training Epoch 8] Batch 7500, Loss 0.1971149742603302\n",
      "[Training Epoch 8] Batch 10000, Loss 0.1809486299753189\n",
      "[Evluating Epoch 8] HR = 0.6377, NDCG = 0.3692\n",
      "Hit Ratio: 0.6377\n",
      "NDCG: 0.3692\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 9] Batch 0, Loss 0.14733588695526123\n",
      "[Training Epoch 9] Batch 2500, Loss 0.1892307847738266\n",
      "[Training Epoch 9] Batch 5000, Loss 0.19023877382278442\n",
      "[Training Epoch 9] Batch 7500, Loss 0.16932928562164307\n",
      "[Training Epoch 9] Batch 10000, Loss 0.18734976649284363\n",
      "[Evluating Epoch 9] HR = 0.6392, NDCG = 0.3711\n",
      "Hit Ratio: 0.6392\n",
      "NDCG: 0.3711\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 10] Batch 0, Loss 0.15913289785385132\n",
      "[Training Epoch 10] Batch 2500, Loss 0.1632993519306183\n",
      "[Training Epoch 10] Batch 5000, Loss 0.1798468977212906\n",
      "[Training Epoch 10] Batch 7500, Loss 0.17736294865608215\n",
      "[Training Epoch 10] Batch 10000, Loss 0.17452678084373474\n",
      "[Evluating Epoch 10] HR = 0.6417, NDCG = 0.3708\n",
      "Hit Ratio: 0.6417\n",
      "NDCG: 0.3708\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 11] Batch 0, Loss 0.15860742330551147\n",
      "[Training Epoch 11] Batch 2500, Loss 0.1628209352493286\n",
      "[Training Epoch 11] Batch 5000, Loss 0.1880742907524109\n",
      "[Training Epoch 11] Batch 7500, Loss 0.17153774201869965\n",
      "[Training Epoch 11] Batch 10000, Loss 0.1333198994398117\n",
      "[Evluating Epoch 11] HR = 0.6422, NDCG = 0.3699\n",
      "Hit Ratio: 0.6422\n",
      "NDCG: 0.3699\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 12] Batch 0, Loss 0.17788702249526978\n",
      "[Training Epoch 12] Batch 2500, Loss 0.17947527766227722\n",
      "[Training Epoch 12] Batch 5000, Loss 0.1594797968864441\n",
      "[Training Epoch 12] Batch 7500, Loss 0.17781057953834534\n",
      "[Training Epoch 12] Batch 10000, Loss 0.16981691122055054\n",
      "[Evluating Epoch 12] HR = 0.6424, NDCG = 0.3722\n",
      "Hit Ratio: 0.6424\n",
      "NDCG: 0.3722\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 13] Batch 0, Loss 0.17549356818199158\n",
      "[Training Epoch 13] Batch 2500, Loss 0.17340202629566193\n",
      "[Training Epoch 13] Batch 5000, Loss 0.1604520082473755\n",
      "[Training Epoch 13] Batch 7500, Loss 0.17235736548900604\n",
      "[Training Epoch 13] Batch 10000, Loss 0.18289688229560852\n",
      "[Evluating Epoch 13] HR = 0.6444, NDCG = 0.3730\n",
      "Hit Ratio: 0.6444\n",
      "NDCG: 0.3730\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 14] Batch 0, Loss 0.19413310289382935\n",
      "[Training Epoch 14] Batch 2500, Loss 0.19145292043685913\n",
      "[Training Epoch 14] Batch 5000, Loss 0.17352041602134705\n",
      "[Training Epoch 14] Batch 7500, Loss 0.14639130234718323\n",
      "[Training Epoch 14] Batch 10000, Loss 0.1837957203388214\n",
      "[Evluating Epoch 14] HR = 0.6442, NDCG = 0.3742\n",
      "Hit Ratio: 0.6442\n",
      "NDCG: 0.3742\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.1722150593996048\n",
      "[Training Epoch 15] Batch 2500, Loss 0.1820855289697647\n",
      "[Training Epoch 15] Batch 5000, Loss 0.1730033904314041\n",
      "[Training Epoch 15] Batch 7500, Loss 0.16818386316299438\n",
      "[Training Epoch 15] Batch 10000, Loss 0.19024533033370972\n",
      "[Evluating Epoch 15] HR = 0.6424, NDCG = 0.3740\n",
      "Hit Ratio: 0.6424\n",
      "NDCG: 0.3740\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.16100946068763733\n",
      "[Training Epoch 16] Batch 2500, Loss 0.20080825686454773\n",
      "[Training Epoch 16] Batch 5000, Loss 0.18580986559391022\n",
      "[Training Epoch 16] Batch 7500, Loss 0.17031222581863403\n",
      "[Training Epoch 16] Batch 10000, Loss 0.15721741318702698\n",
      "[Evluating Epoch 16] HR = 0.6452, NDCG = 0.3741\n",
      "Hit Ratio: 0.6452\n",
      "NDCG: 0.3741\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 17] Batch 0, Loss 0.1844445765018463\n",
      "[Training Epoch 17] Batch 2500, Loss 0.17149695754051208\n",
      "[Training Epoch 17] Batch 5000, Loss 0.17344625294208527\n",
      "[Training Epoch 17] Batch 7500, Loss 0.18962650001049042\n",
      "[Training Epoch 17] Batch 10000, Loss 0.17132839560508728\n",
      "[Evluating Epoch 17] HR = 0.6452, NDCG = 0.3762\n",
      "Hit Ratio: 0.6452\n",
      "NDCG: 0.3762\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 18] Batch 0, Loss 0.1917313188314438\n",
      "[Training Epoch 18] Batch 2500, Loss 0.1672515869140625\n",
      "[Training Epoch 18] Batch 5000, Loss 0.13965070247650146\n",
      "[Training Epoch 18] Batch 7500, Loss 0.18503934144973755\n",
      "[Training Epoch 18] Batch 10000, Loss 0.21705155074596405\n",
      "[Evluating Epoch 18] HR = 0.6457, NDCG = 0.3722\n",
      "Hit Ratio: 0.6457\n",
      "NDCG: 0.3722\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 19] Batch 0, Loss 0.16924838721752167\n",
      "[Training Epoch 19] Batch 2500, Loss 0.16928565502166748\n",
      "[Training Epoch 19] Batch 5000, Loss 0.183818519115448\n",
      "[Training Epoch 19] Batch 7500, Loss 0.17306947708129883\n",
      "[Training Epoch 19] Batch 10000, Loss 0.16909761726856232\n",
      "[Evluating Epoch 19] HR = 0.6440, NDCG = 0.3739\n",
      "Hit Ratio: 0.6440\n",
      "NDCG: 0.3739\n",
      "--------------------------------------------------------------------------------\n",
      "Embedding(6040, 8)\n",
      "Embedding(3706, 8)\n",
      "Linear(in_features=16, out_features=8, bias=True)\n",
      "Linear(in_features=8, out_features=1, bias=True)\n",
      "MLP(\n",
      "  (embedding_user): Embedding(6040, 8)\n",
      "  (embedding_item): Embedding(3706, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.6329963803291321\n",
      "[Training Epoch 0] Batch 2500, Loss 0.20487859845161438\n",
      "[Training Epoch 0] Batch 5000, Loss 0.2657904028892517\n",
      "[Training Epoch 0] Batch 7500, Loss 0.17031559348106384\n",
      "[Training Epoch 0] Batch 10000, Loss 0.2527610957622528\n",
      "[Training Epoch 0] Batch 12500, Loss 0.20892933011054993\n",
      "[Training Epoch 0] Batch 15000, Loss 0.2579941153526306\n",
      "[Training Epoch 0] Batch 17500, Loss 0.19408802688121796\n",
      "[Training Epoch 0] Batch 20000, Loss 0.31079530715942383\n",
      "[Training Epoch 0] Batch 22500, Loss 0.20489832758903503\n",
      "[Training Epoch 0] Batch 25000, Loss 0.3284800052642822\n",
      "[Training Epoch 0] Batch 27500, Loss 0.27459216117858887\n",
      "[Training Epoch 0] Batch 30000, Loss 0.22645333409309387\n",
      "[Training Epoch 14] Batch 12500, Loss 0.22026848793029785\n",
      "[Training Epoch 14] Batch 15000, Loss 0.1798224002122879\n",
      "[Training Epoch 14] Batch 17500, Loss 0.1784643828868866\n",
      "[Training Epoch 14] Batch 20000, Loss 0.19596338272094727\n",
      "[Training Epoch 14] Batch 22500, Loss 0.20007061958312988\n",
      "[Training Epoch 14] Batch 25000, Loss 0.14556625485420227\n",
      "[Training Epoch 14] Batch 27500, Loss 0.21152520179748535\n",
      "[Training Epoch 14] Batch 30000, Loss 0.23658260703086853\n",
      "[Training Epoch 14] Batch 32500, Loss 0.20119833946228027\n",
      "[Training Epoch 14] Batch 35000, Loss 0.21388566493988037\n",
      "[Training Epoch 14] Batch 37500, Loss 0.15267395973205566\n",
      "[Training Epoch 14] Batch 40000, Loss 0.17615871131420135\n",
      "[Training Epoch 14] Batch 42500, Loss 0.15320251882076263\n",
      "[Evluating Epoch 14] HR = 0.5624, NDCG = 0.3174\n",
      "Hit Ratio: 0.5624\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 15] Batch 0, Loss 0.23193047940731049\n",
      "[Training Epoch 15] Batch 2500, Loss 0.1624746322631836\n",
      "[Training Epoch 15] Batch 5000, Loss 0.19180569052696228\n",
      "[Training Epoch 15] Batch 7500, Loss 0.16021472215652466\n",
      "[Training Epoch 15] Batch 10000, Loss 0.21711161732673645\n",
      "[Training Epoch 15] Batch 12500, Loss 0.2075178176164627\n",
      "[Training Epoch 15] Batch 15000, Loss 0.2596590518951416\n",
      "[Training Epoch 15] Batch 17500, Loss 0.22606471180915833\n",
      "[Training Epoch 15] Batch 20000, Loss 0.1907820850610733\n",
      "[Training Epoch 15] Batch 22500, Loss 0.17495256662368774\n",
      "[Training Epoch 15] Batch 25000, Loss 0.18514297902584076\n",
      "[Training Epoch 15] Batch 27500, Loss 0.193623885512352\n",
      "[Training Epoch 15] Batch 30000, Loss 0.16560809314250946\n",
      "[Training Epoch 15] Batch 32500, Loss 0.17146643996238708\n",
      "[Training Epoch 15] Batch 35000, Loss 0.2777532935142517\n",
      "[Training Epoch 15] Batch 37500, Loss 0.21229715645313263\n",
      "[Training Epoch 15] Batch 40000, Loss 0.23750607669353485\n",
      "[Training Epoch 15] Batch 42500, Loss 0.25162217020988464\n",
      "[Evluating Epoch 15] HR = 0.5785, NDCG = 0.3256\n",
      "Hit Ratio: 0.5785\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 16] Batch 0, Loss 0.18675823509693146\n",
      "[Training Epoch 16] Batch 2500, Loss 0.22190113365650177\n",
      "[Training Epoch 16] Batch 5000, Loss 0.20636209845542908\n",
      "[Training Epoch 16] Batch 7500, Loss 0.18929746747016907\n",
      "[Training Epoch 16] Batch 10000, Loss 0.1861053854227066\n",
      "[Training Epoch 16] Batch 12500, Loss 0.2239522635936737\n",
      "[Training Epoch 16] Batch 15000, Loss 0.18385840952396393\n",
      "[Training Epoch 16] Batch 17500, Loss 0.2075466811656952\n",
      "[Training Epoch 16] Batch 20000, Loss 0.1933768093585968\n",
      "[Training Epoch 16] Batch 22500, Loss 0.2289944291114807\n",
      "[Training Epoch 16] Batch 25000, Loss 0.21304288506507874\n",
      "[Training Epoch 16] Batch 27500, Loss 0.2034560889005661\n",
      "[Training Epoch 16] Batch 30000, Loss 0.1783873587846756\n",
      "[Training Epoch 16] Batch 32500, Loss 0.2739907503128052\n",
      "[Training Epoch 16] Batch 35000, Loss 0.25714111328125\n",
      "[Training Epoch 16] Batch 37500, Loss 0.20515123009681702\n",
      "[Training Epoch 16] Batch 40000, Loss 0.22362378239631653\n",
      "[Training Epoch 16] Batch 42500, Loss 0.23980575799942017\n",
      "[Evluating Epoch 16] HR = 0.5796, NDCG = 0.3219\n",
      "Hit Ratio: 0.5796\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17 starts !\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "GMF_negatives_hr = []\n",
    "MLP_negatives_hr = []\n",
    "NeuMF_negatives_hr = []\n",
    "GMF_negatives_ndcg = []\n",
    "MLP_negatives_ndcg = []\n",
    "NeuMF_negatives_ndcg = []\n",
    "for i in range(1,11):\n",
    "    print('+' * 80)\n",
    "    print('number of negatives: {}'.format(i))\n",
    "    if i == 4:\n",
    "        continue\n",
    "    config = gmf_config\n",
    "    engine = GMFEngine(config)\n",
    "    for epoch in range(config['num_epoch']):\n",
    "        print('Epoch {} starts !'.format(epoch))\n",
    "        print('-' * 80)\n",
    "        train_loader = sample_generator.instance_a_train_loader(i, config['batch_size'])\n",
    "        last_batch_loss = engine.train_an_epoch(train_loader, epoch_id=epoch)\n",
    "    \n",
    "        # Evaluate the model after each epoch\n",
    "        hit_ratio, ndcg = engine.evaluate(evaluate_data, epoch_id=epoch)\n",
    "        print('Hit Ratio: {:.4f}'.format(hit_ratio))\n",
    "        print('NDCG: {:.4f}'.format(ndcg))\n",
    "        print('-' * 80)                                \n",
    "        # Save the model after each epoch\n",
    "        engine.save(config['alias'], epoch, hit_ratio, ndcg)\n",
    "    GMF_negatives_hr.append(hit_ratio)\n",
    "    GMF_negatives_ndcg.append(ndcg)\n",
    "    config = mlp_config\n",
    "    engine = MLPEngine(config)\n",
    "    for epoch in range(config['num_epoch']):\n",
    "        print('Epoch {} starts !'.format(epoch))\n",
    "        print('-' * 80)\n",
    "        train_loader = sample_generator.instance_a_train_loader(i, config['batch_size'])\n",
    "        last_batch_loss = engine.train_an_epoch(train_loader, epoch_id=epoch)\n",
    "    \n",
    "        # Evaluate the model after each epoch\n",
    "        hit_ratio, ndcg = engine.evaluate(evaluate_data, epoch_id=epoch)\n",
    "        print('Hit Ratio: {:.4f}'.format(hit_ratio))\n",
    "        print('-' * 80)                                \n",
    "        # Save the model after each epoch\n",
    "        engine.save(config['alias'], epoch, hit_ratio, ndcg)\n",
    "    MLP_negatives_hr.append(hit_ratio)\n",
    "    MLP_negatives_ndcg.append(ndcg)\n",
    "    config = neumf_config\n",
    "    engine = NeuMFEngine(config)\n",
    "    for epoch in range(config['num_epoch']):\n",
    "        print('Epoch {} starts !'.format(epoch))\n",
    "        print('-' * 80)\n",
    "        train_loader = sample_generator.instance_a_train_loader(i, config['batch_size'])\n",
    "        last_batch_loss = engine.train_an_epoch(train_loader, epoch_id=epoch)\n",
    "    \n",
    "        # Evaluate the model after each epoch\n",
    "        hit_ratio, ndcg = engine.evaluate(evaluate_data, epoch_id=epoch)\n",
    "        print('Hit Ratio: {:.4f}'.format(hit_ratio))\n",
    "        print('-' * 80)                                \n",
    "        # Save the model after each epoch\n",
    "        engine.save(config['alias'], epoch, hit_ratio, ndcg)\n",
    "    NeuMF_negatives_hr.append(hit_ratio)\n",
    "    NeuMF_negatives_ndcg.append(ndcg)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dictionary with the data\n",
    "data = {\n",
    "    'Negatives': list(range(1, 11)),\n",
    "    'GMF_HR@10': GMF_negatives_hr,\n",
    "    'MLP_HR@10': MLP_negatives_hr,\n",
    "    'NeuMF_HR@10': NeuMF_negatives_hr,\n",
    "    'GMF_NDCG@10': GMF_negatives_ndcg,\n",
    "    'MLP_NDCG@10': MLP_negatives_ndcg,\n",
    "    'NeuMF_NDCG@10': NeuMF_negatives_ndcg\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('./negatives_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "negatives = np.arange(1, 11)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# HR@10 plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(negatives, NeuMF_negatives_hr, 'r-d', label='NeuMF')\n",
    "plt.plot(negatives, GMF_negatives_hr, 'b-o', label='GMF')\n",
    "plt.plot(negatives, MLP_negatives_hr, 'm-x', label='MLP')\n",
    "plt.axhline(y=0.65, color='y', linestyle='-', label='BPR')\n",
    "plt.xlabel('Number of Negatives')\n",
    "plt.ylabel('HR@10')\n",
    "plt.title('MovieLens')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# NDCG@10 plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(negatives, NeuMF_negatives_ndcg, 'r-d', label='NeuMF')\n",
    "plt.plot(negatives, GMF_negatives_ndcg, 'b-o', label='GMF')\n",
    "plt.plot(negatives, MLP_negatives_ndcg, 'm-x', label='MLP')\n",
    "plt.axhline(y=0.36, color='y', linestyle='-', label='BPR')\n",
    "plt.xlabel('Number of Negatives')\n",
    "plt.ylabel('NDCG@10')\n",
    "plt.title('MovieLens')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
